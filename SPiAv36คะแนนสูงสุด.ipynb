{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk/uR180nrunBJKvP8UEjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treekeaw1/-mana-bento-web/blob/main/SPiAv36%E0%B8%84%E0%B8%B0%E0%B9%81%E0%B8%99%E0%B8%99%E0%B8%AA%E0%B8%B9%E0%B8%87%E0%B8%AA%E0%B8%B8%E0%B8%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMFXRH22xMlS",
        "outputId": "e1fc326b-a780-4208-8312-d1426c487a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
            "üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv'...\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ encoding 'utf-8' ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏° 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Header ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "\n",
            "‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‚Ä¶' ‡πÄ‡∏õ‡πá‡∏ô ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
            "‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '\u0015‚Ä¶' ‡πÄ‡∏õ‡πá‡∏ô ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)\n",
            "‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '√±‚Ä¶' ‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
            "‚ÑπÔ∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ('</artifact identifier=\"lottery_csv\" type=\"text/pla...') ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Footer/Metadata ‡∏à‡∏∞‡∏•‡∏ö‡∏≠‡∏≠‡∏Å\n",
            "\n",
            "‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ 560 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏à‡∏≤‡∏Å 560 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤)\n",
            "‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 2002-01-16 ‡∏ñ‡∏∂‡∏á 2025-06-16\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
        "\n",
        "print(\"--- ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\")\n",
        "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv'...\")\n",
        "\n",
        "file_path = '‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv'\n",
        "df = None\n",
        "original_df_rows_count = 0 # ‡∏à‡∏∞‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏¥‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\n",
        "\n",
        "try:\n",
        "    # 1. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å (metadata header)\n",
        "    # ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏´‡∏•‡∏±‡∏á skip) ‡πÄ‡∏õ‡πá‡∏ô Header ‡∏à‡∏£‡∏¥‡∏á‡πÜ\n",
        "    # ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô .0\n",
        "    encodings_to_try = ['utf-8', 'tis-620', 'cp874', 'latin1', 'iso-8859-1']\n",
        "\n",
        "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "    expected_header_names = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á']\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            # skiprows=1: ‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô metadata\n",
        "            # header=0: ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏´‡∏•‡∏±‡∏á skip (‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°) ‡πÄ‡∏õ‡πá‡∏ô Header\n",
        "            # dtype: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏ñ‡∏π‡∏Å‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
        "            # (‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà Pandas ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏ï‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2)\n",
        "            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype ‡πÄ‡∏õ‡πá‡∏ô 'object' (‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ string) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2\n",
        "            df_temp = pd.read_csv(file_path,\n",
        "                                  encoding=encoding,\n",
        "                                  skiprows=1,\n",
        "                                  header=0,\n",
        "                                  dtype={1: str, 2: str}) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2 ‡πÄ‡∏õ‡πá‡∏ô string\n",
        "\n",
        "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤\n",
        "            if df_temp.shape[1] >= 3 and pd.notna(df_temp.iloc[0, 0]):\n",
        "                df = df_temp\n",
        "                print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ encoding '{encoding}' ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏° 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Header ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ encoding '{encoding}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏π‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏°‡πà‡∏û‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏ß‡πà‡∏≤‡∏á)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ encoding '{encoding}': {e}\") # ‡∏õ‡∏¥‡∏î log error ‡∏ã‡πâ‡∏≥‡πÜ\n",
        "            continue\n",
        "\n",
        "    if df is None:\n",
        "        raise Exception(\"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡∏î‡πâ‡∏ß‡∏¢ encoding ‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡∏•‡∏≠‡∏á‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")\n",
        "\n",
        "    # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "    if len(df.columns) < 3:\n",
        "        raise ValueError(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö ({len(df.columns)}) ‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà, ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1, ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)\")\n",
        "\n",
        "    # ‡πÉ‡∏ä‡πâ df.columns[0], df.columns[1], df.columns[2] ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà Pandas ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å‡πÜ)\n",
        "    # ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
        "    df_selected = df.iloc[:, [0, 1, 2]].copy() # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏Ñ‡πà 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å\n",
        "    df_selected.columns = ['Date_Raw', 'Prize1_Raw', 'Last2Digits_Raw']\n",
        "\n",
        "    print(f\"\\n‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{df.columns[0]}' ‡πÄ‡∏õ‡πá‡∏ô ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\")\n",
        "    print(f\"‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{df.columns[1]}' ‡πÄ‡∏õ‡πá‡∏ô ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)\")\n",
        "    print(f\"‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{df.columns[2]}' ‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\")\n",
        "\n",
        "    # 3. ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô footer (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
        "    last_row_date_val = str(df_selected.iloc[-1]['Date_Raw']).strip()\n",
        "    if \"artifact\" in last_row_date_val.lower():\n",
        "        print(f\"‚ÑπÔ∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ('{last_row_date_val[:50]}...') ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Footer/Metadata ‡∏à‡∏∞‡∏•‡∏ö‡∏≠‡∏≠‡∏Å\")\n",
        "        df_selected = df_selected.iloc[:-1].copy().reset_index(drop=True)\n",
        "\n",
        "    original_df_rows_count = len(df_selected) # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô df_selected ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î\n",
        "\n",
        "    # 4. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN, '-' ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0)\n",
        "    cleaned_data = []\n",
        "    seen_ids = set() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥\n",
        "\n",
        "    for index, row in df_selected.iterrows():\n",
        "        try:\n",
        "            # 4.1 ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á)\n",
        "            date_raw = str(row['Date_Raw']).strip()\n",
        "            if not date_raw or date_raw.lower() == 'nan' or date_raw == '-':\n",
        "                continue\n",
        "\n",
        "            date_obj = None\n",
        "            date_formats = [\n",
        "                '%Y/%m/%d', '%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y',\n",
        "                '%d/%m/%y', '%d-%m-%y', # DD/MM/YY\n",
        "                '%d %b %Y', '%d %B %Y', # DD MonYYYY (e.g., 1 ‡∏°.‡∏Ñ. 2565)\n",
        "            ]\n",
        "\n",
        "            # ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. (‡∏ñ‡πâ‡∏≤‡∏û‡∏ö)\n",
        "            if re.search(r'\\b25\\d{2}\\b', date_raw):\n",
        "                date_raw_parts = date_raw.split()\n",
        "                for i, part in enumerate(date_raw_parts):\n",
        "                    if part.isdigit() and len(part) == 4 and part.startswith('25'):\n",
        "                        date_raw_parts[i] = str(int(part) - 543)\n",
        "                date_raw = ' '.join(date_raw_parts)\n",
        "\n",
        "            for fmt in date_formats:\n",
        "                try:\n",
        "                    date_obj = datetime.strptime(date_raw, fmt)\n",
        "                    break\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            if date_obj is None:\n",
        "                print(f\"‚ö†Ô∏è ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index} (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà '{row['Date_Raw']}'): ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ\")\n",
        "                continue\n",
        "\n",
        "            # 4.2 ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Prize1_Raw (6 ‡∏´‡∏•‡∏±‡∏Å) ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0\n",
        "            prize1_raw = str(row['Prize1_Raw']).strip()\n",
        "            if not prize1_raw or prize1_raw.lower() == 'nan' or prize1_raw == '-':\n",
        "                continue\n",
        "\n",
        "            prize1_cleaned = re.sub(r'[^\\d]', '', prize1_raw) # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "            # *** ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 6 ‡∏´‡∏•‡∏±‡∏Å ***\n",
        "            prize1_cleaned = prize1_cleaned.zfill(6)\n",
        "\n",
        "            if len(prize1_cleaned) != 6:\n",
        "                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å zfill ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà 6 ‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏°‡∏≤‡∏Å\n",
        "                print(f\"‚ö†Ô∏è ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index} (Prize1_Raw '{row['Prize1_Raw']}'): ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö 6 ‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0 ({len(prize1_cleaned)} ‡∏´‡∏•‡∏±‡∏Å) ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ\")\n",
        "                continue\n",
        "\n",
        "            digits = [int(d) for d in prize1_cleaned]\n",
        "\n",
        "            # 4.3 ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Last2Digits_Raw (2 ‡∏´‡∏•‡∏±‡∏Å) ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0\n",
        "            last2_cleaned = None\n",
        "            last2_raw = str(row['Last2Digits_Raw']).strip()\n",
        "\n",
        "            if not last2_raw or last2_raw.lower() == 'nan' or last2_raw == '-':\n",
        "                last2_cleaned = prize1_cleaned[-2:] # ‡πÉ‡∏ä‡πâ 2 ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Prize1_Raw ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á\n",
        "            else:\n",
        "                last2_cleaned = re.sub(r'[^\\d]', '', last2_raw)\n",
        "\n",
        "            # *** ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 2 ‡∏´‡∏•‡∏±‡∏Å ***\n",
        "            last2_cleaned = last2_cleaned.zfill(2)\n",
        "\n",
        "            if len(last2_cleaned) != 2:\n",
        "                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å zfill ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà 2 ‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏°‡∏≤‡∏Å\n",
        "                print(f\"‚ö†Ô∏è ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index} (Last2Digits_Raw '{row.get('Last2Digits_Raw', 'N/A')}'): ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö 2 ‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ\")\n",
        "                continue\n",
        "\n",
        "            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà + Prize1 ‡πÄ‡∏õ‡πá‡∏ô ID\n",
        "            row_id = f\"{date_obj.isoformat()}_{prize1_cleaned}\"\n",
        "            if row_id in seen_ids:\n",
        "                print(f\"‚ö†Ô∏è ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index}: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {date_obj.isoformat()}, Prize1: {prize1_cleaned}) ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ\")\n",
        "                continue\n",
        "            seen_ids.add(row_id)\n",
        "\n",
        "            cleaned_data.append({\n",
        "                'id': row_id,\n",
        "                'date': date_obj,\n",
        "                'dateStr': date_obj.strftime('%Y-%m-%d'),\n",
        "                'prize1': prize1_cleaned,\n",
        "                'last2': last2_cleaned,\n",
        "                'digits': digits,\n",
        "                'sum': sum(digits),\n",
        "                'evenCount': sum(1 for d in digits if d % 2 == 0),\n",
        "                'oddCount': sum(1 for d in digits if d % 2 == 1),\n",
        "                'last2Int': int(last2_cleaned)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index}: {e}. ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ\")\n",
        "            continue\n",
        "\n",
        "    # 5. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "    processed_data = sorted(cleaned_data, key=lambda x: x['date'])\n",
        "\n",
        "    if not processed_data:\n",
        "        raise Exception(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\")\n",
        "\n",
        "    print(f\"\\n‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ {len(processed_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏à‡∏≤‡∏Å {original_df_rows_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤)\")\n",
        "    print(f\"‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà {processed_data[0]['dateStr']} ‡∏ñ‡∏∂‡∏á {processed_data[-1]['dateStr']}\")\n",
        "\n",
        "    _processed_data_cell1 = processed_data\n",
        "    _original_df_rows_cell1 = original_df_rows_count\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå '{file_path}' ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå\")\n",
        "except ValueError as ve:\n",
        "    print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {ve}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß ---\n",
        "if '_processed_data_cell1' in locals() and _processed_data_cell1:\n",
        "    print(\"\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (6 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å) ---\")\n",
        "    for i, item in enumerate(_processed_data_cell1[:6]):\n",
        "        print(f\"  {item['dateStr']} | Prize1: {item['prize1']} | Last2: {item['last2']}\")\n",
        "\n",
        "    if len(_processed_data_cell1) > 6:\n",
        "        print(\"\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (6 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢) ---\")\n",
        "        for i, item in enumerate(_processed_data_cell1[-6:]):\n",
        "            print(f\"  {item['dateStr']} | Prize1: {item['prize1']} | Last2: {item['last2']}\")\n",
        "    else:\n",
        "        print(\"\\n‚ÑπÔ∏è ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 6 ‡πÅ‡∏ñ‡∏ß ‡∏à‡∏∂‡∏á‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5xaHSXFx3GG",
        "outputId": "65c58ef7-64cc-483d-8753-9dcd2cc9141b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (6 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å) ---\n",
            "  2002-01-16 | Prize1: 709670 | Last2: 14\n",
            "  2002-02-01 | Prize1: 633270 | Last2: 40\n",
            "  2002-02-16 | Prize1: 330853 | Last2: 92\n",
            "  2002-03-01 | Prize1: 805590 | Last2: 55\n",
            "  2002-03-16 | Prize1: 659152 | Last2: 04\n",
            "  2002-04-01 | Prize1: 990543 | Last2: 57\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (6 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢) ---\n",
            "  2025-04-01 | Prize1: 669687 | Last2: 36\n",
            "  2025-04-16 | Prize1: 266227 | Last2: 85\n",
            "  2025-05-02 | Prize1: 213388 | Last2: 06\n",
            "  2025-05-16 | Prize1: 251309 | Last2: 87\n",
            "  2025-06-01 | Prize1: 559352 | Last2: 20\n",
            "  2025-06-16 | Prize1: 507392 | Last2: 06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ processed_data ‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: processed_data = [...]\n",
        "\n",
        "# ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ processed_data, ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á dummy data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ\n",
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• processed_data (‡∏Ñ‡∏ß‡∏£‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ)\n",
        "# processed_data = [\n",
        "#     {'id': 1, 'dateStr': '2020-01-01', 'prize1': '123456', 'last2': '78', 'date': datetime(2020, 1, 1), 'last2Int': 78, 'sum': sum(map(int, '123456')), 'evenCount': sum(1 for d in map(int, '123456') if d % 2 == 0), 'digits': [1,2,3,4,5,6]},\n",
        "#     {'id': 2, 'dateStr': '2020-01-16', 'prize1': '654321', 'last2': '23', 'date': datetime(2020, 1, 16), 'last2Int': 23, 'sum': sum(map(int, '654321')), 'evenCount': sum(1 for d in map(int, '654321') if d % 2 == 0), 'digits': [6,5,4,3,2,1]},\n",
        "#     {'id': 3, 'dateStr': '2020-02-01', 'prize1': '789012', 'last2': '45', 'date': datetime(2020, 2, 1), 'last2Int': 45, 'sum': sum(map(int, '789012')), 'evenCount': sum(1 for d in map(int, '789012') if d % 2 == 0), 'digits': [7,8,9,0,1,2]},\n",
        "#     {'id': 4, 'dateStr': '2020-02-16', 'prize1': '102938', 'last2': '67', 'date': datetime(2020, 2, 16), 'last2Int': 67, 'sum': sum(map(int, '102938')), 'evenCount': sum(1 for d in map(int, '102938') if d % 2 == 0), 'digits': [1,0,2,9,3,8]},\n",
        "#     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "#     {'id': 5, 'dateStr': '2020-03-01', 'prize1': '456789', 'last2': '12', 'date': datetime(2020, 3, 1), 'last2Int': 12, 'sum': sum(map(int, '456789')), 'evenCount': sum(1 for d in map(int, '456789') if d % 2 == 0), 'digits': [4,5,6,7,8,9]},\n",
        "#     {'id': 6, 'dateStr': '2020-03-16', 'prize1': '987654', 'last2': '34', 'date': datetime(2020, 3, 16), 'last2Int': 34, 'sum': sum(map(int, '987654')), 'evenCount': sum(1 for d in map(int, '987654') if d % 2 == 0), 'digits': [9,8,7,6,5,4]},\n",
        "#     {'id': 7, 'dateStr': '2020-04-01', 'prize1': '111222', 'last2': '56', 'date': datetime(2020, 4, 1), 'last2Int': 56, 'sum': sum(map(int, '111222')), 'evenCount': sum(1 for d in map(int, '111222') if d % 2 == 0), 'digits': [1,1,1,2,2,2]},\n",
        "#     {'id': 8, 'dateStr': '2020-04-16', 'prize1': '333444', 'last2': '78', 'date': datetime(2020, 4, 16), 'last2Int': 78, 'sum': sum(map(int, '333444')), 'evenCount': sum(1 for d in map(int, '333444') if d % 2 == 0), 'digits': [3,3,3,4,4,4]},\n",
        "#     {'id': 9, 'dateStr': '2020-05-01', 'prize1': '555666', 'last2': '90', 'date': datetime(2020, 5, 1), 'last2Int': 90, 'sum': sum(map(int, '555666')), 'evenCount': sum(1 for d in map(int, '555666') if d % 2 == 0), 'digits': [5,5,5,6,6,6]},\n",
        "#     {'id': 10, 'dateStr': '2020-05-16', 'prize1': '777888', 'last2': '01', 'date': datetime(2020, 5, 16), 'last2Int': 1, 'sum': sum(map(int, '777888')), 'evenCount': sum(1 for d in map(int, '777888') if d % 2 == 0), 'digits': [7,7,7,8,8,8]},\n",
        "#     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö start_train_size ‡∏Ç‡∏≠‡∏á backtesting (300 ‡∏á‡∏ß‡∏î)\n",
        "#     # ... (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ)\n",
        "# ]\n",
        "\n",
        "# ‡∏´‡∏≤‡∏Å processed_data ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\n",
        "if 'processed_data' not in locals() or not processed_data:\n",
        "    print(\"‚ùå Error: 'processed_data' ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•! ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á processed_data ‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "    processed_data = [] # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠\n",
        "\n",
        "# --- 1. Feature Engineering (Extended) ---\n",
        "print(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Sequential Features...\")\n",
        "\n",
        "def extract_sequential_features(data, lookback_window=10, long_lookback_window=50):\n",
        "    features_list = []\n",
        "\n",
        "    # Global counters for overall frequency and last seen (across full history processed so far)\n",
        "    # Initialize for 6 positions, 10 digits each (0-9)\n",
        "    overall_digit_counts = {pos: defaultdict(int) for pos in range(6)}\n",
        "    overall_digit_last_seen = {pos: {digit: -1 for digit in range(10)} for pos in range(6)} # Stores index of last seen\n",
        "\n",
        "    for index, item in enumerate(data):\n",
        "        features = {}\n",
        "\n",
        "        # Ensure all expected fields exist, provide default empty/zero if not\n",
        "        # This is crucial for consistency, especially for dummy future rows\n",
        "        features['date'] = item.get('date')\n",
        "        features['dateStr'] = item.get('dateStr', '')\n",
        "        features['prize1'] = item.get('prize1', '')\n",
        "        features['last2'] = item.get('last2', '')\n",
        "        features['last2Int'] = item.get('last2Int', 0)\n",
        "        features['sum'] = item.get('sum', 0)\n",
        "        features['evenCount'] = item.get('evenCount', 0)\n",
        "        features['digits'] = item.get('digits', [0,0,0,0,0,0]) # Default to all zeros for dummy\n",
        "\n",
        "        # New: Add oddCount here (Crucial fix)\n",
        "        if 'digits' in item and len(item['digits']) == 6:\n",
        "            features['oddCount'] = sum(1 for digit in item['digits'] if digit % 2 != 0)\n",
        "        else:\n",
        "            features['oddCount'] = 0.0 # Ensure it's a float for consistency\n",
        "\n",
        "        # History for lookback calculations\n",
        "        history = data[max(0, index - lookback_window) : index]\n",
        "        full_history_to_current = data[max(0, index - long_lookback_window) : index] # Use a longer window for overall trends\n",
        "\n",
        "        # Initialize position-specific features to 0.0 for robustness\n",
        "        for pos in range(6):\n",
        "            features[f'pos{pos}_value'] = item['digits'][pos] if 'digits' in item and len(item['digits']) > pos else 0.0\n",
        "            features[f'pos{pos}_parity'] = 1 if (item['digits'][pos] % 2 == 0) else 0 if 'digits' in item and len(item['digits']) > pos else 0.0\n",
        "            features[f'pos{pos}_avgLast3'] = 0.0\n",
        "            features[f'pos{pos}_stdLast3'] = 0.0\n",
        "            features[f'pos{pos}_trend'] = 0.0\n",
        "            features[f'pos{pos}_shiftFromPrev'] = 0.0\n",
        "            features[f'pos{pos}_deltaFromAvg'] = 0.0\n",
        "            features[f'pos{pos}_minInWindow'] = 0.0\n",
        "            features[f'pos{pos}_maxInWindow'] = 0.0\n",
        "            features[f'pos{pos}_overallFreq'] = 0.0\n",
        "            features[f'pos{pos}_lastSeenGap'] = 0.0\n",
        "            features[f'pos{pos}_sameDateAvg'] = 0.0\n",
        "            features[f'pos{pos}_sameDateFreq'] = 0.0\n",
        "            features[f'pos{pos}_sameDateLastValue'] = 0.0\n",
        "            features[f'pos{pos}_sameDateTrend'] = 0.0\n",
        "\n",
        "        # General features, initialize to 0.0 or appropriate defaults\n",
        "        features['avgSum'] = 0.0\n",
        "        features['avgEvenCount'] = 0.0\n",
        "        features['avgOddCount'] = 0.0 # Initialize avgOddCount\n",
        "        features['sum_last3_total'] = 0.0\n",
        "        features['even_count_last3_total'] = 0.0\n",
        "        features['odd_count_last3_total'] = 0.0 # Initialize odd_count_last3_total\n",
        "        features['last2_lastValue'] = 0.0\n",
        "        features['last2_frequency_in_history'] = 0.0\n",
        "        features['last2_gap'] = 0.0\n",
        "        features['last2_sameDateAvg'] = 0.0\n",
        "        features['last2_sameDateFreq'] = 0.0\n",
        "        features['last2_sameDateLastValue'] = 0.0\n",
        "        features['last2_sameDateTrend'] = 0.0\n",
        "        features['last2_sameDateGapYears'] = 0.0\n",
        "\n",
        "        features['dayOfWeek'] = 0\n",
        "        features['month'] = 0\n",
        "        features['dayOfMonth'] = 0\n",
        "        features['weekOfYear'] = 0\n",
        "        features['year'] = 0\n",
        "        features['isFirstHalfOfMonth'] = 0\n",
        "        features['isMidMonth'] = 0\n",
        "        features['isEndMonth'] = 0\n",
        "\n",
        "        # --- Basic Positional Features (Existing & Improved) ---\n",
        "        for pos in range(6):\n",
        "            pos_history = [h['digits'][pos] for h in history if 'digits' in h and len(h['digits']) > pos]\n",
        "            if pos_history:\n",
        "                features[f'pos{pos}_avgLast3'] = np.mean(pos_history[-3:]) if len(pos_history) >= 3 else np.mean(pos_history)\n",
        "                features[f'pos{pos}_stdLast3'] = np.std(pos_history[-3:]) if len(pos_history) >= 3 else 0.0\n",
        "\n",
        "                # Trend (difference between current and previous in lookback window)\n",
        "                if len(pos_history) >= 2:\n",
        "                    features[f'pos{pos}_trend'] = pos_history[-1] - pos_history[-2]\n",
        "                else:\n",
        "                    features[f'pos{pos}_trend'] = 0.0\n",
        "\n",
        "                # New: Positional Shift from previous draw\n",
        "                if index > 0 and 'digits' in data[index-1] and len(data[index-1]['digits']) > pos and 'digits' in item and len(item['digits']) > pos:\n",
        "                    features[f'pos{pos}_shiftFromPrev'] = item['digits'][pos] - data[index-1]['digits'][pos]\n",
        "                else:\n",
        "                    features[f'pos{pos}_shiftFromPrev'] = 0.0\n",
        "\n",
        "                # New: Delta from Average in lookback window\n",
        "                if 'digits' in item and len(item['digits']) > pos:\n",
        "                    features[f'pos{pos}_deltaFromAvg'] = item['digits'][pos] - features[f'pos{pos}_avgLast3']\n",
        "                else:\n",
        "                    features[f'pos{pos}_deltaFromAvg'] = 0.0\n",
        "\n",
        "                # New: Min/Max in lookback window\n",
        "                features[f'pos{pos}_minInWindow'] = np.min(pos_history)\n",
        "                features[f'pos{pos}_maxInWindow'] = np.max(pos_history)\n",
        "            else:\n",
        "                # Handle case where pos_history is empty\n",
        "                features[f'pos{pos}_avgLast3'] = 0.0\n",
        "                features[f'pos{pos}_stdLast3'] = 0.0\n",
        "                features[f'pos{pos}_trend'] = 0.0\n",
        "                features[f'pos{pos}_shiftFromPrev'] = 0.0\n",
        "                features[f'pos{pos}_deltaFromAvg'] = 0.0\n",
        "                features[f'pos{pos}_minInWindow'] = 0.0\n",
        "                features[f'pos{pos}_maxInWindow'] = 0.0\n",
        "\n",
        "        # --- Overall Sum/Even/Odd Counts Features (Existing & Improved) ---\n",
        "        full_sums_history = [h['sum'] for h in history if 'sum' in h]\n",
        "        full_even_counts_history = [h['evenCount'] for h in history if 'evenCount' in h]\n",
        "        full_odd_counts_history = [h['oddCount'] for h in history if 'oddCount' in h] # New: Odd count history\n",
        "\n",
        "        if full_sums_history: features['avgSum'] = np.mean(full_sums_history)\n",
        "        if full_even_counts_history: features['avgEvenCount'] = np.mean(full_even_counts_history)\n",
        "        if full_odd_counts_history: features['avgOddCount'] = np.mean(full_odd_counts_history) # New: Avg odd count\n",
        "\n",
        "        features['sum_last3_total'] = np.sum(full_sums_history[-3:]) if len(full_sums_history) >= 3 else 0.0\n",
        "        features['even_count_last3_total'] = np.sum(full_even_counts_history[-3:]) if len(full_even_counts_history) >= 3 else 0.0\n",
        "        features['odd_count_last3_total'] = np.sum(full_odd_counts_history[-3:]) if len(full_odd_counts_history) >= 3 else 0.0 # New: Odd count last 3 total\n",
        "\n",
        "        # --- Last2-specific Features (Existing) ---\n",
        "        last2_history = [h['last2Int'] for h in history if 'last2Int' in h]\n",
        "        if last2_history:\n",
        "            features['last2_lastValue'] = last2_history[-1]\n",
        "            if 'last2Int' in item:\n",
        "                features['last2_frequency_in_history'] = last2_history.count(item['last2Int'])\n",
        "            else:\n",
        "                features['last2_frequency_in_history'] = 0.0\n",
        "        else:\n",
        "            features['last2_lastValue'] = 0.0\n",
        "            features['last2_frequency_in_history'] = 0.0\n",
        "\n",
        "        last_same_index = -1\n",
        "        if 'last2Int' in item:\n",
        "            for i_hist in range(index - 1, -1, -1):\n",
        "                if 'last2Int' in data[i_hist] and data[i_hist]['last2Int'] == item['last2Int']:\n",
        "                    last_same_index = i_hist\n",
        "                    break\n",
        "        features['last2_gap'] = index - last_same_index if last_same_index >= 0 else index + 1\n",
        "\n",
        "        # --- Same-Date-in-History Features (Improved) ---\n",
        "        if 'date' in item and isinstance(item['date'], datetime):\n",
        "            same_date_history = [h for h in data[:index] if 'date' in h and isinstance(h['date'], datetime) and h['date'].month == item['date'].month and h['date'].day == item['date'].day]\n",
        "            if same_date_history:\n",
        "                for pos in range(6):\n",
        "                    pos_same_date_digits = [h['digits'][pos] for h in same_date_history if 'digits' in h and len(h['digits']) > pos]\n",
        "                    if pos_same_date_digits:\n",
        "                        features[f'pos{pos}_sameDateAvg'] = np.mean(pos_same_date_digits)\n",
        "                        if 'digits' in item and len(item['digits']) > pos:\n",
        "                            features[f'pos{pos}_sameDateFreq'] = pos_same_date_digits.count(item['digits'][pos]) / len(pos_same_date_digits)\n",
        "                        else: features[f'pos{pos}_sameDateFreq'] = 0.0\n",
        "                        features[f'pos{pos}_sameDateLastValue'] = pos_same_date_digits[-1]\n",
        "                        # New: Trend on same date (difference between last two same-date occurrences)\n",
        "                        if len(pos_same_date_digits) >= 2:\n",
        "                            features[f'pos{pos}_sameDateTrend'] = pos_same_date_digits[-1] - pos_same_date_digits[-2]\n",
        "                        else:\n",
        "                            features[f'pos{pos}_sameDateTrend'] = 0.0\n",
        "                    else: # If no same_date_digits for this position\n",
        "                        features[f'pos{pos}_sameDateAvg'] = 0.0\n",
        "                        features[f'pos{pos}_sameDateFreq'] = 0.0\n",
        "                        features[f'pos{pos}_sameDateLastValue'] = 0.0\n",
        "                        features[f'pos{pos}_sameDateTrend'] = 0.0\n",
        "\n",
        "\n",
        "                same_date_last2_int = [h['last2Int'] for h in same_date_history if 'last2Int' in h]\n",
        "                if same_date_last2_int:\n",
        "                    features['last2_sameDateAvg'] = np.mean(same_date_last2_int)\n",
        "                    if 'last2Int' in item:\n",
        "                        features['last2_sameDateFreq'] = same_date_last2_int.count(item['last2Int']) / len(same_date_last2_int)\n",
        "                    else: features[f'pos{pos}_sameDateFreq'] = 0.0 # Changed from last2_sameDateFreq to pos{pos}_sameDateFreq\n",
        "                    features['last2_sameDateLastValue'] = same_date_last2_int[-1]\n",
        "                    # New: Trend on same date for last2\n",
        "                    if len(same_date_last2_int) >= 2:\n",
        "                        features['last2_sameDateTrend'] = same_date_last2_int[-1] - same_date_last2_int[-2]\n",
        "                    else:\n",
        "                        features['last2_sameDateTrend'] = 0.0\n",
        "                else: # If no same_date_last2_int\n",
        "                    features['last2_sameDateAvg'] = 0.0\n",
        "                    features['last2_sameDateFreq'] = 0.0\n",
        "                    features['last2_sameDateLastValue'] = 0.0\n",
        "                    features['last2_sameDateTrend'] = 0.0\n",
        "\n",
        "                last_same_date_year_appeared = -1\n",
        "                if 'last2Int' in item:\n",
        "                    for hist_item in reversed(same_date_history):\n",
        "                        if 'last2Int' in hist_item and hist_item['last2Int'] == item['last2Int']:\n",
        "                            if 'date' in hist_item and isinstance(hist_item['date'], datetime):\n",
        "                                last_same_date_year_appeared = hist_item['date'].year\n",
        "                                break\n",
        "\n",
        "                if last_same_date_year_appeared != -1 and 'date' in item and isinstance(item['date'], datetime):\n",
        "                    features['last2_sameDateGapYears'] = item['date'].year - last_same_date_year_appeared\n",
        "                elif same_date_history and 'date' in same_date_history[0] and isinstance(same_date_history[0]['date'], datetime) and 'date' in item and isinstance(item['date'], datetime):\n",
        "                    features['last2_sameDateGapYears'] = item['date'].year - same_date_history[0]['date'].year + 1 # Gap from earliest same-date entry\n",
        "                else:\n",
        "                    features['last2_sameDateGapYears'] = 0.0\n",
        "            else: # If no same_date_history at all\n",
        "                for pos in range(6):\n",
        "                    features[f'pos{pos}_sameDateAvg'] = 0.0\n",
        "                    features[f'pos{pos}_sameDateFreq'] = 0.0\n",
        "                    features[f'pos{pos}_sameDateLastValue'] = 0.0\n",
        "                    features[f'pos{pos}_sameDateTrend'] = 0.0\n",
        "                features['last2_sameDateAvg'] = 0.0\n",
        "                features['last2_sameDateFreq'] = 0.0\n",
        "                features['last2_sameDateLastValue'] = 0.0\n",
        "                features['last2_sameDateTrend'] = 0.0\n",
        "                features['last2_sameDateGapYears'] = 0.0\n",
        "\n",
        "\n",
        "        # --- Date-based Features (Expanded) ---\n",
        "        if 'date' in item and isinstance(item['date'], datetime):\n",
        "            features['dayOfWeek'] = item['date'].weekday() # 0=Monday, 6=Sunday\n",
        "            features['month'] = item['date'].month\n",
        "            features['dayOfMonth'] = item['date'].day\n",
        "            features['weekOfYear'] = item['date'].isocalendar()[1]\n",
        "            features['year'] = item['date'].year\n",
        "\n",
        "            # New: Half of Month indicators\n",
        "            features['isFirstHalfOfMonth'] = 1 if item['date'].day <= 15 else 0\n",
        "            features['isMidMonth'] = 1 if item['date'].day == 16 else 0 # Explicitly for 16th\n",
        "            features['isEndMonth'] = 1 if item['date'].day >= 28 else 0 # Roughly end of month\n",
        "\n",
        "        # --- New Features based on 6 Techniques ---\n",
        "\n",
        "        # 1. Frequency & Recency (Overall for each digit 0-9 for each position)\n",
        "        # Update overall_digit_counts and overall_digit_last_seen with current item's digits\n",
        "        if 'digits' in item and len(item['digits']) == 6:\n",
        "            for pos, digit in enumerate(item['digits']):\n",
        "                overall_digit_counts[pos][digit] += 1\n",
        "                overall_digit_last_seen[pos][digit] = index\n",
        "\n",
        "        # Calculate overall frequency and last seen gap for the *target* digits (if available)\n",
        "        for pos in range(6):\n",
        "            all_pos_digits_full_history = [h['digits'][pos] for h in data[:index] if 'digits' in h and len(h['digits']) > pos]\n",
        "            if all_pos_digits_full_history:\n",
        "                # If target digit exists in current item, calculate its frequency\n",
        "                if 'digits' in item and len(item['digits']) > pos:\n",
        "                    features[f'pos{pos}_overallFreq'] = all_pos_digits_full_history.count(item['digits'][pos]) / len(all_pos_digits_full_history)\n",
        "                else:\n",
        "                    features[f'pos{pos}_overallFreq'] = 0.0 # If not, it remains 0\n",
        "\n",
        "            # Last Seen Gap (number of draws since this digit last appeared at this position)\n",
        "            if 'digits' in item and len(item['digits']) > pos:\n",
        "                target_digit = item['digits'][pos]\n",
        "                last_seen_index = -1\n",
        "                for i_back in range(index - 1, -1, -1):\n",
        "                    if 'digits' in data[i_back] and len(data[i_back]['digits']) > pos and data[i_back]['digits'][pos] == target_digit:\n",
        "                        last_seen_index = i_back\n",
        "                        break\n",
        "                features[f'pos{pos}_lastSeenGap'] = index - last_seen_index if last_seen_index >= 0 else index + 1\n",
        "            else:\n",
        "                features[f'pos{pos}_lastSeenGap'] = 0.0 # Default if no digits for current item\n",
        "\n",
        "        # 2. Sum & Even/Odd Patterns (Already covered somewhat by sum & evenCount and their averages)\n",
        "        # No new explicit features here, as sum/evenCount/oddCount are already used.\n",
        "\n",
        "        # 3. Positional Relationships (Difference from adjacent digits - new)\n",
        "        if 'digits' in item and len(item['digits']) == 6:\n",
        "            for i in range(5):\n",
        "                features[f'pos{i}_diff_pos{i+1}'] = item['digits'][i] - item['digits'][i+1]\n",
        "            # New: Difference from the first digit\n",
        "            features[f'pos1_diff_pos0'] = item['digits'][1] - item['digits'][0]\n",
        "            features[f'pos2_diff_pos0'] = item['digits'][2] - item['digits'][0]\n",
        "            features[f'pos3_diff_pos0'] = item['digits'][3] - item['digits'][0]\n",
        "            features[f'pos4_diff_pos0'] = item['digits'][4] - item['digits'][0]\n",
        "            features[f'pos5_diff_pos0'] = item['digits'][5] - item['digits'][0]\n",
        "        else: # Initialize if digits are not available (e.g., dummy future row)\n",
        "            for i in range(5):\n",
        "                features[f'pos{i}_diff_pos{i+1}'] = 0.0\n",
        "            features[f'pos1_diff_pos0'] = 0.0\n",
        "            features[f'pos2_diff_pos0'] = 0.0\n",
        "            features[f'pos3_diff_pos0'] = 0.0\n",
        "            features[f'pos4_diff_pos0'] = 0.0\n",
        "            features[f'pos5_diff_pos0'] = 0.0\n",
        "\n",
        "        # 4. Cycle Theory / Repetition (Same-Date-in-History covers some of this)\n",
        "        # Already have 'last2_sameDateGapYears'. No new explicit feature from this point.\n",
        "\n",
        "        # 5. Trend Analysis (Already covered: pos_trend, sameDateTrend, avgLast3, shiftFromPrev, deltaFromAvg)\n",
        "        # No new explicit features here.\n",
        "\n",
        "        # 6. Date-Based Influence (Expanded dayOfWeek, month, dayOfMonth, weekOfYear)\n",
        "        # Added 'isFirstHalfOfMonth', 'isMidMonth', 'isEndMonth'\n",
        "\n",
        "        features['id'] = item.get('id', index)  # Ensure 'id' is always present\n",
        "        features_list.append(features)\n",
        "    return features_list\n",
        "\n",
        "features_list_full = extract_sequential_features(processed_data)\n",
        "print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Sequential Features ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(features_list_full)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "\n",
        "# ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'digits' ‡∏Ñ‡∏£‡∏ö 6 ‡∏´‡∏•‡∏±‡∏Å (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train/Test)\n",
        "clean_features_df = pd.DataFrame([f for f in features_list_full if 'digits' in f and len(f['digits']) == 6])\n",
        "if clean_features_df.empty:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ\")\n",
        "    _ml_analysis_data_cell2 = {} # Clear data if no valid features\n",
        "else:\n",
        "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Features ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ\n",
        "    columns_to_drop = ['date', 'dateStr', 'prize1', 'last2', 'last2Int', 'sum', 'evenCount', 'digits', 'id']\n",
        "    feature_cols_for_prediction = [col for col in clean_features_df.columns if col not in columns_to_drop]\n",
        "\n",
        "    # --- 2. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ML ‡πÅ‡∏•‡∏∞ Time-based Split ---\n",
        "    def prepare_ml_data_for_split(features_df_subset, feature_columns, scaler=None, fit_scaler=True):\n",
        "        df = features_df_subset.copy()\n",
        "        valid_feature_columns = [col for col in feature_columns if col in df.columns]\n",
        "        X_raw = df[valid_feature_columns].copy()\n",
        "        y_targets = {}\n",
        "        # Only extract y_targets if 'digits' column exists in the subset\n",
        "        if 'digits' in df.columns:\n",
        "            for pos in range(6): y_targets[pos] = df['digits'].apply(lambda x: x[pos])\n",
        "        else:\n",
        "            pass # y_targets will remain empty\n",
        "\n",
        "        numerical_cols = X_raw.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "        # Handle cases where numerical_cols might be empty or scaler is not needed\n",
        "        if not numerical_cols:\n",
        "            X_final = X_raw.copy() # No numerical columns to scale, return as is\n",
        "            return X_final, y_targets, scaler, numerical_cols\n",
        "\n",
        "        if fit_scaler:\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_raw[numerical_cols])\n",
        "        elif scaler: # Use existing scaler to transform\n",
        "            X_scaled = scaler.transform(X_raw[numerical_cols])\n",
        "        else: # No scaler provided and not fitting\n",
        "            X_scaled = X_raw[numerical_cols].values\n",
        "\n",
        "        # Reconstruct X_final with scaled numerical columns and original non-numerical columns\n",
        "        X_final = pd.DataFrame(X_scaled, columns=numerical_cols, index=X_raw.index)\n",
        "        # Add back any non-numerical columns if they exist and were dropped by select_dtypes\n",
        "        for col in X_raw.columns:\n",
        "            if col not in numerical_cols:\n",
        "                X_final[col] = X_raw[col]\n",
        "\n",
        "        return X_final, y_targets, scaler, numerical_cols\n",
        "\n",
        "    # --- 3. Deep Learning (LSTM) Helper Functions ---\n",
        "    n_timesteps_lstm = 3 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Lookback ‡πÉ‡∏ô LSTM (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)\n",
        "    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ epochs ‡πÅ‡∏•‡∏∞ batch_size ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LSTM\n",
        "    lstm_epochs = 50 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å\n",
        "    lstm_batch_size = 32 # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Batch\n",
        "\n",
        "    def create_lstm_sequences(X_data, y_data_dict, n_timesteps):\n",
        "        \"\"\"\n",
        "        ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• X ‡πÅ‡∏•‡∏∞ y ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö LSTM (samples, timesteps, features)\n",
        "        y_data_dict ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà X_sequences\n",
        "        \"\"\"\n",
        "        X_sequences, y_sequences_dict = [], defaultdict(list)\n",
        "\n",
        "        if X_data.empty or len(X_data) < n_timesteps:\n",
        "            return np.array([]), {} # Return empty if not enough data\n",
        "\n",
        "        X_np = X_data.values\n",
        "\n",
        "        # For LSTM, we predict the target of the *last* timestep in the sequence\n",
        "        # So, if sequence is [t-n+1, ..., t], we predict y at t.\n",
        "        for i in range(len(X_np) - n_timesteps + 1):\n",
        "            X_sequences.append(X_np[i : (i + n_timesteps), :])\n",
        "\n",
        "            if y_data_dict:\n",
        "                for pos in range(6):\n",
        "                    # Ensure the target index is valid for y_data_dict\n",
        "                    if pos in y_data_dict and (i + n_timesteps - 1) < len(y_data_dict[pos]):\n",
        "                        y_sequences_dict[pos].append(y_data_dict[pos].iloc[i + n_timesteps - 1])\n",
        "\n",
        "        X_sequences = np.array(X_sequences)\n",
        "        y_sequences_final_dict = {pos: np.array(y_list) for pos, y_list in y_sequences_dict.items() if y_list} if y_data_dict else {}\n",
        "\n",
        "        return X_sequences, y_sequences_final_dict\n",
        "\n",
        "    def create_lstm_model(input_shape):\n",
        "        model = Sequential([\n",
        "            LSTM(units=50, activation='relu', input_shape=input_shape, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(units=10, activation='softmax') # 10 possible digits (0-9)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # --- 4. Hyperparameter Tuning ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RandomForest) ---\n",
        "    print(\"\\nüî¨ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ Hyperparameter Tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (RandomForest)...\")\n",
        "    param_grid_rf = {\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'max_depth': [10, 20, None]\n",
        "    }\n",
        "    best_accuracy_rf = -1\n",
        "    best_params_rf = {}\n",
        "    best_position_accuracies_rf = {}\n",
        "\n",
        "    # Use a fixed test set for tuning to compare hyperparameters fairly\n",
        "    tuning_split_ratio = 0.8 # 80% for training, 20% for tuning validation\n",
        "    tuning_train_end_index = int(len(clean_features_df) * tuning_split_ratio)\n",
        "\n",
        "    # Ensure that tuning_X_raw_full includes 'digits' for target extraction during prepare_ml_data_for_split\n",
        "    tuning_X_raw_full, tuning_y_targets_full, tuning_scaler, tuning_numerical_cols = prepare_ml_data_for_split(\n",
        "        clean_features_df, feature_cols_for_prediction, fit_scaler=True\n",
        "    )\n",
        "\n",
        "    tuning_X_train = tuning_X_raw_full.iloc[:tuning_train_end_index]\n",
        "    tuning_y_train = {pos: tuning_y_targets_full[pos].iloc[:tuning_train_end_index] for pos in range(6)}\n",
        "\n",
        "    tuning_X_test = tuning_X_raw_full.iloc[tuning_train_end_index:]\n",
        "    tuning_y_test = {pos: tuning_y_targets_full[pos].iloc[tuning_train_end_index:] for pos in range(6)}\n",
        "\n",
        "    # Check if tuning_X_train or tuning_X_test are empty\n",
        "    if tuning_X_train.empty or tuning_X_test.empty:\n",
        "        print(\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Hyperparameter Tuning. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•.\")\n",
        "        best_params_rf = {'n_estimators': 100, 'max_depth': None} # Fallback to default\n",
        "    else:\n",
        "        for params in ParameterGrid(param_grid_rf):\n",
        "            print(f\"  > ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö n_estimators={params['n_estimators']}, max_depth={params['max_depth']}...\")\n",
        "            current_position_accuracies = {}\n",
        "            for pos in range(6):\n",
        "                model = RandomForestClassifier(random_state=42, **params)\n",
        "                model.fit(tuning_X_train, tuning_y_train[pos])\n",
        "                accuracy = model.score(tuning_X_test, tuning_y_test[pos])\n",
        "                current_position_accuracies[pos] = accuracy\n",
        "\n",
        "            # Criterion for best parameters: Average accuracy of last 3 positions (4, 5, 6)\n",
        "            avg_accuracy_last3 = np.mean([current_position_accuracies[pos] for pos in [3, 4, 5]]) # positions 4, 5, 6 are indices 3, 4, 5\n",
        "\n",
        "            if avg_accuracy_last3 > best_accuracy_rf:\n",
        "                best_accuracy_rf = avg_accuracy_last3\n",
        "                best_params_rf = params\n",
        "                best_position_accuracies_rf = current_position_accuracies.copy()\n",
        "\n",
        "        print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Hyperparameter Tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "        print(\"\\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ Tuning (RandomForest) ---\")\n",
        "        print(f\"ü•á ‡∏Ñ‡πà‡∏≤ Hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 4, 5, 6):\")\n",
        "        print(f\"  n_estimators: {best_params_rf.get('n_estimators')}\")\n",
        "        print(f\"  max_depth: {best_params_rf.get('max_depth')}\")\n",
        "        print(f\"  ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 4, 5, 6): {best_accuracy_rf:.4f}\")\n",
        "        print(\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:\")\n",
        "        for pos, acc in best_position_accuracies_rf.items():\n",
        "            print(f\"    ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos+1}: {acc:.4f}\")\n",
        "\n",
        "        # Calculate overall accuracy for 6 digits (all must be correct)\n",
        "        correct_predictions_count = 0\n",
        "        total_samples = len(tuning_X_test)\n",
        "        if total_samples > 0:\n",
        "            predictions_per_position = {}\n",
        "            for pos in range(6):\n",
        "                model = RandomForestClassifier(random_state=42, **best_params_rf)\n",
        "                model.fit(tuning_X_train, tuning_y_train[pos])\n",
        "                predictions_per_position[pos] = model.predict(tuning_X_test)\n",
        "\n",
        "            for i in range(total_samples):\n",
        "                all_correct_for_sample = True\n",
        "                for pos in range(6):\n",
        "                    if predictions_per_position[pos][i] != tuning_y_test[pos].iloc[i]:\n",
        "                        all_correct_for_sample = False\n",
        "                        break\n",
        "                if all_correct_for_sample:\n",
        "                    correct_predictions_count += 1\n",
        "            overall_accuracy_rf = correct_predictions_count / total_samples\n",
        "        else:\n",
        "            overall_accuracy_rf = 0.0\n",
        "        print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏ß‡∏° 6 ‡∏´‡∏•‡∏±‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {overall_accuracy_rf:.4f}\")\n",
        "\n",
        "\n",
        "    # --- 5. Backtesting (Walk-Forward Validation) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö RandomForest ‡πÅ‡∏•‡∏∞ LSTM) ---\n",
        "    def run_backtesting(model_type, features_df_full, feature_cols,\n",
        "                            start_train_size=300, validation_size=10, step_size=10,\n",
        "                            rf_params=None, lstm_timesteps=3, lstm_epochs=50, lstm_batch_size=32):\n",
        "\n",
        "        print(f\"\\nüìà ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ Backtesting (Walk-Forward Validation) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å ({model_type})...\")\n",
        "\n",
        "        all_position_accuracies = defaultdict(list)\n",
        "        all_overall_accuracies = []\n",
        "\n",
        "        # Ensure features_df_full is sorted by date/index\n",
        "        features_df_full = features_df_full.sort_index()\n",
        "\n",
        "        num_records = len(features_df_full)\n",
        "        current_train_start = 0\n",
        "        current_train_end = start_train_size\n",
        "\n",
        "        fold_count = 0\n",
        "        while current_train_end + validation_size <= num_records:\n",
        "            fold_count += 1\n",
        "\n",
        "            # Define train and validation sets for the current fold\n",
        "            train_data_fold = features_df_full.iloc[current_train_start:current_train_end]\n",
        "            validation_data_fold = features_df_full.iloc[current_train_end : current_train_end + validation_size]\n",
        "\n",
        "            print(f\"  > Fold {fold_count}: Training on {len(train_data_fold)} records, Validating on {len(validation_data_fold)} records...\")\n",
        "\n",
        "            # Prepare data for ML (scaling and target extraction)\n",
        "            # Fit scaler on training data of the current fold\n",
        "            X_train_raw, y_train_targets, scaler_fold, numerical_cols_fold = prepare_ml_data_for_split(\n",
        "                train_data_fold, feature_cols, fit_scaler=True\n",
        "            )\n",
        "            # Transform validation data using the same scaler\n",
        "            X_val_raw, y_val_targets, _, _ = prepare_ml_data_for_split(\n",
        "                validation_data_fold, feature_cols, scaler=scaler_fold, fit_scaler=False\n",
        "            )\n",
        "\n",
        "            if X_train_raw.empty or X_val_raw.empty:\n",
        "                print(f\"    ‚ö†Ô∏è Fold {fold_count} ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡∏Ç‡πâ‡∏≤‡∏° Fold ‡∏ô‡∏µ‡πâ\")\n",
        "                current_train_end += step_size\n",
        "                current_train_start += step_size\n",
        "                continue\n",
        "\n",
        "            current_fold_position_accuracies = {}\n",
        "            current_fold_predictions_per_position = {}\n",
        "\n",
        "            # --- Model Training and Prediction based on model_type ---\n",
        "            if model_type == 'random_forest':\n",
        "                for pos in range(6):\n",
        "                    model = RandomForestClassifier(random_state=42, **rf_params)\n",
        "                    model.fit(X_train_raw, y_train_targets[pos])\n",
        "                    accuracy = model.score(X_val_raw, y_val_targets[pos])\n",
        "                    current_fold_position_accuracies[pos] = accuracy\n",
        "                    current_fold_predictions_per_position[pos] = model.predict(X_val_raw)\n",
        "\n",
        "            elif model_type == 'lstm':\n",
        "                # Create LSTM sequences for training and validation\n",
        "                X_train_lstm, y_train_lstm_dict = create_lstm_sequences(X_train_raw, y_train_targets, lstm_timesteps)\n",
        "                X_val_lstm, y_val_lstm_dict = create_lstm_sequences(X_val_raw, y_val_targets, lstm_timesteps)\n",
        "\n",
        "                if X_train_lstm.shape[0] == 0 or X_val_lstm.shape[0] == 0:\n",
        "                    print(f\"    ‚ö†Ô∏è Fold {fold_count} ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• LSTM Sequence ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡∏Ç‡πâ‡∏≤‡∏° Fold ‡∏ô‡∏µ‡πâ\")\n",
        "                    current_train_end += step_size\n",
        "                    current_train_start += step_size\n",
        "                    continue\n",
        "\n",
        "                # Check if y_train_lstm_dict and y_val_lstm_dict are populated for all positions\n",
        "                # This ensures target data exists for training and validation\n",
        "                if not y_train_lstm_dict or any(y_train_lstm_dict[pos].shape[0] == 0 for pos in range(6) if pos in y_train_lstm_dict) or \\\n",
        "                   not y_val_lstm_dict or any(y_val_lstm_dict[pos].shape[0] == 0 for pos in range(6) if pos in y_val_lstm_dict):\n",
        "                        print(f\"    ‚ö†Ô∏è Fold {fold_count} ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Target ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LSTM ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏Ç‡πâ‡∏≤‡∏° Fold ‡∏ô‡∏µ‡πâ\")\n",
        "                        current_train_end += step_size\n",
        "                        current_train_start += step_size\n",
        "                        continue\n",
        "\n",
        "\n",
        "                for pos in range(6):\n",
        "                    # Check explicitly if target for current pos exists and is not empty\n",
        "                    if pos not in y_train_lstm_dict or y_train_lstm_dict[pos].shape[0] == 0 or \\\n",
        "                       pos not in y_val_lstm_dict or y_val_lstm_dict[pos].shape[0] == 0:\n",
        "                        print(f\"    ‚ö†Ô∏è Fold {fold_count} ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Target ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos+1} ‡πÉ‡∏ô LSTM. ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LSTM.\")\n",
        "                        current_fold_position_accuracies[pos] = 0.0 # Default accuracy\n",
        "                        current_fold_predictions_per_position[pos] = np.array([]) # Empty predictions\n",
        "                        continue\n",
        "\n",
        "                    lstm_model = create_lstm_model(input_shape=(lstm_timesteps, X_train_lstm.shape[2]))\n",
        "                    lstm_model.fit(\n",
        "                        X_train_lstm, y_train_lstm_dict[pos],\n",
        "                        epochs=lstm_epochs,\n",
        "                        batch_size=lstm_batch_size,\n",
        "                        verbose=0 # Suppress output during training\n",
        "                    )\n",
        "                    loss, accuracy = lstm_model.evaluate(X_val_lstm, y_val_lstm_dict[pos], verbose=0)\n",
        "                    current_fold_position_accuracies[pos] = accuracy\n",
        "                    current_fold_predictions_per_position[pos] = np.argmax(lstm_model.predict(X_val_lstm), axis=1)\n",
        "\n",
        "            # Store position accuracies for this fold\n",
        "            for pos, acc in current_fold_position_accuracies.items():\n",
        "                all_position_accuracies[pos].append(acc)\n",
        "\n",
        "            # Calculate overall 6-digit accuracy for this fold\n",
        "            correct_predictions_count_fold = 0\n",
        "            total_samples_fold = X_val_lstm.shape[0] if model_type == 'lstm' else len(validation_data_fold)\n",
        "\n",
        "            if total_samples_fold > 0:\n",
        "                for i in range(total_samples_fold):\n",
        "                    all_correct_for_sample = True\n",
        "                    for pos in range(6):\n",
        "                        # Corrected target lookup for LSTM validation\n",
        "                        if model_type == 'lstm':\n",
        "                            # Ensure index 'i' exists in y_val_lstm_dict[pos]\n",
        "                            if pos not in y_val_lstm_dict or i >= len(y_val_lstm_dict[pos]):\n",
        "                                all_correct_for_sample = False\n",
        "                                break\n",
        "                            actual_digit = y_val_lstm_dict[pos][i]\n",
        "                        else: # RandomForest\n",
        "                            actual_digit = y_val_targets[pos].iloc[i]\n",
        "\n",
        "\n",
        "                        if pos not in current_fold_predictions_per_position or \\\n",
        "                           i >= len(current_fold_predictions_per_position[pos]):\n",
        "                            all_correct_for_sample = False\n",
        "                            break\n",
        "\n",
        "                        predicted_digit = current_fold_predictions_per_position[pos][i]\n",
        "\n",
        "                        if predicted_digit != actual_digit:\n",
        "                            all_correct_for_sample = False\n",
        "                            break\n",
        "                    if all_correct_for_sample:\n",
        "                        correct_predictions_count_fold += 1\n",
        "                overall_accuracy_fold = correct_predictions_count_fold / total_samples_fold\n",
        "            else:\n",
        "                overall_accuracy_fold = 0.0\n",
        "            all_overall_accuracies.append(overall_accuracy_fold)\n",
        "\n",
        "            # Move to the next fold\n",
        "            current_train_end += step_size\n",
        "            current_train_start += step_size # Slide window\n",
        "\n",
        "        print(f\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ({fold_count} Folds)\")\n",
        "\n",
        "        print(\"\\n--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Backtesting ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ---\")\n",
        "        avg_position_accuracies = {pos: np.mean(acc_list) for pos, acc_list in all_position_accuracies.items() if acc_list}\n",
        "        for pos in range(6):\n",
        "            print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos+1}): {avg_position_accuracies.get(pos, 0.0):.4f}\")\n",
        "\n",
        "        avg_overall_accuracy = np.mean(all_overall_accuracies) if all_overall_accuracies else 0.0\n",
        "        print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏ß‡∏° 6 ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å): {avg_overall_accuracy:.4f}\")\n",
        "\n",
        "        return avg_position_accuracies, avg_overall_accuracy\n",
        "\n",
        "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Backtesting ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RandomForest (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô)\n",
        "    if not best_params_rf: # If tuning failed or no data for tuning\n",
        "        print(\"\\n‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ best_params_rf ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Tuning, ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backtesting RandomForest.\")\n",
        "        rf_params_for_backtest = {'n_estimators': 100, 'max_depth': None}\n",
        "    else:\n",
        "        rf_params_for_backtest = best_params_rf\n",
        "\n",
        "    rf_avg_pos_acc, rf_avg_overall_acc = run_backtesting(\n",
        "        model_type='random_forest',\n",
        "        features_df_full=clean_features_df,\n",
        "        feature_cols=feature_cols_for_prediction,\n",
        "        rf_params=rf_params_for_backtest\n",
        "    )\n",
        "\n",
        "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Backtesting ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LSTM\n",
        "    lstm_avg_pos_acc, lstm_avg_overall_acc = run_backtesting(\n",
        "        model_type='lstm',\n",
        "        features_df_full=clean_features_df,\n",
        "        feature_cols=feature_cols_for_prediction,\n",
        "        lstm_timesteps=n_timesteps_lstm, # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ n_timesteps_lstm ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\n",
        "        lstm_epochs=lstm_epochs, # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô\n",
        "        lstm_batch_size=lstm_batch_size # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô\n",
        "    )\n",
        "\n",
        "    # --- 6. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ) ---\n",
        "    print(\"\\n--- üî¢ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ) ---\")\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏∂‡∏á‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
        "    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Features\n",
        "\n",
        "    if not processed_data:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô processed_data ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏î‡πâ\")\n",
        "        next_draw_date = datetime.now() # Fallback date\n",
        "        rf_predicted_digits = ['N/A'] * 6\n",
        "        rf_confidence_per_pos = [0.0] * 6\n",
        "        rf_position_probabilities = {pos: [] for pos in range(6)}\n",
        "        lstm_predicted_digits = ['N/A'] * 6\n",
        "        lstm_confidence_per_pos = [0.0] * 6\n",
        "        lstm_position_probabilities = {pos: [] for pos in range(6)}\n",
        "        predicted_last2 = 'N/A'\n",
        "        last2_confidence = 0.0\n",
        "        last2_probabilities_top5 = []\n",
        "\n",
        "    else:\n",
        "        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏´‡∏£‡∏∑‡∏≠ 16 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)\n",
        "        last_date = processed_data[-1]['date']\n",
        "        if last_date.day == 1:\n",
        "            next_draw_date = last_date.replace(day=16)\n",
        "        elif last_date.day == 16:\n",
        "            if last_date.month == 12:\n",
        "                next_draw_date = last_date.replace(year=last_date.year + 1, month=1, day=1)\n",
        "            else:\n",
        "                next_draw_date = last_date.replace(month=last_date.month + 1, day=1)\n",
        "        else: # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏´‡∏£‡∏∑‡∏≠ 16 (‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á)\n",
        "            # ‡∏´‡∏≤ 1 ‡∏´‡∏£‡∏∑‡∏≠ 16 ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\n",
        "            temp_date = last_date + timedelta(days=1)\n",
        "            while True:\n",
        "                if temp_date.day == 1 or temp_date.day == 16:\n",
        "                    next_draw_date = temp_date\n",
        "                    break\n",
        "                temp_date += timedelta(days=1)\n",
        "\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dummy data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏à‡∏£‡∏¥‡∏á)\n",
        "        # Penting: 'digits', 'sum', 'evenCount', 'oddCount', 'last2Int' need to be present even as dummies\n",
        "        # so that extract_sequential_features can process them without errors and include these feature columns.\n",
        "        next_draw_dummy_data = {\n",
        "            'date': next_draw_date,\n",
        "            'dateStr': next_draw_date.strftime('%Y-%m-%d'),\n",
        "            'prize1': '', 'last2': '', 'last2Int': 0,\n",
        "            'sum': 0, 'evenCount': 0, 'oddCount': 0, # Explicitly include oddCount\n",
        "            'digits': [0,0,0,0,0,0] # Dummy digits for feature calculation, won't affect prediction target\n",
        "        }\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á features_list_for_prediction ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° dummy data ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢\n",
        "        processed_data_with_dummy = processed_data + [next_draw_dummy_data]\n",
        "        features_list_for_prediction_context = extract_sequential_features(processed_data_with_dummy)\n",
        "\n",
        "        # ‡∏î‡∏∂‡∏á features ‡∏Ç‡∏≠‡∏á‡∏á‡∏ß‡∏î dummy (‡∏á‡∏ß‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô features_list_for_prediction_context)\n",
        "        future_features_row_data = features_list_for_prediction_context[-1]\n",
        "        future_features_df = pd.DataFrame([future_features_row_data])\n",
        "\n",
        "        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡πÉ‡∏ä‡πâ scaler ‡∏ó‡∏µ‡πà fit ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ target)\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ feature_cols_for_prediction ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ numerical_cols\n",
        "        X_full_scaled, y_full_targets, scaler_for_prediction, numerical_cols_for_scaling = prepare_ml_data_for_split(\n",
        "            clean_features_df, feature_cols_for_prediction, fit_scaler=True # clean_features_df ‡∏°‡∏µ digits ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å\n",
        "        )\n",
        "\n",
        "        # X ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ scaler_for_prediction ‡∏°‡∏≤‡πÅ‡∏õ‡∏•‡∏á\n",
        "        X_predict_rf, _, _, _ = prepare_ml_data_for_split(\n",
        "            future_features_df, feature_cols_for_prediction, scaler=scaler_for_prediction, fit_scaler=False\n",
        "        )\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ RandomForest (‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ best_params_rf)\n",
        "        rf_predicted_digits = []\n",
        "        rf_confidence_per_pos = []\n",
        "        rf_position_probabilities = {}\n",
        "\n",
        "        for pos in range(6):\n",
        "            model = RandomForestClassifier(random_state=42, **best_params_rf)\n",
        "            # Train model on all available data for final prediction\n",
        "            # Ensure X_full_scaled is used for training, as it's the scaled version of clean_features_df\n",
        "            # y_full_targets contains the actual digits\n",
        "            model.fit(X_full_scaled, y_full_targets[pos])\n",
        "\n",
        "            # Ensure X_predict_rf is aligned and has the same columns as X_full_scaled\n",
        "            # This should be handled by prepare_ml_data_for_split\n",
        "            proba = model.predict_proba(X_predict_rf)[0]\n",
        "            predicted_digit = np.argmax(proba)\n",
        "            confidence = proba[predicted_digit] * 100 # Convert to percentage\n",
        "\n",
        "            rf_predicted_digits.append(predicted_digit)\n",
        "            rf_confidence_per_pos.append(confidence)\n",
        "            rf_position_probabilities[pos] = sorted(zip(range(10), proba), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        rf_overall_confidence = np.mean(rf_confidence_per_pos) # Simple average for now\n",
        "\n",
        "        print(f\"‡∏ä‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà 1 (RandomForest - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ): **{''.join(map(str, rf_predicted_digits))}**\")\n",
        "        print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏£‡∏ß‡∏°: {rf_overall_confidence:.1f}%\")\n",
        "        print(\"  ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å:\")\n",
        "        for pos, digit in enumerate(rf_predicted_digits):\n",
        "            print(f\"    ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {pos+1}: ‡πÄ‡∏•‡∏Ç {digit} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {rf_confidence_per_pos[pos]:.0f}%)\")\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ LSTM (‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ n_timesteps_lstm)\n",
        "        lstm_predicted_digits = []\n",
        "        lstm_confidence_per_pos = []\n",
        "        lstm_position_probabilities = {}\n",
        "\n",
        "        # ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á X_predict_lstm ‡∏à‡∏≤‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î n_timesteps_lstm ‡∏á‡∏ß‡∏î‡∏à‡∏≤‡∏Å X_full_scaled\n",
        "        if len(X_full_scaled) >= n_timesteps_lstm:\n",
        "            # ‡∏î‡∏∂‡∏á n_timesteps_lstm ‡∏á‡∏ß‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å X_full_scaled (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πÄ‡∏Å‡∏•‡πÅ‡∏•‡πâ‡∏ß)\n",
        "            X_predict_lstm_sequence_input = X_full_scaled.iloc[-(n_timesteps_lstm):].values.reshape(1, n_timesteps_lstm, X_full_scaled.shape[1])\n",
        "\n",
        "            # create_lstm_sequences ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (y_data_dict ‡πÄ‡∏õ‡πá‡∏ô {} ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• target)\n",
        "            # ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö predict, ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á sequence ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n",
        "            # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á reshape ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single prediction\n",
        "\n",
        "            if X_predict_lstm_sequence_input.shape[0] > 0: # Check if sequence was successfully created\n",
        "                for pos in range(6):\n",
        "                    # Train LSTM model on all available data for final prediction\n",
        "                    X_all_lstm, y_all_lstm_dict = create_lstm_sequences(\n",
        "                        X_full_scaled, # ‡πÉ‡∏ä‡πâ X_full_scaled ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏™‡πÄ‡∏Å‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
        "                        {p: clean_features_df['digits'].apply(lambda x: x[p]) for p in range(6)}, # target y ‡∏à‡∏≤‡∏Å clean_features_df\n",
        "                        n_timesteps_lstm\n",
        "                    )\n",
        "                    if X_all_lstm.shape[0] == 0:\n",
        "                        print(f\"    ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• LSTM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos+1} ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
        "                        lstm_predicted_digits.append('N/A')\n",
        "                        lstm_confidence_per_pos.append(0.0)\n",
        "                        lstm_position_probabilities[pos] = []\n",
        "                        continue\n",
        "\n",
        "                    lstm_model_final = create_lstm_model(input_shape=(n_timesteps_lstm, X_all_lstm.shape[2]))\n",
        "                    lstm_model_final.fit(\n",
        "                        X_all_lstm, y_all_lstm_dict[pos],\n",
        "                        epochs=lstm_epochs, # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ lstm_epochs**\n",
        "                        batch_size=lstm_batch_size, # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ lstm_batch_size**\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                    proba = lstm_model_final.predict(X_predict_lstm_sequence_input)[0]\n",
        "                    predicted_digit = np.argmax(proba)\n",
        "                    confidence = proba[predicted_digit] * 100\n",
        "\n",
        "                    lstm_predicted_digits.append(predicted_digit)\n",
        "                    lstm_confidence_per_pos.append(confidence)\n",
        "                    lstm_position_probabilities[pos] = sorted(zip(range(10), proba), key=lambda x: x[1], reverse=True)\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á LSTM sequence ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏î‡πâ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠).\")\n",
        "                lstm_predicted_digits = ['N/A'] * 6\n",
        "                lstm_confidence_per_pos = [0.0] * 6\n",
        "                lstm_position_probabilities = {pos: [] for pos in range(6)}\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô clean_features_df ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á LSTM sequences (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ n_timesteps_lstm ‡∏á‡∏ß‡∏î).\")\n",
        "            lstm_predicted_digits = ['N/A'] * 6\n",
        "            lstm_confidence_per_pos = [0.0] * 6\n",
        "            lstm_position_probabilities = {pos: [] for pos in range(6)}\n",
        "\n",
        "\n",
        "        lstm_overall_confidence = np.mean([c for c in lstm_confidence_per_pos if c != 0.0]) if any(c != 0.0 for c in lstm_confidence_per_pos) else 0.0\n",
        "\n",
        "        print(f\"\\n‡∏ä‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà 2 (LSTM - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ): **{''.join(map(str, lstm_predicted_digits))}**\")\n",
        "        print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏£‡∏ß‡∏°: {lstm_overall_confidence:.1f}%\")\n",
        "        print(\"  ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å:\")\n",
        "        for pos, digit in enumerate(lstm_predicted_digits):\n",
        "            print(f\"    ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {pos+1}: ‡πÄ‡∏•‡∏Ç {digit} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {lstm_confidence_per_pos[pos]:.0f}%)\")\n",
        "\n",
        "\n",
        "        # --- 7. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ) ---\n",
        "        print(\"\\n--- üî¢ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ) ---\")\n",
        "\n",
        "        # Reuse the future_features_df created for 6-digit prediction\n",
        "        # The 'last2Int' target is independent of 'digits' for prize1\n",
        "\n",
        "        # Train a separate RandomForest model for last2Int\n",
        "        rf_model_last2 = RandomForestClassifier(random_state=42, **best_params_rf) # Reuse best RF params\n",
        "\n",
        "        # Prepare all data for training the last2 model\n",
        "        # For 'last2Int', we need a target for it.\n",
        "        # Ensure 'last2Int' is present in clean_features_df for training\n",
        "        if 'last2Int' not in clean_features_df.columns:\n",
        "            print(\"‚ùå 'last2Int' column not found in clean_features_df. Cannot train last2 prediction model.\")\n",
        "            predicted_last2 = 'N/A'\n",
        "            last2_confidence = 0.0\n",
        "            last2_probabilities_top5 = []\n",
        "        else:\n",
        "            y_all_last2 = clean_features_df['last2Int']\n",
        "\n",
        "            # Make sure X_full_scaled (which is X_all_rf in previous code block) is aligned\n",
        "            # Use the same scaler and feature columns as for 6-digit prediction\n",
        "            # The 'prepare_ml_data_for_split' function ensures numerical features are scaled correctly\n",
        "            # We don't need 'digits' in feature_cols_for_prediction when predicting 'last2Int'\n",
        "            rf_model_last2.fit(X_full_scaled, y_all_last2)\n",
        "\n",
        "            # Predict last2 for the future features\n",
        "            proba_last2 = rf_model_last2.predict_proba(X_predict_rf)[0]\n",
        "            predicted_last2_int = np.argmax(proba_last2)\n",
        "            last2_confidence = proba_last2[predicted_last2_int] * 100\n",
        "\n",
        "            predicted_last2 = str(predicted_last2_int).zfill(2)\n",
        "\n",
        "            # Get top 5 probabilities for last2\n",
        "            # Ensure that the range is appropriate for your 'last2Int' values (0-99)\n",
        "            last2_probabilities_top5 = sorted(zip(range(len(proba_last2)), proba_last2), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "\n",
        "        print(f\"‡∏ä‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß (RandomForest): **{predicted_last2}**\")\n",
        "        print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {last2_confidence:.1f}%\")\n",
        "        print(\"  5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß:\")\n",
        "        for num, prob in last2_probabilities_top5:\n",
        "            print(f\"    ‡πÄ‡∏•‡∏Ç {str(num).zfill(2)}: {prob*100:.1f}%\")\n",
        "\n",
        "        # --- 8. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ---\n",
        "        _ml_analysis_data_cell2 = {\n",
        "            'rf_best_params': best_params_rf,\n",
        "            'rf_avg_position_accuracies_backtest': rf_avg_pos_acc,\n",
        "            'rf_overall_accuracy_backtest': rf_avg_overall_acc,\n",
        "            'lstm_avg_position_accuracies_backtest': lstm_avg_pos_acc,\n",
        "            'lstm_overall_accuracy_backtest': lstm_avg_overall_acc,\n",
        "            'predicted_prize1_rf': ''.join(map(str, rf_predicted_digits)),\n",
        "            'confidence_prize1_rf': rf_overall_confidence,\n",
        "            'position_probs_prize1_rf': rf_position_probabilities,\n",
        "            'predicted_prize1_lstm': ''.join(map(str, [d for d in lstm_predicted_digits if isinstance(d, int)])), # Filter out 'N/A'\n",
        "            'confidence_prize1_lstm': lstm_overall_confidence,\n",
        "            'position_probs_prize1_lstm': lstm_position_probabilities,\n",
        "            'predicted_last2_rf': predicted_last2,\n",
        "            'confidence_last2_rf': last2_confidence,\n",
        "            'top5_probs_last2_rf': last2_probabilities_top5,\n",
        "            'next_draw_date': next_draw_date.strftime('%Y-%m-%d'),\n",
        "            'feature_columns_used': feature_cols_for_prediction,\n",
        "            'scaler_for_prediction': scaler_for_prediction\n",
        "        }\n",
        "\n",
        "    print(\"\\n--- ‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å (Feature Engineering ‡πÅ‡∏•‡∏∞ Model Logic) ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "_Db6Y_82HIM0",
        "outputId": "a76d0e6a-247c-46ab-a15d-bec4968250ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Sequential Features...\n",
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Sequential Features ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 560 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "üî¨ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ Hyperparameter Tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (RandomForest)...\n",
            "  > ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö n_estimators=50, max_depth=10...\n",
            "  > ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö n_estimators=100, max_depth=10...\n",
            "  > ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö n_estimators=150, max_depth=10...\n",
            "  > ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö n_estimators=200, max_depth=10...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2448085169.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuning_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuning_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning_y_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mcurrent_position_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞ Feature Analysis ---\n",
        "\n",
        "print(\"--- ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞ Feature Analysis ---\")\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 2 ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà\n",
        "if '_predictions_cell2' not in globals() or '_analysis_results_cell2' not in globals():\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 2 ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 2 ‡∏Å‡πà‡∏≠‡∏ô\")\n",
        "else:\n",
        "    predictions = _predictions_cell2\n",
        "    analysis_results = _analysis_results_cell2\n",
        "    features_data = features # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ 'features' ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï global ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ã‡∏•‡∏•‡πå 2\n",
        "\n",
        "    print(\"\\n--- üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà 2 ---\")\n",
        "    print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß: {analysis_results['totalRecords']} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "    print(f\"‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà {analysis_results['dataRange']['from']} ‡∏ñ‡∏∂‡∏á {analysis_results['dataRange']['to']}\")\n",
        "    print(f\"‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {analysis_results['qualityScore']:.2f}%\")\n",
        "    print(\"‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå 1 ‡πÅ‡∏•‡∏∞ 2:\")\n",
        "    for imp in analysis_results['improvements']:\n",
        "        print(f\"  {imp}\")\n",
        "\n",
        "    print(\"\\n--- üîç ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Features (‡∏à‡∏≤‡∏Å 'features' ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£) ---\")\n",
        "    print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Features ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á: {len(features_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "    print(\"\\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Feature ‡∏Ç‡∏≠‡∏á‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ):\")\n",
        "    if features_data:\n",
        "        last_feature_set = features_data[-1]\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Features ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "        print(\"  Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å:\")\n",
        "        for pos in range(6):\n",
        "            print(f\"    pos{pos}_lastValue: {last_feature_set.get(f'pos{pos}_lastValue', 'N/A')}\")\n",
        "            print(f\"    pos{pos}_trend: {last_feature_set.get(f'pos{pos}_trend', 'N/A')}\")\n",
        "            print(f\"    pos{pos}_stdDev: {last_feature_set.get(f'pos{pos}_stdDev', 'N/A'):.2f}\")\n",
        "        print(\"  Features ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:\")\n",
        "        print(f\"    avgSum: {last_feature_set.get('avgSum', 'N/A'):.2f}\")\n",
        "        print(f\"    avgEvenCount: {last_feature_set.get('avgEvenCount', 'N/A'):.2f}\")\n",
        "        print(\"  Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á:\")\n",
        "        print(f\"    last2_lastValue: {last_feature_set.get('last2_lastValue', 'N/A')}\")\n",
        "        print(f\"    last2_frequency_in_history: {last_feature_set.get('last2_frequency_in_history', 'N/A')}\")\n",
        "        print(f\"    last2_gap: {last_feature_set.get('last2_gap', 'N/A')}\")\n",
        "        print(\"  Features ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:\")\n",
        "        print(f\"    dayOfWeek: {last_feature_set.get('dayOfWeek', 'N/A')}\")\n",
        "        print(f\"    month: {last_feature_set.get('month', 'N/A')}\")\n",
        "        print(f\"    dayOfMonth: {last_feature_set.get('dayOfMonth', 'N/A')}\")\n",
        "        print(f\"    weekOfYear: {last_feature_set.get('weekOfYear', 'N/A')}\")\n",
        "    else:\n",
        "        print(\"  ‡πÑ‡∏°‡πà‡∏°‡∏µ Features ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)\")\n",
        "\n",
        "    print(\"\\n--- üìà ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ (‡∏à‡∏≤‡∏Å '_predictions_cell2') ---\")\n",
        "\n",
        "    print(\"\\n--- üî¢ ‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏à‡∏≤‡∏Å predict_sequential_6_digits) ---\")\n",
        "    if predictions['sixDigit']['predictions']:\n",
        "        for i, pred in enumerate(predictions['sixDigit']['predictions']):\n",
        "            print(f\"‡∏ä‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà {i+1}: **{pred['sequence']}**\")\n",
        "            print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {pred['confidence']:.1%}\")\n",
        "            print(\"  ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å:\")\n",
        "            for detail in pred['breakdown']:\n",
        "                print(f\"    ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {detail['position']+1}: ‡πÄ‡∏•‡∏Ç {detail['digit']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {detail['confidence']:.0%})\")\n",
        "\n",
        "        print(\"\\n--- ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (Position Analysis) ---\")\n",
        "        for pos_analysis in predictions['sixDigit']['positionAnalysis']:\n",
        "            print(f\"‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos_analysis['position']+1} (Best Guess: {pos_analysis['bestGuess']}):\")\n",
        "            for cand in pos_analysis['candidates']:\n",
        "                print(f\"  - ‡πÄ‡∏•‡∏Ç {cand['digit']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: {cand['probability']:.2%})\")\n",
        "    else:\n",
        "        print(\"  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)\")\n",
        "\n",
        "    print(\"\\n--- ‚ö° ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏à‡∏≤‡∏Å predict_last2_digits) ---\")\n",
        "    if predictions['twoDigit']['predictions']:\n",
        "        print(\"üî• Top Predictions:\")\n",
        "        for i, pred in enumerate(predictions['twoDigit']['predictions']):\n",
        "            print(f\"  {i+1}. **{pred['value']}** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {pred['confidence']:.1%} ({pred['reason']})\")\n",
        "\n",
        "        print(\"\\nüìä Pattern Analysis:\")\n",
        "        analysis_2d = predictions['twoDigit']['analysis']\n",
        "        print(f\"  üéØ ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö: {analysis_2d['totalUniquePairs']} ‡∏Ñ‡∏π‡πà\")\n",
        "        if analysis_2d['mostFrequent']:\n",
        "            print(f\"  üî• ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏∏‡∏î (‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô): {analysis_2d['mostFrequent']['pair']} (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {analysis_2d['mostFrequent']['score']:.1f})\")\n",
        "        print(f\"  ‚è∞ ‡∏ä‡πà‡∏ß‡∏á‡∏´‡πà‡∏≤‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {analysis_2d['longestCurrentGap']} ‡∏á‡∏ß‡∏î\")\n",
        "        print(f\"  Pattern Trends (‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 5 ‡∏á‡∏ß‡∏î):\")\n",
        "        print(f\"    Sum Mod 10: {' ‚Üí '.join(map(str, analysis_2d['patterns']['sumMod10Trend']))}\")\n",
        "        print(f\"    Even/Odd: {' ‚Üí '.join(['‡∏Ñ‡∏µ‡πà' if x == 1 else '‡∏Ñ‡∏π‡πà' for x in analysis_2d['patterns']['evenOddTrend']])}\")\n",
        "        print(f\"  ü•∂ ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà '‡πÄ‡∏¢‡πá‡∏ô' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ô‡∏≤‡∏ô: {', '.join(analysis_2d['coldPairs'])}\")\n",
        "    else:\n",
        "        print(\"  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)\")\n",
        "\n",
        "    print(\"\\n--- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xVenEQzYzBBZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}