{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FHLtSRGSVB1VYr0cL7nZUrFJ9hR4pXds",
      "authorship_tag": "ABX9TyME5zQkyzOHaySKoBayXlqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treekeaw1/-mana-bento-web/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from io import StringIO, BytesIO\n",
        "from google.colab import files # Library for file upload button in Google Colab\n",
        "from sklearn.model_selection import train_test_split # For splitting data into training and test sets\n",
        "from sklearn.ensemble import RandomForestClassifier # Our chosen ML model\n",
        "from sklearn.metrics import accuracy_score # To evaluate model performance\n",
        "import numpy as np # For numerical operations, especially with probabilities\n",
        "\n",
        "class LotteryPatternAnalyzer:\n",
        "    # Modified: Added window_size to init, default to 6\n",
        "    def __init__(self, window_size=6):\n",
        "        self.data = []\n",
        "        self.analysis_results = []\n",
        "        self.summary = None\n",
        "        self.ml_models = {} # Dictionary to store trained ML models for each digit position\n",
        "        self.window_size = window_size # New attribute to store the historical window size\n",
        "\n",
        "        # Mapping for detailed position descriptions in a combined 8-digit string (left to right)\n",
        "        # This mapping assumes the combined string is 'two_digit' (indices 0-1) + 'six_digit' (indices 2-7)\n",
        "        self.combined_digit_positions = {\n",
        "            0: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            1: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)',\n",
        "            2: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏™‡∏ô)',\n",
        "            3: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô)',\n",
        "            4: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)',\n",
        "            5: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢)',\n",
        "            6: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            7: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)'\n",
        "        }\n",
        "\n",
        "    def load_data_from_file(self):\n",
        "        \"\"\"\n",
        "        Allows the user to select a CSV file, loads the data, and performs cleaning.\n",
        "        - Displays a button for file selection.\n",
        "        - Skips the first row (if it's empty or not actual CSV data).\n",
        "        - Validates and pads numbers with leading zeros.\n",
        "        - Converts dates to datetime objects.\n",
        "        - Sorts data from oldest to newest for chronological analysis.\n",
        "        \"\"\"\n",
        "        print(\"üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡πÄ‡∏ä‡πà‡∏ô '‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv')\")\n",
        "\n",
        "        # Display the file upload button in Google Colab\n",
        "        uploaded = files.upload() # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£\")\n",
        "            return False\n",
        "\n",
        "        # Get the filename and binary content of the uploaded file\n",
        "        # files.upload() typically returns a dictionary with one entry if a single file is selected\n",
        "        filepath = list(uploaded.keys())[0] # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î\n",
        "        file_content = uploaded[filepath] # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\n",
        "\n",
        "        print(f\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {filepath}\")\n",
        "        try:\n",
        "            # Read CSV file, skipping the first row (index 0) and using utf-8-sig encoding for Thai characters\n",
        "            # header=0 means the first row after skipping is treated as the header\n",
        "            df = pd.read_csv(BytesIO(file_content), encoding='utf-8-sig', skiprows=[0], header=0)\n",
        "\n",
        "            # Define mapping for expected Thai column names to standard English names\n",
        "            thai_col_mapping = {\n",
        "                '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': 'date',\n",
        "                '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)': 'six_digit',\n",
        "                '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': 'two_digit',\n",
        "                # Add other possible Thai column names here if your file has different variations\n",
        "            }\n",
        "\n",
        "            # Rename columns in the DataFrame. Normalize column names in the file first (strip spaces, lowercase)\n",
        "            # to handle minor variations in spacing or casing.\n",
        "            normalized_columns_map = {col.strip().lower(): col for col in df.columns}\n",
        "            rename_dict = {}\n",
        "            for thai_name, eng_name in thai_col_mapping.items():\n",
        "                if thai_name.lower() in normalized_columns_map:\n",
        "                    rename_dict[normalized_columns_map[thai_name.lower()]] = eng_name\n",
        "            df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "            # Check if all required columns are present after renaming\n",
        "            required_cols = ['date', 'six_digit', 'two_digit']\n",
        "            if not all(col in df.columns for col in required_cols):\n",
        "                missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "                raise ValueError(f\"‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join(missing_cols)}. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á' ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)\")\n",
        "\n",
        "            # Convert 'date' column to datetime objects\n",
        "            # 'errors=coerce' will turn invalid date formats into NaT (Not a Time)\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "            # Remove rows with NaT in the 'date' column (invalid or incomplete dates)\n",
        "            df.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "            # --- Critical fix for handling missing values and float conversion ---\n",
        "            # 1. Attempt to convert 'six_digit' and 'two_digit' columns to numeric.\n",
        "            #    Any non-numeric values (including empty strings or 'nan' strings) will become NaN.\n",
        "            df['six_digit'] = pd.to_numeric(df['six_digit'], errors='coerce')\n",
        "            df['two_digit'] = pd.to_numeric(df['two_digit'], errors='coerce')\n",
        "\n",
        "            # 2. Remove rows where 'six_digit' or 'two_digit' are NaN (meaning they were initially empty or invalid numbers).\n",
        "            df.dropna(subset=['six_digit', 'two_digit'], inplace=True)\n",
        "\n",
        "            # 3. Convert numbers to integers (to remove .0 decimals), then to strings, and pad with leading zeros.\n",
        "            df['six_digit'] = df['six_digit'].astype(int).astype(str).str.zfill(6)\n",
        "            df['two_digit'] = df['two_digit'].astype(int).astype(str).str.zfill(2)\n",
        "            # --- End of critical fix ---\n",
        "\n",
        "            # Convert DataFrame to a list of dictionaries, which aligns with the class's internal data structure.\n",
        "            # Sort data chronologically from oldest to newest (Ascending)\n",
        "            # This ensures that the analysis proceeds from past results towards the present.\n",
        "            self.data = df.sort_values(by='date', ascending=True).to_dict('records')\n",
        "            print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(self.data)} ‡πÅ‡∏ñ‡∏ß\")\n",
        "            return True\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(\"‚ùå ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå\")\n",
        "            return False\n",
        "        except ValueError as e:\n",
        "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå CSV: {e}\")\n",
        "            print(\"‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á' ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏ì‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_flow_pattern(self, data):\n",
        "        \"\"\"\n",
        "        Analyzes the 'Flow Pattern' based on the user's new definition.\n",
        "        For each 'target row' (the (window_size + 1)th row in a chronological window), it checks\n",
        "        how its digits are derived/formed from the preceding 'window_size' 'historical rows'.\n",
        "        It records the 'pattern of occurrence, flow, and arrangement' by detailing\n",
        "        where each digit of the target row originated from in the historical rows.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Modified: Loop through the data starting from self.window_size index\n",
        "        for i in range(self.window_size, len(data)):\n",
        "            current_target_row = data[i] # This is the \"target row\" (the outcome to be explained)\n",
        "            preceding_history_rows = data[i-self.window_size:i] # These are the historical rows\n",
        "\n",
        "            # Combine the 2-digit lower and 6-digit first prize numbers into a single 8-digit string\n",
        "            # Example: R1=\"522630\", L2=\"37\" -> combined \"37522630\"\n",
        "            # Digits are then processed left-to-right from this combined string for the target row.\n",
        "            target_digits_combined_lr = current_target_row['two_digit'] + current_target_row['six_digit']\n",
        "\n",
        "            # Store analysis for each digit of the target row\n",
        "            digit_flow_analysis = []\n",
        "\n",
        "            # Iterate through each digit of the current_target_row (8 digits, left to right)\n",
        "            for target_digit_idx, target_digit_value in enumerate(target_digits_combined_lr):\n",
        "                sources_in_past_rows = [] # To store details of where this target digit was found\n",
        "                found_count_for_target_digit = 0 # Count of how many times this specific target digit appeared\n",
        "\n",
        "                # Check this target_digit_value against all digits in each of the preceding history rows\n",
        "                for past_row_offset, past_row in enumerate(preceding_history_rows):\n",
        "                    # Combine digits of the historical row for searching\n",
        "                    past_row_combined_digits_lr = past_row['two_digit'] + past_row['six_digit']\n",
        "\n",
        "                    # Iterate through each digit in the current historical row to find matches\n",
        "                    for past_digit_idx, past_digit_value in enumerate(past_row_combined_digits_lr):\n",
        "                        if past_digit_value == target_digit_value:\n",
        "                            found_count_for_target_digit += 1\n",
        "                            sources_in_past_rows.append({\n",
        "                                # Modified: Use self.window_size for relative index\n",
        "                                'source_row_relative_index': self.window_size - past_row_offset, # 1 for immediately preceding, window_size for oldest in window\n",
        "                                'source_digit_value': past_digit_value,\n",
        "                                'source_position_desc': self.combined_digit_positions.get(past_digit_idx, f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {past_digit_idx+1} (‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)'),\n",
        "                                'source_date': past_row['date'].strftime('%Y/%m/%d')\n",
        "                            })\n",
        "\n",
        "                digit_flow_analysis.append({\n",
        "                    'target_digit_value': target_digit_value,\n",
        "                    'target_position_desc': self.combined_digit_positions.get(target_digit_idx, f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_digit_idx+1} (‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)'),\n",
        "                    'total_found_in_past_rows': found_count_for_target_digit,\n",
        "                    'sources': sources_in_past_rows # Detailed list of all occurrences\n",
        "                })\n",
        "\n",
        "            # Calculate summary metrics for the current target row\n",
        "            # 'all_target_digits_found_at_least_once': True if each of the 8 target digits was found at least once.\n",
        "            all_target_digits_found_at_least_once = all(item['total_found_in_past_rows'] > 0 for item in digit_flow_analysis)\n",
        "\n",
        "            # 'total_occurrences_of_target_digits_from_past': Sum of all times *any* digit from the target row was found.\n",
        "            total_occurrences_of_target_digits_from_past = sum(item['total_found_in_past_rows'] for item in digit_flow_analysis)\n",
        "\n",
        "            # 'match_percentage_coverage': Percentage of *distinct* target digits (out of 8) that were found at least once. (Max 100%)\n",
        "            match_percentage_coverage = (len([item for item in digit_flow_analysis if item['total_found_in_past_rows'] > 0]) / 8) * 100\n",
        "\n",
        "            # 'match_percentage_total_occurrence': Percentage based on the sum of *all* occurrences (can exceed 100%)\n",
        "            match_percentage_total_occurrence = (total_occurrences_of_target_digits_from_past / 8) * 100\n",
        "\n",
        "            results.append({\n",
        "                'index': i, # Index of the current_target_row in the original (sorted) data\n",
        "                'date': current_target_row['date'].strftime('%Y/%m/%d'),\n",
        "                'six_digit': current_target_row['six_digit'],\n",
        "                'two_digit': current_target_row['two_digit'],\n",
        "                # Modified: Use generic key for preceding history rows\n",
        "                'preceding_history_rows': [{'date': r['date'].strftime('%Y/%m/%d'),\n",
        "                                            'six_digit': r['six_digit'],\n",
        "                                            'two_digit': r['two_digit']} for r in preceding_history_rows],\n",
        "                'digit_flow_analysis': digit_flow_analysis, # Detailed flow analysis\n",
        "                'all_target_digits_found_at_least_once': all_target_digits_found_at_least_once,\n",
        "                'total_occurrences_of_target_digits_from_past': total_occurrences_of_target_digits_from_past,\n",
        "                'match_percentage_coverage': match_percentage_coverage,\n",
        "                'match_percentage_total_occurrence': match_percentage_total_occurrence\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def calculate_summary(self, results):\n",
        "        \"\"\"Calculates overall summary statistics for the analysis results.\"\"\"\n",
        "        total_rows = len(results)\n",
        "        if total_rows == 0: # Prevent division by zero\n",
        "            return {\n",
        "                'total_rows': 0, 'perfect_matches_coverage': 0, 'partial_matches_coverage': 0,\n",
        "                'no_matches_coverage': 0, 'perfect_match_rate_coverage': 0,\n",
        "                'avg_percentage_coverage': 0, 'min_percentage_coverage': 0, 'max_percentage_coverage': 0,\n",
        "                'avg_percentage_total_occurrence': 0, 'min_percentage_total_occurrence': 0, 'max_percentage_total_occurrence': 0\n",
        "            }\n",
        "\n",
        "        # Summary statistics for Coverage (percentage of distinct digits found)\n",
        "        perfect_matches_coverage = len([r for r in results if r['all_target_digits_found_at_least_once']])\n",
        "        partial_matches_coverage = len([r for r in results if not r['all_target_digits_found_at_least_once'] and r['match_percentage_coverage'] > 0])\n",
        "        no_matches_coverage = len([r for r in results if r['match_percentage_coverage'] == 0])\n",
        "\n",
        "        percentages_coverage = [r['match_percentage_coverage'] for r in results]\n",
        "        avg_percentage_coverage = sum(percentages_coverage) / total_rows\n",
        "\n",
        "        # Summary statistics for Total Occurrence (percentage based on all appearances, can exceed 100%)\n",
        "        percentages_total_occurrence = [r['match_percentage_total_occurrence'] for r in results]\n",
        "        avg_percentage_total_occurrence = sum(percentages_total_occurrence) / total_rows\n",
        "\n",
        "        return {\n",
        "            'total_rows': total_rows,\n",
        "            'perfect_matches_coverage': perfect_matches_coverage,\n",
        "            'partial_matches_coverage': partial_matches_coverage,\n",
        "            'no_matches_coverage': no_matches_coverage,\n",
        "            'perfect_match_rate_coverage': (perfect_matches_coverage / total_rows) * 100,\n",
        "            'avg_percentage_coverage': round(avg_percentage_coverage, 1),\n",
        "            'min_percentage_coverage': min(percentages_coverage),\n",
        "            'max_percentage_coverage': max(percentages_coverage),\n",
        "            'avg_percentage_total_occurrence': round(avg_percentage_total_occurrence, 1),\n",
        "            'min_percentage_total_occurrence': min(percentages_total_occurrence),\n",
        "            'max_percentage_total_occurrence': max(percentages_total_occurrence)\n",
        "        }\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Executes the entire analysis process.\"\"\"\n",
        "        print(\"\\nüéØ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern...\")\n",
        "        # Modified: Print window size\n",
        "        print(f\"‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á {self.window_size} ‡∏á‡∏ß‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if not self.data:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå\")\n",
        "            return None, None\n",
        "\n",
        "        self.analysis_results = self.analyze_flow_pattern(self.data)\n",
        "        print(f\"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ {len(self.analysis_results)} ‡πÅ‡∏ñ‡∏ß\")\n",
        "\n",
        "        self.summary = self.calculate_summary(self.analysis_results)\n",
        "        self.display_summary()\n",
        "\n",
        "        return self.analysis_results, self.summary\n",
        "\n",
        "    def display_summary(self):\n",
        "        \"\"\"Displays the overall summary of the Flow Pattern test.\"\"\"\n",
        "        print(\"\\nüìä ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Flow Pattern\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if self.summary:\n",
        "            print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß (‡∏á‡∏ß‡∏î) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {self.summary['total_rows']} ‡πÅ‡∏ñ‡∏ß\")\n",
        "\n",
        "            print(f\"\\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å (Percentage of Unique Digits Covered) (‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï) ---\")\n",
        "            # Modified: Update print statement to reflect window_size\n",
        "            print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (8 ‡∏´‡∏•‡∏±‡∏Å) ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÉ‡∏ô {self.window_size} ‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {self.summary['perfect_matches_coverage']} ‡∏á‡∏ß‡∏î ({self.summary['perfect_match_rate_coverage']:.1f}%)\")\n",
        "            print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô: {self.summary['partial_matches_coverage']} ‡∏á‡∏ß‡∏î\")\n",
        "            print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÄ‡∏•‡∏¢: {self.summary['no_matches_coverage']} ‡∏á‡∏ß‡∏î\")\n",
        "            print(f\"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: {self.summary['avg_percentage_coverage']}%\")\n",
        "            print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {self.summary['min_percentage_coverage']}%\")\n",
        "            print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {self.summary['max_percentage_coverage']}%\")\n",
        "\n",
        "            print(\"\\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è (Total Occurrence Percentage) ---\")\n",
        "            print(\"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡∏¥‡∏ô 100% ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "            print(f\"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è: {self.summary['avg_percentage_total_occurrence']}%\")\n",
        "            print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {self.summary['min_percentage_total_occurrence']}%\")\n",
        "            print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {self.summary['max_percentage_total_occurrence']}%\")\n",
        "\n",
        "            print(\"\\nüéØ ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏≠‡∏á Pattern ‡∏ô‡∏µ‡πâ:\")\n",
        "            if self.summary['perfect_match_rate_coverage'] > 50 and self.summary['avg_percentage_total_occurrence'] > 100:\n",
        "                # Modified: Update print statement to reflect window_size\n",
        "                print(f\"‚úÖ Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å! ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏™‡∏π‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á\")\n",
        "            elif self.summary['perfect_match_rate_coverage'] > 20 or self.summary['avg_percentage_total_occurrence'] > 50:\n",
        "                # Modified: Update print statement to reflect window_size\n",
        "                print(f\"‚ö†Ô∏è Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
        "            elif self.summary['perfect_match_rate_coverage'] > 0 or self.summary['avg_percentage_total_occurrence'] > 0:\n",
        "                print(\"‚ùå Pattern ‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\")\n",
        "            else:\n",
        "                print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Pattern ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ\")\n",
        "\n",
        "    def get_perfect_matches(self):\n",
        "        \"\"\"Retrieves all rows (target rows) where the pattern is 100% covered (all 8 distinct digits found).\"\"\"\n",
        "        if not self.analysis_results:\n",
        "            return []\n",
        "\n",
        "        perfect_results = [r for r in self.analysis_results if r['all_target_digits_found_at_least_once']]\n",
        "        return perfect_results\n",
        "\n",
        "    def export_results_to_csv(self, filename=\"lottery_analysis_results.csv\"):\n",
        "        \"\"\"Exports the analysis results to a CSV file.\"\"\"\n",
        "        if not self.analysis_results:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å\")\n",
        "            return\n",
        "\n",
        "        data_for_export = []\n",
        "        for result in self.analysis_results:\n",
        "            data_for_export.append({\n",
        "                '‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà (‡∏á‡∏ß‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢)': result['index'] + 1,\n",
        "                '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà_‡∏á‡∏ß‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢': result['date'],\n",
        "                '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1_‡∏á‡∏ß‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢': result['six_digit'],\n",
        "                '‡πÄ‡∏•‡∏Ç2‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á_‡∏á‡∏ß‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢': result['two_digit'],\n",
        "                '‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°_Pattern_100%': '‡πÉ‡∏ä‡πà' if result['all_target_digits_found_at_least_once'] else '‡πÑ‡∏°‡πà',\n",
        "                '‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå_‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°': f\"{result['match_percentage_coverage']:.1f}%\",\n",
        "                '‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå_‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è': f\"{result['match_percentage_total_occurrence']:.1f}%\",\n",
        "                '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö_‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î_‡∏à‡∏≤‡∏Å‡∏≠‡∏î‡∏µ‡∏ï': result['total_occurrences_of_target_digits_from_past'] # Modified for clarity\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data_for_export)\n",
        "        # Use encoding='utf-8-sig' for correct display of Thai characters in spreadsheet programs like Microsoft Excel\n",
        "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå '{filename}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n",
        "    def extract_flow_matrix(self):\n",
        "        \"\"\"\n",
        "        Extracts and summarizes the detailed flow patterns into a Pandas DataFrame (Flow Matrix).\n",
        "        This matrix shows how often specific target digits (at specific positions)\n",
        "        originate from specific source digits (at specific positions) within the preceding 'window_size' rows.\n",
        "        The aggregation is by (Target Digit, Target Position, Source Digit, Source Position, Source Row Relative Index).\n",
        "        \"\"\"\n",
        "        if not self.analysis_results:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Flow Matrix\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        flow_data_list = []\n",
        "        for result in self.analysis_results:\n",
        "            for flow_analysis_item in result['digit_flow_analysis']:\n",
        "                target_digit = flow_analysis_item['target_digit_value']\n",
        "                target_pos_desc = flow_analysis_item['target_position_desc'] # Use full description\n",
        "\n",
        "                # If the target digit was found in past rows\n",
        "                if flow_analysis_item['sources']:\n",
        "                    for source in flow_analysis_item['sources']:\n",
        "                        flow_data_list.append({\n",
        "                            'Target_Date': result['date'], # Keep date for context\n",
        "                            'Target_Prize1': result['six_digit'],\n",
        "                            'Target_Last2': result['two_digit'],\n",
        "                            'Target_Digit': target_digit,\n",
        "                            'Target_Position': target_pos_desc, # Use description\n",
        "                            'Source_Digit': source['source_digit_value'],\n",
        "                            'Source_Position': source['source_position_desc'], # Use description\n",
        "                            'Source_Row_Relative_Index': source['source_row_relative_index'], # 1 = immediately preceding, window_size = oldest\n",
        "                            'Source_Date': source['source_date'], # Keep source date for context\n",
        "                            'Occurrences': 1 # Each entry represents one occurrence\n",
        "                        })\n",
        "                else:\n",
        "                    # If target digit was not found, still record it with 'N/A' sources\n",
        "                    flow_data_list.append({\n",
        "                        'Target_Date': result['date'],\n",
        "                        'Target_Prize1': result['six_digit'],\n",
        "                        'Target_Last2': result['two_digit'],\n",
        "                        'Target_Digit': target_digit,\n",
        "                        'Target_Position': target_pos_desc,\n",
        "                        'Source_Digit': 'N/A',\n",
        "                        'Source_Position': 'N/A',\n",
        "                        'Source_Row_Relative_Index': 0, # Use 0 or some indicator for 'not found'\n",
        "                        'Source_Date': 'N/A',\n",
        "                        'Occurrences': 0\n",
        "                    })\n",
        "\n",
        "        flow_df = pd.DataFrame(flow_data_list)\n",
        "\n",
        "        if flow_df.empty:\n",
        "            print(\"‚ùó Flow Matrix ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ Pattern ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Group by the desired dimensions and sum occurrences\n",
        "        # This creates the summary matrix\n",
        "        flow_matrix = flow_df.groupby([\n",
        "            'Target_Digit', 'Target_Position',\n",
        "            'Source_Digit', 'Source_Position',\n",
        "            'Source_Row_Relative_Index'\n",
        "        ])['Occurrences'].sum().reset_index()\n",
        "\n",
        "        # Sort the results for better readability\n",
        "        flow_matrix.sort_values(by=['Target_Digit', 'Target_Position', 'Occurrences'], ascending=[True, True, False], inplace=True)\n",
        "\n",
        "        print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Flow Matrix ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "        print(\"üí° ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\")\n",
        "        return flow_matrix\n",
        "\n",
        "    # --- Machine Learning Section (from previous code context) ---\n",
        "    def prepare_features_for_ml(self, data_subset):\n",
        "        \"\"\"\n",
        "        Prepares features for the ML model.\n",
        "        For each target digit position (0-7 in combined 8-digit string),\n",
        "        creates features based on the presence of digits in the preceding 'window_size' rows.\n",
        "        The target for each model is the digit at that specific position in the next draw.\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_prize1 = [] # Labels for each of the 6 digits of prize 1\n",
        "        labels_last2 = []  # Labels for each of the 2 digits of last 2\n",
        "\n",
        "        # Start from window_size to ensure enough history for features and a target\n",
        "        for i in range(self.window_size, len(data_subset)):\n",
        "            current_row = data_subset[i]\n",
        "            historical_rows = data_subset[i - self.window_size : i]\n",
        "\n",
        "            # Features for the current target row will be based on the digits in historical_rows\n",
        "            feature_vector = {}\n",
        "\n",
        "            # Populate feature_vector: count occurrences of each digit (0-9)\n",
        "            # at each of the 8 combined positions, across the window_size historical rows.\n",
        "            for digit_val in range(10): # For digits 0-9\n",
        "                for pos_idx in range(8): # For positions 0-7\n",
        "                    feature_vector[f'digit_{digit_val}_at_pos_{pos_idx}'] = 0\n",
        "\n",
        "            # Also add features for general digit frequency and position frequency across the window\n",
        "            for digit_val in range(10):\n",
        "                feature_vector[f'digit_freq_{digit_val}_window'] = 0\n",
        "            for pos_idx in range(8):\n",
        "                feature_vector[f'pos_freq_window_{pos_idx}'] = 0\n",
        "\n",
        "\n",
        "            for row_offset, h_row in enumerate(historical_rows):\n",
        "                combined_h_digits = h_row['two_digit'] + h_row['six_digit']\n",
        "                for pos_idx, digit_val_str in enumerate(combined_h_digits):\n",
        "                    digit_val = int(digit_val_str)\n",
        "                    feature_vector[f'digit_{digit_val}_at_pos_{pos_idx}'] += 1 # Count specific digit at specific position\n",
        "                    feature_vector[f'digit_freq_{digit_val}_window'] += 1 # General digit frequency\n",
        "                    feature_vector[f'pos_freq_window_{pos_idx}'] += 1 # General position frequency\n",
        "\n",
        "\n",
        "            features_list.append(feature_vector)\n",
        "\n",
        "            # Get target labels (the actual digits for the current row)\n",
        "            current_combined_digits = current_row['two_digit'] + current_row['six_digit']\n",
        "            labels_last2.append([int(current_combined_digits[0]), int(current_combined_digits[1])]) # D0, D1 for last 2\n",
        "            labels_prize1.append([int(current_combined_digits[2]), int(current_combined_digits[3]),\n",
        "                                  int(current_combined_digits[4]), int(current_combined_digits[5]),\n",
        "                                  int(current_combined_digits[6]), int(current_combined_digits[7])]) # D2-D7 for prize 1\n",
        "\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        # Ensure all possible feature columns exist even if some didn't appear in a specific run\n",
        "        all_possible_features = []\n",
        "        for digit_val in range(10):\n",
        "            for pos_idx in range(8):\n",
        "                all_possible_features.append(f'digit_{digit_val}_at_pos_{pos_idx}')\n",
        "        for digit_val in range(10):\n",
        "            all_possible_features.append(f'digit_freq_{digit_val}_window')\n",
        "        for pos_idx in range(8):\n",
        "            all_possible_features.append(f'pos_freq_window_{pos_idx}')\n",
        "\n",
        "        for col in all_possible_features:\n",
        "            if col not in features_df.columns:\n",
        "                features_df[col] = 0 # Add missing columns with 0\n",
        "\n",
        "        # Ensure order of columns for consistency in training/prediction\n",
        "        features_df = features_df[all_possible_features]\n",
        "\n",
        "        return features_df, np.array(labels_last2), np.array(labels_prize1)\n",
        "\n",
        "\n",
        "    def train_ml_models(self, test_size_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Trains a separate Random Forest Classifier model for each of the 8 digit positions.\n",
        "        Each model predicts the digit (0-9) for its respective position.\n",
        "        \"\"\"\n",
        "        print(f\"\\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á...\")\n",
        "\n",
        "        if len(self.data) < self.window_size + 10: # Need enough data for features + some for training\n",
        "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ {self.window_size + 10} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)\")\n",
        "            self.ml_models = {}\n",
        "            return\n",
        "\n",
        "        features_df, labels_last2, labels_prize1 = self.prepare_features_for_ml(self.data)\n",
        "\n",
        "        if features_df.empty:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Features\")\n",
        "            self.ml_models = {}\n",
        "            return\n",
        "\n",
        "        all_labels_combined = np.concatenate((labels_last2, labels_prize1), axis=1) # Combine all 8 labels\n",
        "\n",
        "        # Train a model for each of the 8 positions\n",
        "        for pos_idx in range(8):\n",
        "            print(f\"   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {self.combined_digit_positions[pos_idx]} (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos_idx+1})...\")\n",
        "            y_pos = all_labels_combined[:, pos_idx] # Labels for the current position\n",
        "\n",
        "            # Check if there's enough diversity in labels for stratification\n",
        "            if len(np.unique(y_pos)) < 2:\n",
        "                print(f\"     ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {self.combined_digit_positions[pos_idx]} ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß). ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify\")\n",
        "                X_train, X_test, y_train, y_test = train_test_split(features_df, y_pos, test_size=test_size_ratio, random_state=42)\n",
        "            else:\n",
        "                X_train, X_test, y_train, y_test = train_test_split(features_df, y_pos, test_size=test_size_ratio, random_state=42, stratify=y_pos)\n",
        "\n",
        "            if len(X_train) == 0 or len(X_test) == 0:\n",
        "                print(f\"     ‚ùå ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {self.combined_digit_positions[pos_idx]} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ test_size_ratio\")\n",
        "                continue\n",
        "\n",
        "            model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced') # Use class_weight for imbalanced classes\n",
        "            try:\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                print(f\"     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {accuracy:.2%}\")\n",
        "                self.ml_models[pos_idx] = model\n",
        "            except Exception as e:\n",
        "                print(f\"     ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {self.combined_digit_positions[pos_idx]}: {e}\")\n",
        "                self.ml_models[pos_idx] = None\n",
        "\n",
        "        print(\"‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\")\n",
        "\n",
        "\n",
        "    def predict_next_draw_ml(self, num_predictions=5):\n",
        "        \"\"\"\n",
        "        Uses the trained ML models to predict the digits for the next draw.\n",
        "        Returns the top 'num_predictions' sets of 8 digits and their probabilities.\n",
        "        \"\"\"\n",
        "        if not self.ml_models or any(model is None for model in self.ml_models.values()):\n",
        "            print(\"‚ùó ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå. ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å train_ml_models() ‡∏Å‡πà‡∏≠‡∏ô.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"\\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ML (‡πÅ‡∏™‡∏î‡∏á {num_predictions} ‡∏ä‡∏∏‡∏î):\")\n",
        "\n",
        "        # Prepare features for the very last set of historical data\n",
        "        # We need the last 'window_size' rows from self.data\n",
        "        if len(self.data) < self.window_size:\n",
        "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ({len(self.data)} < {self.window_size}) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ML.\")\n",
        "            return []\n",
        "\n",
        "        # Create a dummy list with only the last 'window_size' rows to pass to prepare_features_for_ml\n",
        "        # The prepare_features_for_ml expects a list of dicts, and it internally handles the shifting.\n",
        "        # To get the features for the *next* draw, we feed it the *last* window_size rows as if they are\n",
        "        # the \"history\" leading to a \"target\" row that we don't have yet.\n",
        "        # So we need to feed it self.data[-(self.window_size):] as the 'historical_rows'\n",
        "        # The method returns features_df, labels_last2, labels_prize1 where features_df is based on\n",
        "        # the *current* window predicting the *next*.\n",
        "\n",
        "        # A simpler way to get the feature vector for the next prediction:\n",
        "        # Create a temporary DataFrame from the last 'self.window_size' rows\n",
        "        # The 'prepare_features_for_ml' expects a list, so we make a dummy structure\n",
        "        # to get one prediction row.\n",
        "\n",
        "        # Create a single 'fake' row for features_list to represent the input for the next prediction\n",
        "        # The target row index in prepare_features_for_ml is 'i'. If we want to predict the (len(data))th row,\n",
        "        # its historical data will be data[len(data) - self.window_size : len(data)]\n",
        "\n",
        "        # This is a bit tricky with `prepare_features_for_ml` as it's designed for batch processing.\n",
        "        # Let's adapt it to get features for just the next prediction.\n",
        "\n",
        "        # The historical data for the *next* draw is the last `self.window_size` rows from `self.data`.\n",
        "        last_n_rows = self.data[-self.window_size:]\n",
        "\n",
        "        # Create a dummy target row to satisfy `prepare_features_for_ml`'s structure.\n",
        "        # Its values don't matter, only its existence to trigger feature creation for the last window.\n",
        "        dummy_target_row = {\n",
        "            'date': datetime.now(), # Placeholder date\n",
        "            'six_digit': '000000', # Placeholder number\n",
        "            'two_digit': '00'      # Placeholder number\n",
        "        }\n",
        "\n",
        "        # Combine last_n_rows with the dummy_target_row to simulate the input for `prepare_features_for_ml`\n",
        "        # so it generates features for predicting the dummy_target_row (which is our next draw).\n",
        "        prediction_input_data = last_n_rows + [dummy_target_row]\n",
        "\n",
        "        # Call prepare_features_for_ml with this specially crafted data\n",
        "        # It will return features_df, labels_last2, labels_prize1.\n",
        "        # We are interested in the *last* row of features_df, which corresponds to predicting dummy_target_row.\n",
        "        X_predict_all, _, _ = self.prepare_features_for_ml(prediction_input_data)\n",
        "\n",
        "        if X_predict_all.empty:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö prepare_features_for_ml.\")\n",
        "            return []\n",
        "\n",
        "        X_predict = X_predict_all.iloc[-1:].copy() # Get the feature vector for the next draw prediction\n",
        "\n",
        "        # Ensure that X_predict has the same columns and order as the training data\n",
        "        # This is crucial for the model to work correctly.\n",
        "        # We need the feature names from one of the trained models. Assuming all models trained on same features.\n",
        "        if self.ml_models and 0 in self.ml_models and self.ml_models[0]:\n",
        "            trained_features = self.ml_models[0].feature_names_in_\n",
        "            missing_cols = set(trained_features) - set(X_predict.columns)\n",
        "            for c in missing_cols:\n",
        "                X_predict[c] = 0 # Add any missing features with a default value of 0\n",
        "\n",
        "            # Ensure the order of columns matches the training order\n",
        "            X_predict = X_predict[trained_features]\n",
        "        else:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Features ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ.\")\n",
        "            return []\n",
        "\n",
        "\n",
        "        # Get probabilities for each position\n",
        "        position_probabilities = []\n",
        "        for pos_idx in range(8):\n",
        "            model = self.ml_models.get(pos_idx)\n",
        "            if model:\n",
        "                # Predict probabilities for each digit (0-9)\n",
        "                proba = model.predict_proba(X_predict)[0]\n",
        "                # Get digit labels (0, 1, ..., 9)\n",
        "                classes = model.classes_\n",
        "                # Map probabilities to digits and sort by probability (descending)\n",
        "                digit_proba_map = {int(c): p for c, p in zip(classes, proba)}\n",
        "\n",
        "                # Ensure all 0-9 digits are present, even if probability is 0\n",
        "                full_digit_proba_map = {d: digit_proba_map.get(d, 0.0) for d in range(10)}\n",
        "\n",
        "                sorted_digits_with_proba = sorted(full_digit_proba_map.items(), key=lambda item: item[1], reverse=True)\n",
        "                position_probabilities.append(sorted_digits_with_proba)\n",
        "            else:\n",
        "                position_probabilities.append([]) # No model for this position\n",
        "\n",
        "\n",
        "        # Generate top N combinations\n",
        "        # This part requires combining predictions from 8 independent models.\n",
        "        # A simple approach is to take the top N most probable digits for each position\n",
        "        # and then combine them. This can lead to a very large number of combinations.\n",
        "        # For simplicity, we can list the top predicted digit for each position,\n",
        "        # or combine the top few into 'sets'.\n",
        "\n",
        "        predicted_combinations = []\n",
        "\n",
        "        # Strategy 1: Top 1 prediction for each position\n",
        "        top_1_combination = \"\"\n",
        "        for pos_idx in range(8):\n",
        "            if position_probabilities[pos_idx]:\n",
        "                top_1_combination += str(position_probabilities[pos_idx][0][0])\n",
        "            else:\n",
        "                top_1_combination += \"?\" # Indicate missing prediction\n",
        "\n",
        "        if len(top_1_combination) == 8 and '?' not in top_1_combination:\n",
        "            predicted_combinations.append({\n",
        "                'Combination': top_1_combination,\n",
        "                'Type': 'Top 1 Probability'\n",
        "            })\n",
        "\n",
        "        # Strategy 2: Generate N diverse combinations based on high probabilities\n",
        "        # This is more complex. For now, let's just list top 3-5 digits for each position.\n",
        "        print(\"\\n   --- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å (‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---\")\n",
        "        for pos_idx in range(8):\n",
        "            pos_desc = self.combined_digit_positions[pos_idx]\n",
        "            top_digits_for_pos = position_probabilities[pos_idx][:5] # Get top 5 for display\n",
        "            if top_digits_for_pos:\n",
        "                print(f\"   {pos_desc}: \" + \", \".join([f\"{digit} ({prob:.2%})\" for digit, prob in top_digits_for_pos]))\n",
        "            else:\n",
        "                print(f\"   {pos_desc}: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\")\n",
        "\n",
        "        return predicted_combinations\n",
        "\n",
        "\n",
        "    def main_run(self):\n",
        "        \"\"\"\n",
        "        Main function to run the entire analysis pipeline.\n",
        "        \"\"\"\n",
        "        print(\"======== üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢ üöÄ ========\")\n",
        "\n",
        "        # 1. Load Data\n",
        "        if not self.load_data_from_file():\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.\")\n",
        "            return\n",
        "\n",
        "        # 2. Run Flow Pattern Analysis\n",
        "        analysis_results, summary = self.run_analysis()\n",
        "        if analysis_results is None:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.\")\n",
        "            return\n",
        "\n",
        "        # 3. Export Results (Optional)\n",
        "        self.export_results_to_csv()\n",
        "\n",
        "        # 4. Extract and Display Flow Matrix\n",
        "        flow_matrix_df = self.extract_flow_matrix()\n",
        "        if not flow_matrix_df.empty:\n",
        "            print(\"\\nüìä Flow Matrix (‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß 10 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):\")\n",
        "            print(flow_matrix_df.head(10).to_string()) # Use .to_string() to avoid truncation\n",
        "            print(\"\\nüí° ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Flow Matrix ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô CSV ‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\")\n",
        "            flow_matrix_df.to_csv(\"flow_matrix_summary.csv\", index=False, encoding='utf-8-sig')\n",
        "            print(\"‚úÖ Flow Matrix ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'flow_matrix_summary.csv'\")\n",
        "\n",
        "        # 5. Train ML Models\n",
        "        self.train_ml_models()\n",
        "\n",
        "        # 6. Predict Next Draw with ML\n",
        "        self.predict_next_draw_ml(num_predictions=5) # Predict top 5 combinations\n",
        "\n",
        "        print(\"\\n======== ‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ========\")\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Create an instance of the analyzer with a specific window size\n",
        "    # You can change window_size here, e.g., LotteryPatternAnalyzer(window_size=10)\n",
        "    analyzer = LotteryPatternAnalyzer(window_size=6)\n",
        "    analyzer.main_run()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rm83F5ejMaM-",
        "outputId": "025a5a08-61b8-4992-947d-e22cb8311d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢ üöÄ ========\n",
            "üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡πÄ‡∏ä‡πà‡∏ô '‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-be59e771-62c6-48c9-ba5f-589a0fd153d7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-be59e771-62c6-48c9-ba5f-589a0fd153d7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv to ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21 (1).csv\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21 (1).csv\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 559 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "üéØ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern...\n",
            "‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 6 ‡∏á‡∏ß‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern\n",
            "============================================================\n",
            "‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ 553 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "üìä ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Flow Pattern\n",
            "============================================================\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß (‡∏á‡∏ß‡∏î) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö: 553 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å (Percentage of Unique Digits Covered) (‡∏à‡∏≤‡∏Å 6 ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï) ---\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (8 ‡∏´‡∏•‡∏±‡∏Å) ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÉ‡∏ô 6 ‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: 533 ‡∏á‡∏ß‡∏î (96.4%)\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô: 20 ‡∏á‡∏ß‡∏î\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÄ‡∏•‡∏¢: 0 ‡∏á‡∏ß‡∏î\n",
            "‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: 99.4%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: 75.0%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: 100.0%\n",
            "\n",
            "--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è (Total Occurrence Percentage) ---\n",
            "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡∏¥‡∏ô 100% ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
            "‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è: 477.9%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: 275.0%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: 762.5%\n",
            "\n",
            "üéØ ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏≠‡∏á Pattern ‡∏ô‡∏µ‡πâ:\n",
            "‚úÖ Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å! ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å 6 ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏™‡∏π‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á\n",
            "‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå 'lottery_analysis_results.csv' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\n",
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Flow Matrix ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\n",
            "üí° ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
            "\n",
            "üìä Flow Matrix (‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß 10 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):\n",
            "   Target_Digit        Target_Position Source_Digit          Source_Position  Source_Row_Relative_Index  Occurrences\n",
            "1             0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)                          2           11\n",
            "13            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)                          2            9\n",
            "24            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô)                          1            9\n",
            "0             0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)                          1            8\n",
            "18            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)                          1            8\n",
            "40            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0  ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)                          5            8\n",
            "4             0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)                          5            7\n",
            "11            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢)                          6            7\n",
            "16            0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)                          5            7\n",
            "2             0  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)            0    ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)                          3            6\n",
            "\n",
            "üí° ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Flow Matrix ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô CSV ‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
            "‚úÖ Flow Matrix ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'flow_matrix_summary.csv'\n",
            "\n",
            "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á...\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 13.51%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 2)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 2.70%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏™‡∏ô) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 3)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 9.01%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 4)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 9.91%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 5)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 13.51%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 6)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 13.51%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 7)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 9.91%\n",
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢) (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 8)...\n",
            "     ‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: 8.11%\n",
            "‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\n",
            "\n",
            "ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ML (‡πÅ‡∏™‡∏î‡∏á 5 ‡∏ä‡∏∏‡∏î):\n",
            "\n",
            "   --- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å (‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---\n",
            "   ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö): 2 (30.00%), 4 (13.00%), 8 (12.00%), 0 (10.00%), 3 (8.00%)\n",
            "   ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢): 0 (25.00%), 5 (12.00%), 9 (12.00%), 6 (11.00%), 7 (10.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏™‡∏ô): 5 (31.00%), 1 (14.00%), 7 (12.00%), 3 (9.00%), 8 (9.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô): 5 (22.00%), 3 (14.00%), 7 (12.00%), 6 (11.00%), 1 (9.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô): 9 (38.00%), 1 (11.00%), 6 (9.00%), 3 (8.00%), 4 (8.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢): 4 (24.00%), 1 (15.00%), 9 (12.00%), 0 (9.00%), 6 (9.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö): 5 (22.00%), 0 (16.00%), 1 (14.00%), 2 (12.00%), 6 (10.00%)\n",
            "   ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢): 9 (19.00%), 4 (16.00%), 7 (14.00%), 8 (11.00%), 3 (10.00%)\n",
            "\n",
            "======== ‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "from scipy.stats import chisquare\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regular Expressions ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô Colab ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)\n",
        "# ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ, ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô\n",
        "# !pip install -U matplotlib\n",
        "# !apt-get install -y fonts-thai-tlwg\n",
        "# import matplotlib.font_manager as fm\n",
        "# fm.fontManager.addfont('/usr/share/fonts/thai-tlwg/THSarabunNew.ttf') # ‡∏´‡∏£‡∏∑‡∏≠‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ\n",
        "# plt.rcParams['font.family'] = ['TH Sarabun New'] # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans'] # ‡πÉ‡∏ä‡πâ Dejavu Sans ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "class LotteryAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.analysis_results = {}\n",
        "        self.model = None # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning\n",
        "\n",
        "    def load_and_clean_data(self, file_path):\n",
        "        \"\"\"\n",
        "        ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV\n",
        "        ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞ encoding\n",
        "        \"\"\"\n",
        "        print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
        "\n",
        "        # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏î‡πâ‡∏ß‡∏¢ encoding ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding='utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, encoding='cp874') # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "            except UnicodeDecodeError:\n",
        "                df = pd.read_csv(file_path, encoding='latin1') # fallback ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ó‡∏µ‡πà {file_path}. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ò.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
        "            raise # ‡∏™‡πà‡∏á exception ‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
        "\n",
        "        print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: '{file_path}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n",
        "        # --- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ---\n",
        "        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô\n",
        "        original_cols = df.columns.tolist()\n",
        "        cleaned_col_map = {}\n",
        "        for col in original_cols:\n",
        "            # ‡∏•‡∏ö‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏´‡∏ô‡πâ‡∏≤/‡∏´‡∏•‡∏±‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "            # ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏¢‡∏±‡∏á‡∏ñ‡∏≠‡∏î‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å strip)\n",
        "            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: '<artifact identifier=\"lottery_csv\" type=\"text/plain\" title=\"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 20.csv\">'\n",
        "            # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 'artifact identifier lottery_csv type text plain title ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 20.csv'\n",
        "            # ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "\n",
        "            # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà \"‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á\" ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà\n",
        "            # ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡∏Ç‡∏±‡∏ô\n",
        "\n",
        "            cleaned_col = re.sub(r'\\s+', ' ', col).strip() # ‡∏•‡∏ö‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÄ‡∏Å‡∏¥‡∏ô‡πÅ‡∏•‡∏∞ trim\n",
        "            cleaned_col_map[col] = cleaned_col\n",
        "        df = df.rename(columns=cleaned_col_map)\n",
        "\n",
        "        print(\"üîç ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô):\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        # --- ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì) ---\n",
        "        # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà **‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á** ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏à‡∏£‡∏¥‡∏á‡πÜ\n",
        "        expected_date_col = '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'\n",
        "        expected_prize1_col = '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'\n",
        "        expected_last2_col = '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ/‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©)\n",
        "        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà \"‡∏°‡∏µ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å\" ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "        # ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ó‡∏ô\n",
        "\n",
        "        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\"\n",
        "        found_date_col = None\n",
        "        for col_name in df.columns.tolist():\n",
        "            if expected_date_col in col_name: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ \"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\" ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "                found_date_col = col_name\n",
        "                break\n",
        "        if not found_date_col:\n",
        "            raise KeyError(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{expected_date_col}' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {df.columns.tolist()}\")\n",
        "\n",
        "        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)\"\n",
        "        found_prize1_col = None\n",
        "        for col_name in df.columns.tolist():\n",
        "            if expected_prize1_col in col_name: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ \"‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)\" ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "                found_prize1_col = col_name\n",
        "                break\n",
        "        if not found_prize1_col:\n",
        "            raise KeyError(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{expected_prize1_col}' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {df.columns.tolist()}\")\n",
        "\n",
        "        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\"\n",
        "        found_last2_col = None\n",
        "        for col_name in df.columns.tolist():\n",
        "            if expected_last2_col in col_name: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ \"‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\" ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "                found_last2_col = col_name\n",
        "                break\n",
        "        if not found_last2_col:\n",
        "            raise KeyError(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{expected_last2_col}' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {df.columns.tolist()}\")\n",
        "\n",
        "        print(f\"‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà='{found_date_col}', ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1='{found_prize1_col}', ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á='{found_last2_col}'\")\n",
        "\n",
        "        # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö\n",
        "        df = df.dropna(subset=[found_date_col, found_prize1_col, found_last2_col])\n",
        "\n",
        "        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î\n",
        "        df = df.rename(columns={\n",
        "            found_date_col: '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà',\n",
        "            found_prize1_col: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)',\n",
        "            found_last2_col: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'\n",
        "        })\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "        df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'] = pd.to_datetime(df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'], format='%Y/%m/%d', errors='coerce')\n",
        "        df = df.dropna(subset=['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà']) # ‡∏î‡∏£‡∏≠‡∏õ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• - ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 6 ‡∏´‡∏•‡∏±‡∏Å\n",
        "        df['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'] = df['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'].astype(str).str.zfill(6)\n",
        "        df = df[df['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'].str.match(r'^\\d{6}$')]\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á - ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 2 ‡∏´‡∏•‡∏±‡∏Å\n",
        "        df['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'] = df['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'].astype(str).str.zfill(2)\n",
        "        df = df[df['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'].str.match(r'^\\d{2}$')]\n",
        "\n",
        "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤)\n",
        "        df = df.sort_values('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        self.data = df\n",
        "        print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "        print(f\"üìÖ ‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].min().strftime('%d/%m/%Y')} - {df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].max().strftime('%d/%m/%Y')}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def analyze_frequency(self):\n",
        "        \"\"\"\n",
        "        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "        \"\"\"\n",
        "        print(\"\\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç...\")\n",
        "\n",
        "        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9 ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1\n",
        "        digit_freq = Counter()\n",
        "        for number in self.data['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)']:\n",
        "            for digit in str(number):\n",
        "                digit_freq[digit] += 1\n",
        "\n",
        "        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "        last2_freq = Counter(self.data['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'])\n",
        "\n",
        "        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏•‡∏Ç\n",
        "        position_analysis = {}\n",
        "        for pos in range(6):\n",
        "            position_analysis[f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {pos+1}'] = Counter()\n",
        "\n",
        "        for number in self.data['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)']:\n",
        "            for i, digit in enumerate(str(number)):\n",
        "                position_analysis[f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {i+1}'][digit] += 1\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Digital Root\n",
        "        self.data['digital_root_prize1'] = self.data['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'].apply(lambda x: self._calculate_digital_root(int(x)))\n",
        "        self.data['digital_root_last2'] = self.data['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'].apply(lambda x: self._calculate_digital_root(int(x)))\n",
        "\n",
        "        digital_root_prize1_freq = Counter(self.data['digital_root_prize1'])\n",
        "        digital_root_last2_freq = Counter(self.data['digital_root_last2'])\n",
        "\n",
        "\n",
        "        self.analysis_results = {\n",
        "            'digit_frequency': dict(digit_freq),\n",
        "            'last2_frequency': dict(last2_freq),\n",
        "            'position_analysis': position_analysis,\n",
        "            'total_records': len(self.data),\n",
        "            'digital_root_prize1_freq': dict(digital_root_prize1_freq), # ‡πÄ‡∏û‡∏¥‡πà‡∏° Digital Root Freq\n",
        "            'digital_root_last2_freq': dict(digital_root_last2_freq)\n",
        "        }\n",
        "\n",
        "        print(\"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "        return self.analysis_results\n",
        "\n",
        "    def _calculate_digital_root(self, n):\n",
        "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Digital Root ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\"\"\"\n",
        "        return (n - 1) % 9 + 1 if n > 0 else 0\n",
        "\n",
        "    def _is_fibonacci(self, n):\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç Fibonacci ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)\"\"\"\n",
        "        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•\n",
        "        if n < 0: return False\n",
        "        phi = 0.5 + 0.5 * np.sqrt(5.0)\n",
        "        a = phi * n\n",
        "        return n == 0 or abs(round(a) - a) < 1.0 / n # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡πà‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\n",
        "\n",
        "    def display_analysis(self):\n",
        "        \"\"\"\n",
        "        ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°\n",
        "        print(f\"\\nüìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°:\")\n",
        "        print(f\"   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {self.analysis_results['total_records']:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πá‡∏ô \"‡∏õ‡∏µ\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö UI ‡πÄ‡∏î‡∏¥‡∏°\n",
        "        date_range_days = (self.data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].max() - self.data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].min()).days\n",
        "        date_range_years = date_range_days / 365.25\n",
        "        print(f\"   ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤: {date_range_years:.1f} ‡∏õ‡∏µ ({date_range_days:,} ‡∏ß‡∏±‡∏ô)\")\n",
        "        print(f\"   ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: {len(self.analysis_results['last2_frequency'])} ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö\")\n",
        "        # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢/‡πÄ‡∏•‡∏Ç: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏Ç 0-9 ‡∏≠‡∏≠‡∏Å / ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏•‡∏Ç 10 ‡∏ï‡∏±‡∏ß\n",
        "        total_digit_occurrences = sum(self.analysis_results['digit_frequency'].values())\n",
        "        avg_occurrence_per_digit = total_digit_occurrences / 10\n",
        "        print(f\"   ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢/‡πÄ‡∏•‡∏Ç (0-9): {avg_occurrence_per_digit:.0f} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "\n",
        "\n",
        "        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "        print(f\"\\nüî• ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0-9):\")\n",
        "        digit_freq = self.analysis_results['digit_frequency']\n",
        "        sorted_digits = sorted(digit_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for digit, freq in sorted_digits:\n",
        "            percentage = (freq / sum(digit_freq.values())) * 100\n",
        "            print(f\"   ‡πÄ‡∏•‡∏Ç {digit}: {freq:,} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({percentage:.1f}%)\")\n",
        "\n",
        "        # TOP 10 ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "        print(f\"\\nüéØ TOP 10 ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á:\")\n",
        "        last2_freq = self.analysis_results['last2_frequency']\n",
        "        sorted_last2 = sorted(last2_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        for i, (number, freq) in enumerate(sorted_last2, 1):\n",
        "            percentage = (freq / sum(last2_freq.values())) * 100\n",
        "            print(f\"   {i:2d}. ‡πÄ‡∏•‡∏Ç {number}: {freq} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({percentage:.1f}%)\")\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Digital Root\n",
        "        print(f\"\\nüî¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1:\")\n",
        "        digital_root_prize1_freq = self.analysis_results['digital_root_prize1_freq']\n",
        "        sorted_dr_prize1 = sorted(digital_root_prize1_freq.items(), key=lambda x: x[0])\n",
        "        for dr, freq in sorted_dr_prize1:\n",
        "            print(f\"   DR {dr}: {freq:,} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "\n",
        "        print(f\"\\nüî¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á:\")\n",
        "        digital_root_last2_freq = self.analysis_results['digital_root_last2_freq']\n",
        "        sorted_dr_last2 = sorted(digital_root_last2_freq.items(), key=lambda x: x[0])\n",
        "        for dr, freq in sorted_dr_last2:\n",
        "            print(f\"   DR {dr}: {freq:,} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"\n",
        "        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
        "        \"\"\"\n",
        "        print(\"\\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•...\")\n",
        "\n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12)) # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 2x3 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏≤‡∏ü Digital Root\n",
        "        fig.suptitle('‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 1: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9\n",
        "        digit_freq = self.analysis_results['digit_frequency']\n",
        "        digits = list(map(str, range(10))) # ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô str ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö key ‡πÉ‡∏ô Counter\n",
        "        frequencies = [digit_freq.get(d, 0) for d in digits]\n",
        "\n",
        "        axes[0, 0].bar(digits, frequencies, color='skyblue', edgecolor='navy')\n",
        "        axes[0, 0].set_title('üìà ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9', fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç')\n",
        "        axes[0, 0].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 2: TOP 15 ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "        last2_freq = self.analysis_results['last2_frequency']\n",
        "        top_last2 = sorted(last2_freq.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "        numbers, freqs = zip(*top_last2)\n",
        "\n",
        "        axes[0, 1].bar(range(len(numbers)), freqs, color='lightgreen', edgecolor='darkgreen')\n",
        "        axes[0, 1].set_title('üéØ TOP 15 ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á', fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á')\n",
        "        axes[0, 1].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[0, 1].set_xticks(range(len(numbers)))\n",
        "        axes[0, 1].set_xticklabels(numbers, rotation=45, ha='right')\n",
        "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 3: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1)\n",
        "        pos1_freq = self.analysis_results['position_analysis']['‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1']\n",
        "        pos1_digits = list(map(str, range(10)))\n",
        "        pos1_frequencies = [pos1_freq.get(d, 0) for d in pos1_digits]\n",
        "\n",
        "        axes[0, 2].bar(pos1_digits, pos1_frequencies, color='orange', edgecolor='red')\n",
        "        axes[0, 2].set_title('ü•á ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1', fontweight='bold')\n",
        "        axes[0, 2].set_xlabel('‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç')\n",
        "        axes[0, 2].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 4: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 6)\n",
        "        pos6_freq = self.analysis_results['position_analysis']['‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 6']\n",
        "        pos6_digits = list(map(str, range(10)))\n",
        "        pos6_frequencies = [pos6_freq.get(d, 0) for d in pos6_digits]\n",
        "\n",
        "        axes[1, 0].bar(pos6_digits, pos6_frequencies, color='purple', edgecolor='darkviolet')\n",
        "        axes[1, 0].set_title('üèÜ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 6', fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç')\n",
        "        axes[1, 0].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 5: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1\n",
        "        digital_root_prize1_freq = self.analysis_results['digital_root_prize1_freq']\n",
        "        dr_prize1_values = list(map(str, sorted(digital_root_prize1_freq.keys())))\n",
        "        dr_prize1_freqs = [digital_root_prize1_freq.get(int(dr), 0) for dr in dr_prize1_values]\n",
        "\n",
        "        axes[1, 1].bar(dr_prize1_values, dr_prize1_freqs, color='darkred', edgecolor='black')\n",
        "        axes[1, 1].set_title('üî¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1)', fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Digital Root')\n",
        "        axes[1, 1].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # ‡∏Å‡∏£‡∏≤‡∏ü 6: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "        digital_root_last2_freq = self.analysis_results['digital_root_last2_freq']\n",
        "        dr_last2_values = list(map(str, sorted(digital_root_last2_freq.keys())))\n",
        "        dr_last2_freqs = [digital_root_last2_freq.get(int(dr), 0) for dr in dr_last2_values]\n",
        "\n",
        "        axes[1, 2].bar(dr_last2_values, dr_last2_freqs, color='darkblue', edgecolor='white')\n",
        "        axes[1, 2].set_title('üî¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà Digital Root (2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á)', fontweight='bold')\n",
        "        axes[1, 2].set_xlabel('Digital Root')\n",
        "        axes[1, 2].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà')\n",
        "        axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # ‡∏õ‡∏£‡∏±‡∏ö layout ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö suptitle\n",
        "        plt.show()\n",
        "        print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "\n",
        "    def predict_numbers(self, method='frequency'):\n",
        "        \"\"\"\n",
        "        ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîÆ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ: {method}\")\n",
        "\n",
        "        predictions = {}\n",
        "        digit_freq = self.analysis_results['digit_frequency']\n",
        "        last2_freq = self.analysis_results['last2_frequency']\n",
        "\n",
        "        if method == 'frequency':\n",
        "            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "            hot_digits = sorted(digit_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "            predictions['hot_digits'] = [digit for digit, _ in hot_digits]\n",
        "\n",
        "            hot_last2 = sorted(last2_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "            predictions['hot_last2'] = [number for number, _ in hot_last2]\n",
        "\n",
        "        elif method == 'cold':\n",
        "            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î\n",
        "            cold_digits = sorted(digit_freq.items(), key=lambda x: x[1])[:5]\n",
        "            predictions['cold_digits'] = [digit for digit, _ in cold_digits]\n",
        "\n",
        "            cold_last2 = sorted(last2_freq.items(), key=lambda x: x[1])[:5]\n",
        "            predictions['cold_last2'] = [number for number, _ in cold_last2]\n",
        "\n",
        "        elif method == 'digital_root':\n",
        "            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å Digital Root ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢\n",
        "            dr_prize1_freq = self.analysis_results['digital_root_prize1_freq']\n",
        "            hot_dr_prize1 = sorted(dr_prize1_freq.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            predictions['hot_digital_root_prize1'] = [str(dr) for dr, _ in hot_dr_prize1]\n",
        "\n",
        "            dr_last2_freq = self.analysis_results['digital_root_last2_freq']\n",
        "            hot_dr_last2 = sorted(dr_last2_freq.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            predictions['hot_digital_root_last2'] = [str(dr) for dr, _ in hot_dr_last2]\n",
        "\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
        "        print(\"\\nüéØ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:\")\n",
        "        if 'hot_digits' in predictions:\n",
        "            print(f\"   ‡πÄ‡∏•‡∏Ç‡∏£‡πâ‡∏≠‡∏ô (‡∏≠‡∏≠‡∏Å‡∏ö‡πà‡∏≠‡∏¢): {', '.join(predictions['hot_digits'])}\")\n",
        "            print(f\"   2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏£‡πâ‡∏≠‡∏ô: {', '.join(predictions['hot_last2'])}\")\n",
        "        if 'cold_digits' in predictions:\n",
        "            print(f\"   ‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡πá‡∏ô (‡∏≠‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏¢): {', '.join(predictions['cold_digits'])}\")\n",
        "            print(f\"   2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏¢‡πá‡∏ô: {', '.join(predictions['cold_last2'])}\")\n",
        "        if 'hot_digital_root_prize1' in predictions:\n",
        "            print(f\"   Digital Root ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {', '.join(predictions['hot_digital_root_prize1'])}\")\n",
        "            print(f\"   Digital Root 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {', '.join(predictions['hot_digital_root_last2'])}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def prepare_features_for_ml(self, df):\n",
        "        \"\"\"\n",
        "        ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning\n",
        "        ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
        "        \"\"\"\n",
        "        features_df = pd.DataFrame()\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Feature: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï (Lagged features)\n",
        "        for i in range(1, 4): # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 3 ‡∏á‡∏ß‡∏î\n",
        "            # ‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡πÄ‡∏õ‡πá‡∏ô string)\n",
        "            features_df[f'prev_prize1_str_lag{i}'] = df['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)'].shift(i)\n",
        "            # ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡πÄ‡∏õ‡πá‡∏ô string)\n",
        "            features_df[f'prev_last2_str_lag{i}'] = df['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'].shift(i)\n",
        "\n",
        "            # Digital Root (‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
        "            features_df[f'prev_dr_prize1_lag{i}'] = df['digital_root_prize1'].shift(i)\n",
        "            features_df[f'prev_dr_last2_lag{i}'] = df['digital_root_last2'].shift(i)\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Target: ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏á‡∏ß‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
        "        features_df['target_last2'] = df['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á']\n",
        "\n",
        "        # ‡πÅ‡∏õ‡∏•‡∏á string features ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô one-hot encoding\n",
        "        cols_to_dummies = [col for col in features_df.columns if re.match(r'prev_prize1_str_lag|prev_last2_str_lag', col)]\n",
        "\n",
        "        if cols_to_dummies:\n",
        "            features_df = pd.get_dummies(features_df, columns=cols_to_dummies, dummy_na=False)\n",
        "\n",
        "        features_df = features_df.dropna() # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ shift\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def train_ml_model(self, test_size_ratio=0.2):\n",
        "        \"\"\"\n",
        "        ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "        \"\"\"\n",
        "        print(f\"\\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning...\")\n",
        "\n",
        "        if self.data is None or len(self.data) < 100:\n",
        "            print(\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 100 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)\")\n",
        "            self.model = None\n",
        "            return\n",
        "\n",
        "        ml_data = self.prepare_features_for_ml(self.data.copy())\n",
        "\n",
        "        if ml_data.empty:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")\n",
        "            self.model = None\n",
        "            return\n",
        "\n",
        "        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î X (Features) ‡πÅ‡∏•‡∏∞ y (Target)\n",
        "        X = ml_data.drop('target_last2', axis=1)\n",
        "        y = ml_data['target_last2']\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Features ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
        "        if numeric_cols.empty:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ Features ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥ One-Hot Encoding\")\n",
        "            self.model = None\n",
        "            return\n",
        "\n",
        "        X = X[numeric_cols] # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô y\n",
        "        if len(y.unique()) < 2:\n",
        "            print(\"‚ùå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏Ç‡∏≠‡∏á Target (‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á) ‡∏°‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML\")\n",
        "            self.model = None\n",
        "            return\n",
        "\n",
        "        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training ‡πÅ‡∏•‡∏∞ Test set\n",
        "        try:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_ratio, random_state=42, stratify=y)\n",
        "        except ValueError as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ class ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ): {e}\")\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_ratio, random_state=42) # ‡∏•‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify\n",
        "            if len(X_train) == 0 or len(X_test) == 0:\n",
        "                print(\"‚ùå ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ test_size_ratio\")\n",
        "                self.model = None\n",
        "                return\n",
        "\n",
        "        print(f\"   ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: {len(X_train)} ‡∏ä‡∏∏‡∏î, ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {len(X_test)} ‡∏ä‡∏∏‡∏î\")\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÉ‡∏ä‡πâ Logistic Regression (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏î‡πâ)\n",
        "        self.model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=2000, n_jobs=-1) # ‡πÄ‡∏û‡∏¥‡πà‡∏° max_iter ‡πÅ‡∏•‡∏∞ n_jobs\n",
        "        try:\n",
        "            self.model.fit(X_train, y_train)\n",
        "            y_pred = self.model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"‚úÖ ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á): {accuracy:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML: {e}\")\n",
        "            self.model = None\n",
        "\n",
        "    def predict_with_ml(self, num_predictions=1):\n",
        "        \"\"\"\n",
        "        ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ùó ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML. ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å train_ml_model() ‡∏Å‡πà‡∏≠‡∏ô.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"\\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ML...\")\n",
        "\n",
        "        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
        "        rows_needed_for_features = 3 # ‡∏à‡∏≤‡∏Å lag 1-3\n",
        "\n",
        "        if len(self.data) < rows_needed_for_features:\n",
        "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ML (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ {rows_needed_for_features} ‡∏á‡∏ß‡∏î)\")\n",
        "            return []\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
        "        latest_data_for_features = self.data.head(rows_needed_for_features).copy()\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
        "        temp_features_df = self.prepare_features_for_ml(self.data.copy())\n",
        "\n",
        "        if temp_features_df.empty:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ML ‡πÑ‡∏î‡πâ. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö prepare_features_for_ml\")\n",
        "            return []\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á X_predict ‡∏à‡∏≤‡∏Å row ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á temp_features_df\n",
        "        X_predict = temp_features_df.drop('target_last2', axis=1).iloc[-1:].copy()\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô train\n",
        "        numeric_cols_trained = self.model.feature_names_in_ if hasattr(self.model, 'feature_names_in_') else X_predict.select_dtypes(include=np.number).columns\n",
        "        X_predict = X_predict[numeric_cols_trained]\n",
        "\n",
        "        if X_predict.empty or X_predict.shape[1] != len(numeric_cols_trained):\n",
        "            print(\"‚ùå ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ Features ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô\")\n",
        "            print(f\"   Shape of X_predict: {X_predict.shape}\")\n",
        "            print(f\"   Expected features: {len(numeric_cols_trained)}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô\n",
        "            predicted_proba = self.model.predict_proba(X_predict)[0]\n",
        "\n",
        "            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢\n",
        "            sorted_proba_indices = np.argsort(predicted_proba)[::-1]\n",
        "\n",
        "            top_predictions = []\n",
        "            for i in range(min(num_predictions, len(sorted_proba_indices))):\n",
        "                idx = sorted_proba_indices[i]\n",
        "                predicted_num = self.model.classes_[idx]\n",
        "                probability = predicted_proba[idx]\n",
        "                top_predictions.append((predicted_num, probability))\n",
        "\n",
        "            print(f\"   ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏î‡∏¢ ML:\")\n",
        "            for num, prob in top_predictions:\n",
        "                print(f\"     {num} (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: {prob:.2%})\")\n",
        "            return top_predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ML: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "\n",
        "    def backtest_predictions(self, test_size=50):\n",
        "        \"\"\"\n",
        "        ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "        \"\"\"\n",
        "        print(f\"\\nüß™ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û (‡∏ó‡∏î‡∏™‡∏≠‡∏ö {test_size} ‡∏á‡∏ß‡∏î)...\")\n",
        "\n",
        "        if len(self.data) < test_size + 100: # ‡πÄ‡∏û‡∏¥‡πà‡∏° buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train data\n",
        "            print(\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 150 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)\")\n",
        "            return None\n",
        "\n",
        "        # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå\n",
        "        results = {\n",
        "            'frequency': {'correct_digits': 0, 'correct_last2': 0, 'total_digit_attempts': 0, 'total_last2_attempts': 0},\n",
        "            'cold': {'correct_digits': 0, 'correct_last2': 0, 'total_digit_attempts': 0, 'total_last2_attempts': 0},\n",
        "            'ml_last2': {'correct_last2': 0, 'total_last2_attempts': 0} # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML\n",
        "        }\n",
        "\n",
        "        train_end_index = len(self.data) - test_size\n",
        "        if train_end_index <= 0:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Backtest ‡πÑ‡∏î‡πâ. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ.\")\n",
        "            return None\n",
        "\n",
        "        historical_data_for_backtest = self.data.iloc[test_size:].copy()\n",
        "        test_data_for_backtest = self.data.iloc[:test_size].copy()\n",
        "\n",
        "        for i in range(test_size):\n",
        "            current_train_data = historical_data_for_backtest.iloc[i:].copy()\n",
        "\n",
        "            if len(current_train_data) < 50:\n",
        "                continue\n",
        "\n",
        "            temp_analyzer = LotteryAnalyzer()\n",
        "            temp_analyzer.data = current_train_data\n",
        "            temp_analyzer.analyze_frequency()\n",
        "\n",
        "            actual_row = test_data_for_backtest.iloc[i]\n",
        "            actual_prize1 = actual_row['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)']\n",
        "            actual_last2 = actual_row['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á']\n",
        "\n",
        "            # --- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Frequency ---\n",
        "            freq_predictions = temp_analyzer.predict_numbers('frequency')\n",
        "            predicted_hot_digits = freq_predictions.get('hot_digits', [])\n",
        "            predicted_hot_last2 = freq_predictions.get('hot_last2', [])\n",
        "\n",
        "            results['frequency']['total_digit_attempts'] += 6\n",
        "            results['frequency']['total_last2_attempts'] += 1\n",
        "\n",
        "            for digit in actual_prize1:\n",
        "                if digit in predicted_hot_digits:\n",
        "                    results['frequency']['correct_digits'] += 1\n",
        "            if actual_last2 in predicted_hot_last2:\n",
        "                results['frequency']['correct_last2'] += 1\n",
        "\n",
        "            # --- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Cold ---\n",
        "            cold_predictions = temp_analyzer.predict_numbers('cold')\n",
        "            predicted_cold_digits = cold_predictions.get('cold_digits', [])\n",
        "            predicted_cold_last2 = cold_predictions.get('cold_last2', [])\n",
        "\n",
        "            results['cold']['total_digit_attempts'] += 6\n",
        "            results['cold']['total_last2_attempts'] += 1\n",
        "\n",
        "            for digit in actual_prize1:\n",
        "                if digit in predicted_cold_digits:\n",
        "                    results['cold']['correct_digits'] += 1\n",
        "            if actual_last2 in predicted_cold_last2:\n",
        "                results['cold']['correct_last2'] += 1\n",
        "\n",
        "            # --- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ML ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á ---\n",
        "            rows_needed_for_ml_features = 3\n",
        "            min_train_data_for_ml = 100\n",
        "\n",
        "            if len(current_train_data) >= min_train_data_for_ml:\n",
        "                try:\n",
        "                    temp_analyzer.train_ml_model(test_size_ratio=0.0)\n",
        "                    if temp_analyzer.model:\n",
        "                        if len(current_train_data) >= rows_needed_for_ml_features:\n",
        "                            ml_input_df_for_prediction = temp_analyzer.prepare_features_for_ml(current_train_data.head(rows_needed_for_ml_features))\n",
        "\n",
        "                            if not ml_input_df_for_prediction.empty:\n",
        "                                X_ml_predict = ml_input_df_for_prediction.drop('target_last2', axis=1).iloc[-1:].copy()\n",
        "\n",
        "                                expected_features = temp_analyzer.model.feature_names_in_ if hasattr(temp_analyzer.model, 'feature_names_in_') else []\n",
        "                                if expected_features:\n",
        "                                    missing_cols = set(expected_features) - set(X_ml_predict.columns)\n",
        "                                    for c in missing_cols:\n",
        "                                        X_ml_predict[c] = 0\n",
        "                                    X_ml_predict = X_ml_predict[expected_features]\n",
        "\n",
        "                                    if not X_ml_predict.empty and X_ml_predict.shape[1] == len(expected_features):\n",
        "                                        ml_predicted_last2_raw = temp_analyzer.model.predict(X_ml_predict)\n",
        "                                        ml_predicted_last2 = ml_predicted_last2_raw[0] if len(ml_predicted_last2_raw) > 0 else None\n",
        "\n",
        "                                        results['ml_last2']['total_last2_attempts'] += 1\n",
        "                                        if ml_predicted_last2 == actual_last2:\n",
        "                                            results['ml_last2']['correct_last2'] += 1\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "        print(\"\\nüìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for method, result in results.items():\n",
        "            if method == 'ml_last2':\n",
        "                if result['total_last2_attempts'] == 0:\n",
        "                    print(f\"\\nüéØ ‡∏ß‡∏¥‡∏ò‡∏µ: Machine Learning (‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á) - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö/‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô\")\n",
        "                    continue\n",
        "                else:\n",
        "                    method_name = \"ü§ñ Machine Learning (‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á)\"\n",
        "                    last2_accuracy = (result['correct_last2'] / result['total_last2_attempts']) * 100\n",
        "                    print(f\"\\nüéØ ‡∏ß‡∏¥‡∏ò‡∏µ: {method_name}\")\n",
        "                    print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (ML): {last2_accuracy:.1f}%\")\n",
        "                    print(f\"   ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {result['total_last2_attempts']} ‡∏á‡∏ß‡∏î\")\n",
        "            else:\n",
        "                method_name = \"üî• ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏™‡∏π‡∏á\" if method == 'frequency' else \"‚ùÑÔ∏è ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡πà‡∏≥\"\n",
        "                if result['total_digit_attempts'] > 0:\n",
        "                    digit_accuracy = (result['correct_digits'] / result['total_digit_attempts']) * 100\n",
        "                    print(f\"\\nüéØ ‡∏ß‡∏¥‡∏ò‡∏µ: {method_name}\")\n",
        "                    print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏•‡∏Ç‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å: {digit_accuracy:.1f}%\")\n",
        "\n",
        "                if result['total_last2_attempts'] > 0:\n",
        "                    last2_accuracy = (result['correct_last2'] / result['total_last2_attempts']) * 100\n",
        "                    print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {last2_accuracy:.1f}%\")\n",
        "\n",
        "                print(f\"   ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {result['total_last2_attempts']} ‡∏á‡∏ß‡∏î\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_recommendations(self):\n",
        "        \"\"\"\n",
        "        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "        \"\"\"\n",
        "        print(\"\\nüåü ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\n",
        "        if self.data is None or self.data.empty:\n",
        "            print(\"   ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥.\")\n",
        "            return\n",
        "\n",
        "        recent_data = self.data.head(10)\n",
        "        if recent_data.empty:\n",
        "            print(\"   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 10 ‡∏á‡∏ß‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥.\")\n",
        "            return\n",
        "\n",
        "        recent_digits = Counter()\n",
        "        for number in recent_data['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)']:\n",
        "            for digit in str(number):\n",
        "                recent_digits[digit] += 1\n",
        "\n",
        "        # ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á)\n",
        "        all_last2_numbers = [f\"{i:02d}\" for i in range(100)] # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏•‡∏Ç 00-99\n",
        "        recent_last2 = set(recent_data['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'])\n",
        "\n",
        "        # ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (10 ‡∏á‡∏ß‡∏î)\n",
        "        missing_last2_recent = sorted([num for num in all_last2_numbers if num not in recent_last2])\n",
        "\n",
        "        print(f\"\\nüî• ‡πÄ‡∏•‡∏Ç‡∏£‡πâ‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î):\")\n",
        "        recent_hot = sorted(recent_digits.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        if recent_hot:\n",
        "            for digit, freq in recent_hot:\n",
        "                print(f\"   ‡πÄ‡∏•‡∏Ç {digit}: {freq} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "        else:\n",
        "            print(\"   ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏Ç‡∏£‡πâ‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\")\n",
        "\n",
        "        print(f\"\\n‚ùÑÔ∏è ‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô 10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:\")\n",
        "        # ‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß 0-9 ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á 10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
        "        missing_digits_recent = set('0123456789') - set(recent_digits.keys())\n",
        "        if missing_digits_recent:\n",
        "            print(f\"   {', '.join(sorted(list(missing_digits_recent)))}\")\n",
        "        else:\n",
        "            print(\"   ‡πÑ‡∏°‡πà‡∏°‡∏µ (‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô 10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\")\n",
        "\n",
        "        print(f\"\\nüéØ ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\")\n",
        "        # ‡∏£‡∏ß‡∏°‡πÄ‡∏•‡∏Ç‡∏Æ‡∏¥‡∏ï‡∏Å‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏ô\n",
        "        last2_freq = self.analysis_results['last2_frequency']\n",
        "        hot_last2 = sorted(last2_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "        print(\"   ‡πÄ‡∏•‡∏Ç‡∏Æ‡∏¥‡∏ï (‡∏≠‡∏≠‡∏Å‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):\")\n",
        "        if hot_last2:\n",
        "            for number, freq in hot_last2:\n",
        "                print(f\"     {number} ({freq} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)\")\n",
        "        else:\n",
        "            print(\"     ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏Æ‡∏¥‡∏ï\")\n",
        "\n",
        "        if missing_last2_recent:\n",
        "            print(\"   ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô 10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):\")\n",
        "            sample_missing = sorted(missing_last2_recent)[:10] # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "            for number in sample_missing:\n",
        "                print(f\"     {number}\")\n",
        "        else:\n",
        "            print(\"   ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô 10 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\")\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å Digital Root\n",
        "        recommended_dr_prize1 = None\n",
        "        if self.analysis_results and 'digital_root_prize1_freq' in self.analysis_results and self.analysis_results['digital_root_prize1_freq']:\n",
        "            recommended_dr_prize1 = sorted(self.analysis_results['digital_root_prize1_freq'].items(), key=lambda x: x[1], reverse=True)[0][0] # DR ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î\n",
        "\n",
        "        recommended_dr_last2 = None\n",
        "        if self.analysis_results and 'digital_root_last2_freq' in self.analysis_results and self.analysis_results['digital_root_last2_freq']:\n",
        "            recommended_dr_last2 = sorted(self.analysis_results['digital_root_last2_freq'].items(), key=lambda x: x[1], reverse=True)[0][0] # DR ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î\n",
        "\n",
        "        print(f\"\\nüí° ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\")\n",
        "        print(f\"   - ‡πÄ‡∏•‡∏Ç‡∏£‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à (‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß): {', '.join([d for d, _ in recent_hot[:3]]) if recent_hot else 'N/A'}\")\n",
        "        print(f\"   - ‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏±‡∏ö‡∏ï‡∏≤ (‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß): {', '.join(sorted(list(missing_digits_recent))[:3]) if missing_digits_recent else 'N/A'}\")\n",
        "        print(f\"   - 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà: {hot_last2[0][0] if hot_last2 else 'N/A'}\")\n",
        "\n",
        "        if self.model:\n",
        "            ml_predictions = self.predict_with_ml(num_predictions=3) # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 3 ‡∏ï‡∏±‡∏ß‡∏î‡πâ‡∏ß‡∏¢ ML\n",
        "            if ml_predictions:\n",
        "                print(f\"   - 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å ML: {', '.join([p[0] for p in ml_predictions])}\")\n",
        "            else:\n",
        "                print(\"   - 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å ML: N/A (‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ)\")\n",
        "        else:\n",
        "            print(\"   - 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å ML: N/A (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô)\")\n",
        "\n",
        "        print(f\"   - Digital Root ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢: {recommended_dr_prize1 if recommended_dr_prize1 is not None else 'N/A'}\")\n",
        "        print(f\"   - Digital Root 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢: {recommended_dr_last2 if recommended_dr_last2 is not None else 'N/A'}\")\n",
        "\n",
        "\n",
        "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏ß‡∏¢\n",
        "    \"\"\"\n",
        "    print(\"üé∞ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏≠‡∏ô‡∏≤‡πÑ‡∏•‡πÄ‡∏ã‡∏≠‡∏£‡πå\n",
        "    analyzer = LotteryAnalyzer()\n",
        "\n",
        "    # ‡∏£‡∏∞‡∏ö‡∏∏ path ‡πÑ‡∏ü‡∏•‡πå CSV\n",
        "    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ uploaded_file_name ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÅ‡∏£‡∏Å\n",
        "    global uploaded_file_name\n",
        "\n",
        "    file_path_to_use = None\n",
        "    if 'uploaded_file_name' in globals() and uploaded_file_name is not None:\n",
        "        file_path_to_use = uploaded_file_name\n",
        "        print(f\"üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: '{file_path_to_use}'\")\n",
        "    else:\n",
        "        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\n",
        "        print(\"‚ùó ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤. ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô '‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv' ‡πÅ‡∏ó‡∏ô\")\n",
        "        file_path_to_use = \"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv\" # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ (‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Colab ‡πÑ‡∏î‡πâ)\n",
        "\n",
        "    try:\n",
        "        # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "        df = analyzer.load_and_clean_data(file_path_to_use)\n",
        "\n",
        "        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà\n",
        "        results = analyzer.analyze_frequency()\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
        "        analyzer.display_analysis()\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü\n",
        "        analyzer.create_visualizations()\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç\n",
        "        analyzer.predict_numbers('frequency')\n",
        "        analyzer.predict_numbers('cold')\n",
        "        analyzer.predict_numbers('digital_root') # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Digital Root\n",
        "\n",
        "        # ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning\n",
        "        analyzer.train_ml_model()\n",
        "\n",
        "        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û (‡∏£‡∏ß‡∏° ML ‡∏î‡πâ‡∏ß‡∏¢)\n",
        "        analyzer.backtest_predictions(30) # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n",
        "        analyzer.generate_recommendations()\n",
        "\n",
        "        print(f\"\\n‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}\")\n",
        "        print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\")\n",
        "    except KeyError as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î KeyError: {str(e)}\")\n",
        "        print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á ('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á') ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô load_and_clean_data ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # ‡∏û‡∏¥‡∏°‡∏û‡πå stack trace ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ debug\n",
        "\n",
        "# ‡∏£‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXHFaJ1ZQ1Xq",
        "outputId": "cc9acb59-f7e2-4186-fece-ff0956fddd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé∞ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢\n",
            "==================================================\n",
            "‚ùó ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤. ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô '‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv' ‡πÅ‡∏ó‡∏ô\n",
            "üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: '‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\n",
            "üîç ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô):\n",
            "['<artifact identifier=\"lottery_csv\" type=\"text/plain\" title=\"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 20.csv\">', 'Unnamed: 1', 'Unnamed: 2']\n",
            "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î KeyError: '‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \\'‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\\' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: [\\'<artifact identifier=\"lottery_csv\" type=\"text/plain\" title=\"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 20.csv\">\\', \\'Unnamed: 1\\', \\'Unnamed: 2\\']'\n",
            "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á ('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á') ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô load_and_clean_data ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á\n"
          ]
        }
      ]
    }
  ]
}