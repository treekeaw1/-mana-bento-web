{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeJ0BP5rpGY5vmIwMi5Wsj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a48423f8e2e4b728844b91461598f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e289ec2f481a4ac1b0408b3933f86461",
              "IPY_MODEL_2da732da55fd4b1cab631d6c30c74b23",
              "IPY_MODEL_7ad9c7a46a384c239bb63a2d2e39651b"
            ],
            "layout": "IPY_MODEL_34b2390db6b84b47807a75f4cb523a83"
          }
        },
        "e289ec2f481a4ac1b0408b3933f86461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4da717f7527f450ab8d669cb46c3adfd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_751858a32600460ab6e9f294622b5190",
            "value": "Processing‚Äá6d:‚Äá100%"
          }
        },
        "2da732da55fd4b1cab631d6c30c74b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02c32d2c0b1f4f29b2cde7e6881e5bf2",
            "max": 554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbdb11e358624b7c9dbfc9c68a105170",
            "value": 554
          }
        },
        "7ad9c7a46a384c239bb63a2d2e39651b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c83f55963efe42279304f45bcacbe176",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc4ba4e705ad4f6fb870be0ea8afce70",
            "value": "‚Äá554/554‚Äá[00:00&lt;00:00,‚Äá1073.80it/s]"
          }
        },
        "34b2390db6b84b47807a75f4cb523a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da717f7527f450ab8d669cb46c3adfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "751858a32600460ab6e9f294622b5190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02c32d2c0b1f4f29b2cde7e6881e5bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbdb11e358624b7c9dbfc9c68a105170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c83f55963efe42279304f45bcacbe176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4ba4e705ad4f6fb870be0ea8afce70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treekeaw1/-mana-bento-web/blob/main/Untitled37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Libraries and Setup ---\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from io import StringIO, BytesIO\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "import math # For sqrt in isPrime\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Machine Learning Tools\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
        "\n",
        "class UltimateSynergyEngine:\n",
        "    def __init__(self, window_size=6):\n",
        "        self.data = []\n",
        "        self.window_size = window_size\n",
        "        self.expert_models = {'6d': {}, '2d': {}}\n",
        "        self.meta_models = {'6d': [None]*6, '2d': [None]*2}\n",
        "        self.feature_extractors = {'6d': None, '2d': None}\n",
        "        self.math_analysis_results = None # To store results from mathematical analysis\n",
        "        self.math_prediction = None # To store prediction from mathematical analysis\n",
        "        self.synergy_prediction_results = {'6d': 'N/A', '2d': 'N/A'} # New: To store combined synergy prediction\n",
        "        self.flow_pattern_analysis_results = None # New: To store flow pattern analysis results\n",
        "\n",
        "        # Mapping for detailed position descriptions in a combined 8-digit string (left to right)\n",
        "        # This mapping assumes the combined string is 'two_digit' (indices 0-1) + 'six_digit' (indices 2-7)\n",
        "        self.combined_digit_positions = {\n",
        "            0: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            1: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)',\n",
        "            2: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏™‡∏ô)',\n",
        "            3: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô)',\n",
        "            4: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)',\n",
        "            5: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢)',\n",
        "            6: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            7: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)'\n",
        "        }\n",
        "\n",
        "        print(\"üöÄ Ultimate Synergy Engine ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Window Size = 6)\")\n",
        "        print(\"   - ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\")\n",
        "\n",
        "    # --- 1. ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£) ---\n",
        "    def load_data_from_file(self):\n",
        "        print(\"\\n\" + \"=\"*80); print(\"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\"); print(\"=\"*80)\n",
        "        print(\"üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded: print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå\"); return False\n",
        "        filepath = list(uploaded.keys())[0]; file_content = uploaded[filepath]\n",
        "        print(f\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {filepath}\")\n",
        "        try:\n",
        "            # ‡πÉ‡∏ä‡πâ skiprows=[0, 1] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≤‡∏° 2 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "            # ‡πÅ‡∏•‡∏∞ header=None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß\n",
        "            # ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏î‡πâ‡∏ß‡∏¢ names\n",
        "            df = pd.read_csv(BytesIO(file_content), encoding='utf-8-sig',\n",
        "                             skiprows=[0, 1], # Skip the first two rows (index 0 and 1)\n",
        "                             header=None, # No header row in the file as we are providing names\n",
        "                             names=['date', 'six_digit', 'two_digit']) # Manually assign column names\n",
        "\n",
        "            # Convert 'date' column to datetime objects\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "            # Remove rows with NaT in the 'date' column (invalid or incomplete dates)\n",
        "            df.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "            # Convert 'six_digit' and 'two_digit' columns to numeric.\n",
        "            df['six_digit'] = pd.to_numeric(df['six_digit'], errors='coerce')\n",
        "            df['two_digit'] = pd.to_numeric(df['two_digit'], errors='coerce')\n",
        "\n",
        "            # Remove rows where 'six_digit' or 'two_digit' are NaN (meaning they were initially empty or invalid numbers).\n",
        "            df.dropna(subset=['six_digit', 'two_digit'], inplace=True)\n",
        "\n",
        "            # Convert numbers to integers, then to strings, and pad with leading zeros.\n",
        "            df['six_digit'] = df['six_digit'].astype(int).astype(str).str.zfill(6)\n",
        "            df['two_digit'] = df['two_digit'].astype(int).astype(str).str.zfill(2)\n",
        "\n",
        "            self.data = df.sort_values(by='date', ascending=True).to_dict('records')\n",
        "            print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(self.data)} ‡πÅ‡∏ñ‡∏ß\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    # --- Helper: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ---\n",
        "    def _create_date_features(self, date_obj, prev_date_obj=None):\n",
        "        date_features = {}\n",
        "        date_features['target_year'] = date_obj.year\n",
        "        date_features['target_month'] = date_obj.month\n",
        "        date_features['target_day'] = date_obj.day\n",
        "        date_features['target_dayofweek'] = date_obj.dayofweek\n",
        "        date_features['target_month_sin'] = np.sin(2 * np.pi * date_obj.month / 12)\n",
        "        date_features['target_month_cos'] = np.cos(2 * np.pi * date_obj.month / 12)\n",
        "        date_features['target_dayofweek_sin'] = np.sin(2 * np.pi * date_obj.dayofweek / 7)\n",
        "        date_features['target_dayofweek_cos'] = np.cos(2 * np.pi * date_obj.dayofweek / 7)\n",
        "\n",
        "        if prev_date_obj:\n",
        "            time_diff_days = (date_obj - prev_date_obj).days\n",
        "            date_features['time_since_last_draw_days'] = time_diff_days\n",
        "        else:\n",
        "            date_features['time_since_last_draw_days'] = 0\n",
        "        return date_features\n",
        "\n",
        "    # --- 2. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Feature (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Model) ---\n",
        "    def _create_handcrafted_features(self, history_matrix, num_digits):\n",
        "        features = {}; num_rows, num_cols = history_matrix.shape;\n",
        "\n",
        "        last_row = history_matrix[-1, :] if num_rows > 0 else np.zeros(num_digits)\n",
        "\n",
        "        for col in range(num_cols):\n",
        "            col_data = history_matrix[:, col] if num_rows > 0 else np.zeros(self.window_size)\n",
        "            features[f'hydro_mean_c{col}'] = np.mean(col_data)\n",
        "            features[f'hydro_std_c{col}'] = np.std(col_data)\n",
        "            features[f'hydro_last_val_c{col}'] = col_data[-1] if num_rows > 0 else 0\n",
        "\n",
        "            features[f'even_count_c{col}'] = np.sum(col_data % 2 == 0)\n",
        "            features[f'odd_count_c{col}'] = np.sum(col_data % 2 != 0)\n",
        "            features[f'high_count_c{col}'] = np.sum(col_data >= 5)\n",
        "            features[f'low_count_c{col}'] = np.sum(col_data < 5)\n",
        "\n",
        "        for r in range(num_rows):\n",
        "            for c in range(num_cols): features[f'cyc_D{r+1}_P{c+1}'] = history_matrix[r, c]\n",
        "        for r in range(num_rows, self.window_size):\n",
        "            for c in range(num_cols):\n",
        "                features[f'cyc_D{r+1}_P{c+1}'] = 0\n",
        "\n",
        "        for col in range(num_cols):\n",
        "            streak = 0\n",
        "            if num_rows > 0:\n",
        "                val_to_check = history_matrix[-1, col]\n",
        "                for r in range(num_rows - 2, -1, -1):\n",
        "                    if history_matrix[r, col] == val_to_check: streak += 1\n",
        "                    else: break\n",
        "            features[f'patt_streak_c{col}'] = streak\n",
        "\n",
        "        if num_rows > 0:\n",
        "            features['patt_sum_last_draw'] = np.sum(last_row)\n",
        "            features['patt_even_last_draw'] = np.sum(last_row % 2 == 0)\n",
        "            features['patt_odd_last_draw'] = np.sum(last_row % 2 != 0)\n",
        "            features['patt_high_last_draw'] = np.sum(last_row >= 5)\n",
        "            features['patt_low_last_draw'] = np.sum(last_row < 5)\n",
        "\n",
        "            all_digits_in_window = history_matrix.flatten()\n",
        "            digit_counts_window = Counter(all_digits_in_window)\n",
        "            for d in range(10):\n",
        "                features[f'freq_digit_{d}_window'] = digit_counts_window[d]\n",
        "\n",
        "            digit_counts_last = Counter(last_row)\n",
        "            for d in range(10):\n",
        "                features[f'freq_digit_{d}_last'] = digit_counts_last[d]\n",
        "\n",
        "            features['patt_diff_c0_c1'] = abs(last_row[0] - last_row[1]) if num_cols > 1 else 0\n",
        "            features['patt_diff_c1_c2'] = abs(last_row[1] - last_row[2]) if num_cols > 2 else 0\n",
        "\n",
        "            if num_cols == 6:\n",
        "                features['patt_sum_first3'] = np.sum(last_row[:3])\n",
        "                features['patt_sum_last3'] = np.sum(last_row[-3:])\n",
        "                features['patt_diff_first_last'] = abs(last_row[0] - last_row[-1])\n",
        "                features['patt_sum_all_digits'] = np.sum(last_row)\n",
        "            else:\n",
        "                features['patt_sum_first3'] = 0\n",
        "                features['patt_sum_last3'] = 0\n",
        "                features['patt_diff_first_last'] = 0\n",
        "                features['patt_sum_all_digits'] = 0\n",
        "\n",
        "            # --- New Features from previous iteration ---\n",
        "            unique_digits_in_window, counts_in_window = np.unique(all_digits_in_window, return_counts=True)\n",
        "            probabilities_in_window = counts_in_window / (len(all_digits_in_window) + 1e-9)\n",
        "            features['entropy_window'] = -np.sum(probabilities_in_window * np.log2(probabilities_in_window + 1e-9)) if len(all_digits_in_window) > 0 else 0\n",
        "\n",
        "            for col in range(num_cols):\n",
        "                features[f'moving_avg_c{col}'] = np.mean(history_matrix[:, col])\n",
        "\n",
        "            if num_rows >= 3:\n",
        "                last_3_rows = history_matrix[-3:, :]\n",
        "                for col in range(num_cols):\n",
        "                    features[f'last3_sum_c{col}'] = np.sum(last_3_rows[:, col])\n",
        "                    features[f'last3_avg_c{col}'] = np.mean(last_3_rows[:, col])\n",
        "            else:\n",
        "                for col in range(num_cols):\n",
        "                    features[f'last3_sum_c{col}'] = 0\n",
        "                    features[f'last3_avg_c{col}'] = 0\n",
        "\n",
        "            features['unique_digits_last_draw'] = len(np.unique(last_row))\n",
        "\n",
        "            parity_changes = 0\n",
        "            if num_cols > 1:\n",
        "                for i in range(num_cols - 1):\n",
        "                    if (last_row[i] % 2) != (last_row[i+1] % 2):\n",
        "                        parity_changes += 1\n",
        "            features['parity_changes_last_draw'] = parity_changes\n",
        "\n",
        "            product_sum = 0\n",
        "            if num_cols == 6:\n",
        "                for i in range(num_cols - 1):\n",
        "                    product_sum += last_row[i] * last_row[i+1]\n",
        "            features['sum_product_adjacent'] = product_sum\n",
        "\n",
        "        else: # Default all features to 0 for empty history_matrix\n",
        "            default_features = {\n",
        "                'patt_sum_last_draw': 0, 'patt_even_last_draw': 0, 'patt_odd_last_draw': 0,\n",
        "                'patt_high_last_draw': 0, 'patt_low_last_draw': 0,\n",
        "                'patt_diff_c0_c1': 0, 'patt_diff_c1_c2': 0,\n",
        "                'patt_sum_first3': 0, 'patt_sum_last3': 0,\n",
        "                'patt_diff_first_last': 0, 'patt_sum_all_digits': 0,\n",
        "                'entropy_window': 0, 'unique_digits_last_draw': 0,\n",
        "                'parity_changes_last_draw': 0, 'sum_product_adjacent': 0\n",
        "            }\n",
        "            for d in range(10):\n",
        "                default_features[f'freq_digit_{d}_window'] = 0\n",
        "                default_features[f'freq_digit_{d}_last'] = 0\n",
        "            for col in range(num_digits):\n",
        "                default_features[f'hydro_mean_c{col}'] = 0\n",
        "                default_features[f'hydro_std_c{col}'] = 0\n",
        "                default_features[f'hydro_last_val_c{col}'] = 0\n",
        "                default_features[f'even_count_c{col}'] = 0\n",
        "                default_features[f'odd_count_c{col}'] = 0\n",
        "                default_features[f'high_count_c{col}'] = 0\n",
        "                default_features[f'low_count_c{col}'] = 0\n",
        "                default_features[f'patt_streak_c{col}'] = 0\n",
        "                default_features[f'moving_avg_c{col}'] = 0\n",
        "                default_features[f'last3_sum_c{col}'] = 0\n",
        "                default_features[f'last3_avg_c{col}'] = 0\n",
        "            for r in range(self.window_size):\n",
        "                for c in range(num_digits):\n",
        "                    default_features[f'cyc_D{r+1}_P{c+1}'] = 0\n",
        "            features.update(default_features)\n",
        "        return features\n",
        "\n",
        "    # --- 3. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å Engine (ML Model) ---\n",
        "    def build_and_train_engine(self):\n",
        "        print(\"\\n\" + \"=\"*80); print(\"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å 'Ultimate Synergy Engine'\"); print(\"=\"*80)\n",
        "\n",
        "        for prize_type in ['6d', '2d']:\n",
        "            num_digits = 6 if prize_type == '6d' else 2; col_name = 'six_digit' if prize_type == '6d' else 'two_digit'; prize_name = '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1' if prize_type == '6d' else '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'\n",
        "            print(f\"\\n--- üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{prize_name}' ---\")\n",
        "\n",
        "            X_images, y_df, X_handcrafted_list_base, X_date_features_list = [], pd.DataFrame(), [], []\n",
        "            temp_labels = {f'pos_{i}': [] for i in range(num_digits)}\n",
        "\n",
        "            for i in tqdm(range(self.window_size, len(self.data)), desc=f\"Processing {prize_type}\"):\n",
        "                history_rows = self.data[i - self.window_size : i]; target_row = self.data[i]\n",
        "                matrix = np.array([[int(d) for d in row[col_name]] for row in history_rows])\n",
        "                X_images.append(matrix)\n",
        "\n",
        "                handcrafted_feats_base = self._create_handcrafted_features(matrix, num_digits)\n",
        "                X_handcrafted_list_base.append(handcrafted_feats_base)\n",
        "\n",
        "                target_date = target_row['date']\n",
        "                prev_date = self.data[i-1]['date'] if i > 0 else None\n",
        "                date_feats = self._create_date_features(target_date, prev_date)\n",
        "                X_date_features_list.append(date_feats)\n",
        "\n",
        "                target_digits = [int(d) for d in target_row[col_name]];\n",
        "                for pos in range(num_digits):\n",
        "                    temp_labels[f'pos_{pos}'].append(target_digits[pos])\n",
        "\n",
        "            X_images = np.array(X_images).reshape(-1, self.window_size, num_digits, 1) / 9.0\n",
        "\n",
        "            X_handcrafted_df_base = pd.DataFrame(X_handcrafted_list_base).fillna(0)\n",
        "            X_date_features_df = pd.DataFrame(X_date_features_list).fillna(0)\n",
        "            X_handcrafted_full_train = pd.concat([X_handcrafted_df_base, X_date_features_df], axis=1)\n",
        "\n",
        "            y_df = pd.DataFrame(temp_labels)\n",
        "\n",
        "            if len(X_images) == 0:\n",
        "                print(f\"‚ùó ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• '{prize_name}'. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ {self.window_size + 1} ‡∏á‡∏ß‡∏î.\")\n",
        "                continue\n",
        "\n",
        "            print(\"   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å CNN Feature Extractor...\")\n",
        "            input_shape = (self.window_size, num_digits, 1)\n",
        "            image_input = Input(shape=input_shape)\n",
        "            x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n",
        "            x = MaxPooling2D((2, 2))(x)\n",
        "            x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "            x = MaxPooling2D((1, 1))(x)\n",
        "            x = Flatten()(x)\n",
        "            vision_output = Dense(64, activation='relu')(x)\n",
        "\n",
        "            # Ensure feature_extractor is always created, even if training is skipped\n",
        "            self.feature_extractors[prize_type] = Model(inputs=image_input, outputs=vision_output)\n",
        "\n",
        "            if len(y_df['pos_0'].unique()) < 2:\n",
        "                print(f\"‚ùó 'pos_0' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{prize_name}' ‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß. CNN Feature Extractor ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤. ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å CNN.\")\n",
        "                vision_features = self.feature_extractors[prize_type].predict(X_images, verbose=0) # Still predict to get features\n",
        "            else:\n",
        "                temp_model = Model(inputs=image_input, outputs=Dense(10, activation='softmax')(vision_output))\n",
        "                temp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "                temp_model.fit(X_images, to_categorical(y_df['pos_0'], num_classes=10), epochs=50, batch_size=32, verbose=0)\n",
        "                vision_features = self.feature_extractors[prize_type].predict(X_images, verbose=0)\n",
        "\n",
        "            hybrid_features = np.concatenate([vision_features, X_handcrafted_full_train.values], axis=1)\n",
        "\n",
        "            print(f\"\\n--- üë• ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{prize_name}' ---\")\n",
        "            for target_pos in range(num_digits):\n",
        "                print(f\"  - ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_pos + 1}:\")\n",
        "                y = y_df[f'pos_{target_pos}']\n",
        "\n",
        "                if len(y.unique()) < 2:\n",
        "                    single_unique_digit = y.iloc[0]\n",
        "                    print(f\"    ‚ùó ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_pos + 1} ‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ({single_unique_digit}). ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢.\")\n",
        "                    # Mark experts and meta-model to return this constant\n",
        "                    self.meta_models[prize_type][target_pos] = f'CONSTANT:{single_unique_digit}'\n",
        "                    self.expert_models[prize_type][f'expert_A_pos_{target_pos}'] = f'CONSTANT:{single_unique_digit}'\n",
        "                    self.expert_models[prize_type][f'expert_B_pos_{target_pos}'] = f'CONSTANT:{single_unique_digit}'\n",
        "                    self.expert_models[prize_type][f'expert_C_pos_{target_pos}'] = f'CONSTANT:{single_unique_digit}'\n",
        "                    self.expert_models[prize_type][f'expert_D_pos_{target_pos}'] = f'CONSTANT:{single_unique_digit}'\n",
        "                    continue # Skip training for this position\n",
        "\n",
        "                kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                meta_features = np.zeros((len(y), 4))\n",
        "\n",
        "                # --- Hyperparameter Search Spaces for Experts ---\n",
        "                xgb_vision_param_dist = {\n",
        "                    'n_estimators': randint(200, 600), 'learning_rate': uniform(0.01, 0.15),\n",
        "                    'max_depth': randint(3, 8), 'subsample': uniform(0.6, 0.4),\n",
        "                    'colsample_bytree': uniform(0.6, 0.4)\n",
        "                }\n",
        "                xgb_hist_param_dist = {\n",
        "                    'n_estimators': randint(200, 600), 'learning_rate': uniform(0.01, 0.15),\n",
        "                    'max_depth': randint(3, 8), 'subsample': uniform(0.6, 0.4),\n",
        "                    'colsample_bytree': uniform(0.6, 0.4)\n",
        "                }\n",
        "                lgb_hybrid_param_dist = {\n",
        "                    'n_estimators': randint(200, 600), 'learning_rate': uniform(0.01, 0.15),\n",
        "                    'num_leaves': randint(20, 60), 'max_depth': randint(3, 8),\n",
        "                    'subsample': uniform(0.6, 0.4), 'colsample_bytree': uniform(0.6, 0.4)\n",
        "                }\n",
        "                rf_param_dist = {\n",
        "                    'n_estimators': randint(100, 500), 'max_depth': randint(5, 15),\n",
        "                    'min_samples_split': randint(2, 10), 'min_samples_leaf': randint(1, 5)\n",
        "                }\n",
        "\n",
        "                # --- Cross-Validation and Expert Prediction Collection ---\n",
        "                for fold, (train_idx, val_idx) in enumerate(kf.split(hybrid_features, y)):\n",
        "                    train_idx_np = np.array(train_idx)\n",
        "                    val_idx_np = np.array(val_idx)\n",
        "\n",
        "                    # Expert A (XGBoost - Visionary)\n",
        "                    xgb_A_model = xgb.XGBClassifier(objective='multi:softprob', num_class=10, use_label_encoder=False, eval_metric='mlogloss', seed=42)\n",
        "                    rand_search_A = RandomizedSearchCV(xgb_A_model, param_distributions=xgb_vision_param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
        "                    rand_search_A.fit(vision_features[train_idx_np], y.iloc[train_idx_np])\n",
        "                    expert_A = rand_search_A.best_estimator_\n",
        "                    meta_features[val_idx_np, 0] = expert_A.predict(vision_features[val_idx_np])\n",
        "\n",
        "                    # Expert B (XGBoost - Historian)\n",
        "                    xgb_B_model = xgb.XGBClassifier(objective='multi:softprob', num_class=10, use_label_encoder=False, eval_metric='mlogloss', seed=42)\n",
        "                    rand_search_B = RandomizedSearchCV(xgb_B_model, param_distributions=xgb_hist_param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
        "                    rand_search_B.fit(X_handcrafted_full_train.iloc[train_idx_np], y.iloc[train_idx_np])\n",
        "                    expert_B = rand_search_B.best_estimator_\n",
        "                    meta_features[val_idx_np, 1] = expert_B.predict(X_handcrafted_full_train.iloc[val_idx_np])\n",
        "\n",
        "                    # Expert C (LightGBM - Hybrid Master)\n",
        "                    lgb_C_model = lgb.LGBMClassifier(objective='multiclass', num_class=10, seed=42, verbose=-1)\n",
        "                    rand_search_C = RandomizedSearchCV(lgb_C_model, param_distributions=lgb_hybrid_param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
        "                    rand_search_C.fit(X_handcrafted_full_train.iloc[train_idx_np], y.iloc[train_idx_np])\n",
        "                    expert_C = rand_search_C.best_estimator_\n",
        "                    meta_features[val_idx_np, 2] = expert_C.predict(X_handcrafted_full_train.iloc[val_idx_np])\n",
        "\n",
        "                    # Expert D (RandomForest - New Expert)\n",
        "                    rf_D_model = RandomForestClassifier(random_state=42)\n",
        "                    rand_search_D = RandomizedSearchCV(rf_D_model, param_distributions=rf_param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
        "                    rand_search_D.fit(X_handcrafted_full_train.iloc[train_idx_np], y.iloc[train_idx_np])\n",
        "                    expert_D = rand_search_D.best_estimator_\n",
        "                    meta_features[val_idx_np, 3] = expert_D.predict(X_handcrafted_full_train.iloc[val_idx_np])\n",
        "\n",
        "\n",
        "                print(f\"    - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏°' (Meta-Model)...\")\n",
        "                if len(meta_features) > 1 and len(y.unique()) > 1:\n",
        "                    X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(meta_features, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "                    meta_xgb_param_dist = {\n",
        "                        'n_estimators': randint(300, 800), 'learning_rate': uniform(0.05, 0.2),\n",
        "                        'max_depth': randint(2, 5), 'subsample': uniform(0.7, 0.3),\n",
        "                        'colsample_bytree': uniform(0.7, 0.3)\n",
        "                    }\n",
        "                    meta_xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=10, use_label_encoder=False, eval_metric='mlogloss', seed=42)\n",
        "                    rand_search_meta = RandomizedSearchCV(meta_xgb_model, param_distributions=meta_xgb_param_dist, n_iter=20, cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
        "                    rand_search_meta.fit(X_meta_train, y_meta_train)\n",
        "                    meta_model = rand_search_meta.best_estimator_\n",
        "\n",
        "                    accuracy = accuracy_score(y_meta_test, meta_model.predict(X_meta_test))\n",
        "                    print(f\"      ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏°': {accuracy:.3f}\")\n",
        "                    self.meta_models[prize_type][target_pos] = meta_model\n",
        "\n",
        "                    self.expert_models[prize_type][f'expert_A_pos_{target_pos}'] = rand_search_A.best_estimator_.fit(vision_features, y, verbose=False)\n",
        "                    self.expert_models[prize_type][f'expert_B_pos_{target_pos}'] = rand_search_B.best_estimator_.fit(X_handcrafted_full_train, y, verbose=False)\n",
        "                    self.expert_models[prize_type][f'expert_C_pos_{target_pos}'] = rand_search_C.best_estimator_.fit(X_handcrafted_full_train, y)\n",
        "                    self.expert_models[prize_type][f'expert_D_pos_{target_pos}'] = rand_search_D.best_estimator_.fit(X_handcrafted_full_train, y)\n",
        "                else:\n",
        "                    print(f\"      ‚ùó ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏°' ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_pos + 1}. ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Meta-model.\")\n",
        "                    self.meta_models[prize_type][target_pos] = None\n",
        "                    if len(y.unique()) >= 2 and len(vision_features) > 0:\n",
        "                        xgb_expert_params_fallback = {'objective':'multi:softprob', 'num_class':10, 'use_label_encoder':False, 'eval_metric':'mlogloss', 'seed':42, 'n_estimators':300, 'learning_rate':0.05, 'max_depth':5}\n",
        "                        lgb_expert_params_fallback = {'objective': 'multiclass', 'num_class': 10, 'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'seed': 42, 'verbose': -1}\n",
        "                        rf_expert_params_fallback = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
        "\n",
        "                        self.expert_models[prize_type][f'expert_A_pos_{target_pos}'] = xgb.XGBClassifier(**xgb_expert_params_fallback).fit(vision_features, y, verbose=False)\n",
        "                        self.expert_models[prize_type][f'expert_B_pos_{target_pos}'] = xgb.XGBClassifier(**xgb_expert_params_fallback).fit(X_handcrafted_full_train, y, verbose=False)\n",
        "                        self.expert_models[prize_type][f'expert_C_pos_{target_pos}'] = lgb.LGBMClassifier(**lgb_expert_params_fallback).fit(X_handcrafted_full_train, y)\n",
        "                        self.expert_models[prize_type][f'expert_D_pos_{target_pos}'] = RandomForestClassifier(**rf_expert_params_fallback).fit(X_handcrafted_full_train, y)\n",
        "                    else:\n",
        "                        print(f\"        ‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å Expert Models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_pos + 1} ‡πÑ‡∏î‡πâ.\")\n",
        "                        self.expert_models[prize_type][f'expert_A_pos_{target_pos}'] = None\n",
        "                        self.expert_models[prize_type][f'expert_B_pos_{target_pos}'] = None\n",
        "                        self.expert_models[prize_type][f'expert_C_pos_{target_pos}'] = None\n",
        "                        self.expert_models[prize_type][f'expert_D_pos_{target_pos}'] = None\n",
        "\n",
        "    # --- 4. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (ML Model) ---\n",
        "    def predict_next_draw(self):\n",
        "        print(\"\\n\" + \"=\"*80); print(\"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÇ‡∏î‡∏¢ 'The Ultimate Synergy Engine'\"); print(\"=\"*80)\n",
        "\n",
        "        if len(self.data) < self.window_size:\n",
        "            print(f\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ {self.window_size} ‡∏á‡∏ß‡∏î ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(self.data)}).\")\n",
        "            return {} # Return empty dict for predictions\n",
        "\n",
        "        history_rows = self.data[-self.window_size:]\n",
        "        last_draw_date = history_rows[-1]['date']\n",
        "\n",
        "        time_diffs = []\n",
        "        for i in range(1, len(self.data)):\n",
        "            time_diffs.append((self.data[i]['date'] - self.data[i-1]['date']).days)\n",
        "\n",
        "        avg_time_diff = np.mean(time_diffs) if time_diffs else 0\n",
        "        estimated_next_draw_date = last_draw_date + timedelta(days=avg_time_diff)\n",
        "\n",
        "        # Display last `self.window_size` rows used for prediction\n",
        "        print(\"\\n--- üìú ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î) ---\")\n",
        "        if len(self.data) >= self.window_size:\n",
        "            history_df_display = pd.DataFrame(self.data[-self.window_size:])\n",
        "            print(history_df_display[['date', 'six_digit', 'two_digit']].to_string(index=False))\n",
        "        else:\n",
        "            print(\"  (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏° Window Size)\")\n",
        "\n",
        "\n",
        "        ml_predictions = {}\n",
        "        for prize_type in ['6d', '2d']:\n",
        "            num_digits = 6 if prize_type == '6d' else 2; col_name = 'six_digit' if prize_type == '6d' else 'two_digit'; prize_name = '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1' if prize_type == '6d' else '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á'\n",
        "            print(f\"\\n--- üìà ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{prize_name}' ---\")\n",
        "\n",
        "            try:\n",
        "                matrix = np.array([[int(d) for d in row[col_name]] for row in history_rows])\n",
        "            except ValueError as e:\n",
        "                print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {prize_type}: {e}. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 'six_digit' ‡∏´‡∏£‡∏∑‡∏≠ 'two_digit'.\")\n",
        "                ml_predictions[f'set1_{prize_type}'] = \"N/A\" * num_digits\n",
        "                ml_predictions[f'set2_{prize_type}'] = \"N/A\" * num_digits\n",
        "                ml_predictions[f'set3_{prize_type}'] = \"N/A\" * num_digits\n",
        "                ml_predictions[f'set4_{prize_type}'] = \"N/A\" * num_digits\n",
        "                ml_predictions[f'set5_{prize_type}'] = \"N/A\" * num_digits\n",
        "                ml_predictions[f'set6_{prize_type}'] = \"N/A\" * num_digits # For Flow Pattern Prediction\n",
        "                ml_predictions[f'set7_{prize_type}'] = \"N/A\" * num_digits # For Seasonal Pattern Prediction\n",
        "                ml_predictions[f'set8_{prize_type}'] = \"N/A\" * num_digits # For Cyclical Pattern Prediction\n",
        "                continue\n",
        "\n",
        "            image_for_pred = np.array(matrix).reshape(1, self.window_size, num_digits, 1) / 9.0\n",
        "\n",
        "            if self.feature_extractors[prize_type] is None:\n",
        "                print(f\"‚ùó CNN Feature Extractor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{prize_type}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å. ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Visionary/Hybrid.\")\n",
        "                vision_features = np.zeros((1, 64))\n",
        "            else:\n",
        "                vision_features = self.feature_extractors[prize_type].predict(image_for_pred, verbose=0)\n",
        "\n",
        "            handcrafted_features_pred_base = pd.DataFrame([self._create_handcrafted_features(matrix, num_digits)]).fillna(0)\n",
        "            date_features_for_pred = pd.DataFrame([self._create_date_features(estimated_next_draw_date, last_draw_date)]).fillna(0)\n",
        "            full_handcrafted_features_for_pred = pd.concat([handcrafted_features_pred_base, date_features_for_pred], axis=1)\n",
        "\n",
        "            manager_preds, expert_A_preds, expert_B_preds, expert_C_preds, expert_D_preds = [], [], [], [], []\n",
        "            for target_pos in range(num_digits):\n",
        "                pred_A, pred_B, pred_C, pred_D, manager_pred = '?', '?', '?', '?', '?'\n",
        "\n",
        "                # Expert A (XGBoost - Visionary)\n",
        "                expert_A_model = self.expert_models[prize_type].get(f'expert_A_pos_{target_pos}')\n",
        "                if isinstance(expert_A_model, str) and expert_A_model.startswith('CONSTANT:'):\n",
        "                    pred_A = expert_A_model.split(':')[1]\n",
        "                elif expert_A_model is not None:\n",
        "                    try:\n",
        "                        pred_A = expert_A_model.predict(vision_features)[0]\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è Expert A (Visionary) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "                        pred_A = 'E'\n",
        "                expert_A_preds.append(str(pred_A))\n",
        "\n",
        "                # Expert B (XGBoost - Historian)\n",
        "                expert_B_model = self.expert_models[prize_type].get(f'expert_B_pos_{target_pos}')\n",
        "                if isinstance(expert_B_model, str) and expert_B_model.startswith('CONSTANT:'):\n",
        "                    pred_B = expert_B_model.split(':')[1]\n",
        "                elif expert_B_model is not None:\n",
        "                    try:\n",
        "                        pred_B = expert_B_model.predict(full_handcrafted_features_for_pred)[0]\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è Expert B (Historian) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Feature Mismatch.\")\n",
        "                        pred_B = 'E'\n",
        "                expert_B_preds.append(str(pred_B))\n",
        "\n",
        "                # Expert C (LightGBM - Hybrid Master)\n",
        "                expert_C_model = self.expert_models[prize_type].get(f'expert_C_pos_{target_pos}')\n",
        "                if isinstance(expert_C_model, str) and expert_C_model.startswith('CONSTANT:'):\n",
        "                    pred_C = expert_C_model.split(':')[1]\n",
        "                elif expert_C_model is not None:\n",
        "                    try:\n",
        "                        pred_C = expert_C_model.predict(full_handcrafted_features_for_pred)[0]\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è Expert C (Hybrid Master) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Feature Mismatch.\")\n",
        "                        pred_C = 'E'\n",
        "                expert_C_preds.append(str(pred_C))\n",
        "\n",
        "                # Expert D (RandomForest - New Expert)\n",
        "                expert_D_model = self.expert_models[prize_type].get(f'expert_D_pos_{target_pos}')\n",
        "                if isinstance(expert_D_model, str) and expert_D_model.startswith('CONSTANT:'):\n",
        "                    pred_D = expert_D_model.split(':')[1]\n",
        "                elif expert_D_model is not None:\n",
        "                    try:\n",
        "                        pred_D = expert_D_model.predict(full_handcrafted_features_for_pred)[0]\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è Expert D (Forest Scout) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Feature Mismatch.\")\n",
        "                        pred_D = 'E'\n",
        "                expert_D_preds.append(str(pred_D))\n",
        "\n",
        "\n",
        "                # Manager (Meta-Model)\n",
        "                meta_model_for_pos = self.meta_models[prize_type][target_pos]\n",
        "                if isinstance(meta_model_for_pos, str) and meta_model_for_pos.startswith('CONSTANT:'):\n",
        "                    manager_pred = meta_model_for_pos.split(':')[1]\n",
        "                elif meta_model_for_pos is not None:\n",
        "                    meta_input_A = int(pred_A) if str(pred_A).isdigit() else 0\n",
        "                    meta_input_B = int(pred_B) if str(pred_B).isdigit() else 0\n",
        "                    meta_input_C = int(pred_C) if str(pred_C).isdigit() else 0\n",
        "                    meta_input_D = int(pred_D) if str(pred_D).isdigit() else 0\n",
        "\n",
        "                    meta_feature = np.array([[meta_input_A, meta_input_B, meta_input_C, meta_input_D]])\n",
        "                    try:\n",
        "                        manager_pred = self.meta_models[prize_type][target_pos].predict(meta_feature)[0]\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è ‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ (Meta-Model) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "                        manager_pred = 'E'\n",
        "                manager_preds.append(str(manager_pred))\n",
        "\n",
        "                print(f\"  - ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á {target_pos + 1}: Visionary(A):'{pred_A}', Historian(B):'{pred_B}', Hybrid(C):'{pred_C}', Forest(D):'{pred_D}' -> üëë Manager: '{manager_pred}'\")\n",
        "\n",
        "            ml_predictions[f'set1_{prize_type}'] = \"\".join(manager_preds)\n",
        "            ml_predictions[f'set2_{prize_type}'] = \"\".join(expert_A_preds)\n",
        "            ml_predictions[f'set3_{prize_type}'] = \"\".join(expert_B_preds)\n",
        "            ml_predictions[f'set4_{prize_type}'] = \"\".join(expert_C_preds)\n",
        "            ml_predictions[f'set5_{prize_type}'] = \"\".join(expert_D_preds)\n",
        "\n",
        "            # --- NEW: Add Flow Pattern Prediction as set6 ---\n",
        "            flow_pred_6d, flow_pred_2d = self.predict_flow_pattern_next_draw()\n",
        "            if prize_type == '6d':\n",
        "                ml_predictions[f'set6_6d'] = flow_pred_6d\n",
        "            elif prize_type == '2d':\n",
        "                ml_predictions[f'set6_2d'] = flow_pred_2d\n",
        "            # --- END NEW ---\n",
        "\n",
        "            # --- NEW: Add Seasonal Pattern Prediction as set7 ---\n",
        "            seasonal_pred_6d, seasonal_pred_2d = self._predict_seasonal_pattern(estimated_next_draw_date)\n",
        "            if prize_type == '6d':\n",
        "                ml_predictions[f'set7_6d'] = seasonal_pred_6d\n",
        "            elif prize_type == '2d':\n",
        "                ml_predictions[f'set7_2d'] = seasonal_pred_2d\n",
        "            # --- END NEW ---\n",
        "\n",
        "            # --- NEW: Add Cyclical Pattern Prediction as set8 ---\n",
        "            cyclical_pred_6d, cyclical_pred_2d = self._predict_cyclical_pattern(cycle_length=10) # Default cycle_length=10\n",
        "            if prize_type == '6d':\n",
        "                ml_predictions[f'set8_6d'] = cyclical_pred_6d\n",
        "            elif prize_type == '2d':\n",
        "                ml_predictions[f'set8_2d'] = cyclical_pred_2d\n",
        "            # --- END NEW ---\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80); print(\"üåü ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Portfolio of Experts & Manager) [ML Model] üåü\"); print(\"=\"*80)\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 1 (‡∏Ñ‡∏≥‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set1_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set1_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 2 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Visionary - CNN): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set2_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set2_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 3 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Historian - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set3_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set3_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 4 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Hybrid Master): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set4_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set4_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 5 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Forest Scout): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set5_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set5_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 6 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set6_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set6_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 7 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Seasonal Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set7_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set7_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 8 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Cyclical Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions.get('set8_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions.get('set8_2d', 'N/A')}\")\n",
        "        print(\"\\nüí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏ç‡∏≤‡∏ì\")\n",
        "\n",
        "        return ml_predictions # Return the full dictionary\n",
        "\n",
        "    # --- Helper: Calculate Digital Root (Moved from AnalyticalCore/GrandMathematicalEngine) ---\n",
        "    def _get_digital_root(self, n):\n",
        "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Digital Root ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\"\"\"\n",
        "        while n >= 10:\n",
        "            n = sum(int(digit) for digit in str(n))\n",
        "        return n\n",
        "\n",
        "    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (‡∏à‡∏≤‡∏Å React ‡πÄ‡∏î‡∏¥‡∏°) ---\n",
        "\n",
        "    # Helper: Check if a number is prime\n",
        "    def is_prime(self, num):\n",
        "        if num < 2: return False\n",
        "        for i in range(2, int(math.sqrt(num)) + 1):\n",
        "            if num % i == 0: return False\n",
        "        return True\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Fibonacci Sequence\n",
        "    def analyze_fibonacci(self, data):\n",
        "        fibonacci = {0, 1, 1, 2, 3, 5, 8} # Single-digit Fibonacci numbers\n",
        "        fib_matches = []\n",
        "\n",
        "        for item in data:\n",
        "            digits = [int(d) for d in item['number']]\n",
        "            fib_count = 0\n",
        "            for digit in digits:\n",
        "                if digit in fibonacci: fib_count += 1\n",
        "\n",
        "            if fib_count >= 3: # Consider a match if at least 3 digits are Fibonacci numbers (adjusted threshold)\n",
        "                fib_matches.append({\n",
        "                    'date': item['date'].strftime('%Y-%m-%d') if isinstance(item['date'], datetime) else item['date'],\n",
        "                    'number': item['number'],\n",
        "                    'fib_count': fib_count,\n",
        "                    'percentage': (fib_count / 6) * 100\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'matches': fib_matches,\n",
        "            'total_matches': len(fib_matches),\n",
        "            'average_fib_percentage': sum(item['percentage'] for item in fib_matches) / len(fib_matches) if fib_matches else 0\n",
        "        }\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Mathematical Progressions\n",
        "    def analyze_progressions(self, data):\n",
        "        progressions = []\n",
        "\n",
        "        for item in data:\n",
        "            digits = [int(d) for d in item['number']]\n",
        "            if len(digits) < 3: continue\n",
        "\n",
        "            # Arithmetic Progression check\n",
        "            is_arithmetic = True\n",
        "            diff = digits[1] - digits[0]\n",
        "            for i in range(2, len(digits)):\n",
        "                if digits[i] - digits[i-1] != diff:\n",
        "                    is_arithmetic = False\n",
        "                    break\n",
        "\n",
        "            # Geometric Progression check (modified for single digits and avoiding division by zero)\n",
        "            is_geometric = True\n",
        "            if digits[0] != 0: # First digit cannot be zero for a simple geometric ratio\n",
        "                # Avoid division by zero and handle float precision\n",
        "                if digits[0] == 0: # If first digit is 0, geometric progression with non-zero ratio is not simple\n",
        "                    is_geometric = False\n",
        "                else:\n",
        "                    ratio = digits[1] / digits[0] if digits[0] != 0 else float('inf')\n",
        "                    for i in range(2, len(digits)):\n",
        "                        if digits[i-1] == 0 or abs(digits[i] / digits[i-1] - ratio) > 0.001:\n",
        "                            is_geometric = False\n",
        "                            break\n",
        "            else: # If first digit is 0, it can only be 0,0,0... for simple geometric\n",
        "                if not all(d == 0 for d in digits):\n",
        "                    is_geometric = False\n",
        "\n",
        "            if is_arithmetic:\n",
        "                progressions.append({\n",
        "                    'date': item['date'].strftime('%Y-%m-%d') if isinstance(item['date'], datetime) else item['date'],\n",
        "                    'number': item['number'],\n",
        "                    'type': 'Arithmetic',\n",
        "                    'pattern': ' ‚Üí '.join(map(str, digits))\n",
        "                })\n",
        "            elif is_geometric:\n",
        "                progressions.append({\n",
        "                    'date': item['date'].strftime('%Y-%m-%d') if isinstance(item['date'], datetime) else item['date'],\n",
        "                    'number': item['number'],\n",
        "                    'type': 'Geometric',\n",
        "                    'pattern': ' ‚Üí '.join(map(str, digits))\n",
        "                })\n",
        "        return progressions\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Digital Root\n",
        "    def analyze_digital_roots(self, data):\n",
        "        digital_roots = {}\n",
        "\n",
        "        for item in data:\n",
        "            s = sum(int(digit) for digit in item['number'])\n",
        "            root = self._get_digital_root(s) # Use the class's _get_digital_root method\n",
        "\n",
        "            if root not in digital_roots:\n",
        "                digital_roots[root] = {'count': 0, 'numbers': []}\n",
        "            digital_roots[root]['count'] += 1\n",
        "            digital_roots[root]['numbers'].append(item['number'])\n",
        "\n",
        "        chart_data = sorted([\n",
        "            {'root': r, 'count': d['count'], 'percentage': (d['count'] / len(data)) * 100}\n",
        "            for r, d in digital_roots.items()\n",
        "        ], key=lambda x: x['root'])\n",
        "\n",
        "        return {'distribution': digital_roots, 'chart_data': chart_data}\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Prime Number Relationships\n",
        "    def analyze_prime_relationships(self, data):\n",
        "        primes = {2, 3, 5, 7} # Single-digit prime numbers\n",
        "        prime_analysis = []\n",
        "\n",
        "        for item in data:\n",
        "            digits = [int(d) for d in item['number']]\n",
        "            prime_count = sum(1 for digit in digits if digit in primes)\n",
        "            s = sum(digits)\n",
        "            is_prime_sum = self.is_prime(s)\n",
        "\n",
        "            if prime_count >= 2 or is_prime_sum:\n",
        "                prime_analysis.append({\n",
        "                    'date': item['date'].strftime('%Y-%m-%d') if isinstance(item['date'], datetime) else item['date'],\n",
        "                    'number': item['number'],\n",
        "                    'prime_digits': prime_count,\n",
        "                    'sum': s,\n",
        "                    'is_prime_sum': is_prime_sum\n",
        "                })\n",
        "        return prime_analysis\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Modular Arithmetic\n",
        "    def analyze_modular_patterns(self, data):\n",
        "        mod_results = {}\n",
        "        mods = [3, 7, 9] # Common modulo values\n",
        "\n",
        "        for mod in mods:\n",
        "            mod_results[mod] = {}\n",
        "            for item in data:\n",
        "                s = sum(int(digit) for digit in item['number'])\n",
        "                remainder = s % mod\n",
        "\n",
        "                if remainder not in mod_results[mod]:\n",
        "                    mod_results[mod][remainder] = []\n",
        "                mod_results[mod][remainder].append(item['number'])\n",
        "        return mod_results\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Sequential Dependencies\n",
        "    def analyze_sequential_dependencies(self, data):\n",
        "        dependencies = []\n",
        "        # Ensure data is sorted from most recent to oldest as it was in React's `csvData` for `slice(0, 20)`\n",
        "        # `self.data` is sorted ascending, so `data[i]` is older than `data[i+1]` if we iterate normally.\n",
        "        # To match React's \"most recent first\" for analysis, we'll reverse or use `data[i]` and `data[i-1]`\n",
        "        # Let's use the most recent 20 for analysis, which means `self.data[-20:]`\n",
        "        recent_data = data[-20:] # Take last 20 draws (most recent)\n",
        "\n",
        "        for i in range(len(recent_data) - 1, 0, -1): # Iterate from second-to-last to first (most recent to oldest)\n",
        "            current_item = recent_data[i] # This is the older draw\n",
        "            next_item = recent_data[i-1] # This is the newer draw (the \"next\" in sequence from current_item)\n",
        "\n",
        "            current_digits = [int(d) for d in current_item['number']]\n",
        "            next_digits = [int(d) for d in next_item['number']]\n",
        "\n",
        "            correlation_score = 0\n",
        "            for pos in range(min(len(current_digits), len(next_digits))):\n",
        "                diff = abs(current_digits[pos] - next_digits[pos])\n",
        "                if diff <= 2: correlation_score += 1\n",
        "\n",
        "            if correlation_score >= 3:\n",
        "                dependencies.append({\n",
        "                    'from': current_item['date'].strftime('%Y-%m-%d') if isinstance(current_item['date'], datetime) else current_item['date'],\n",
        "                    'to': next_item['date'].strftime('%Y-%m-%d') if isinstance(next_item['date'], datetime) else next_item['date'],\n",
        "                    'from_number': current_item['number'],\n",
        "                    'to_number': next_item['number'],\n",
        "                    'correlation': correlation_score\n",
        "                })\n",
        "        return dependencies\n",
        "\n",
        "    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Sum Patterns\n",
        "    def analyze_sum_patterns(self, data):\n",
        "        sum_data = []\n",
        "        for item in data:\n",
        "            s = sum(int(digit) for digit in item['number'])\n",
        "            sum_digital_root = sum(int(digit) for digit in str(s)) # Digital root of the sum\n",
        "            sum_data.append({\n",
        "                'date': item['date'].strftime('%Y-%m-%d') if isinstance(item['date'], datetime) else item['date'],\n",
        "                'number': item['number'],\n",
        "                'sum': s,\n",
        "                'sum_digital_root': sum_digital_root\n",
        "            })\n",
        "\n",
        "        sum_freq = {}\n",
        "        for item in sum_data:\n",
        "            r = (item['sum'] // 5) * 5 # Group sums into ranges of 5\n",
        "            if r not in sum_freq: sum_freq[r] = 0\n",
        "            sum_freq[r] += 1\n",
        "\n",
        "        chart_data = sorted([\n",
        "            {'range': f\"{r}-{r+4}\", 'count': c}\n",
        "            for r, c in sum_freq.items()\n",
        "        ], key=lambda x: int(x['range'].split('-')[0]))\n",
        "\n",
        "        return {'data': sum_data, 'frequency': sum_freq, 'chart_data': chart_data}\n",
        "\n",
        "    # --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå (‡∏à‡∏≤‡∏Å React ‡πÄ‡∏î‡∏¥‡∏°) ---\n",
        "    def predict_with_math(self):\n",
        "        if not self.data or len(self.data) == 0:\n",
        "            return None\n",
        "\n",
        "        # Analyze data using the implemented mathematical functions\n",
        "        # Use only recent data for mathematical prediction to simulate the React logic's 100 recent draws\n",
        "        recent_csv_data = self.data[-100:]\n",
        "\n",
        "        # Ensure 'number' field exists for mathematical analysis\n",
        "        # Create a temporary 'number' field by combining 'six_digit' and 'two_digit' for analysis\n",
        "        # This is a workaround as the original React code assumed a 'number' field directly.\n",
        "        temp_data_for_math_analysis = []\n",
        "        for row in recent_csv_data:\n",
        "            temp_row = row.copy()\n",
        "            temp_row['number'] = row['six_digit'] # For 6-digit analysis\n",
        "            temp_data_for_math_analysis.append(temp_row)\n",
        "\n",
        "\n",
        "        fibonacci_analysis = self.analyze_fibonacci(temp_data_for_math_analysis)\n",
        "        digital_root_analysis = self.analyze_digital_roots(temp_data_for_math_analysis)\n",
        "        modular_analysis = self.analyze_modular_patterns(temp_data_for_math_analysis)\n",
        "        sum_analysis = self.analyze_sum_patterns(temp_data_for_math_analysis)\n",
        "\n",
        "        self.math_analysis_results = {\n",
        "            'fibonacci': fibonacci_analysis,\n",
        "            'digitalRoot': digital_root_analysis,\n",
        "            'modular': modular_analysis,\n",
        "            'sum': sum_analysis,\n",
        "            'progression': self.analyze_progressions(temp_data_for_math_analysis), # Add others for completeness of analysis\n",
        "            'prime': self.analyze_prime_relationships(temp_data_for_math_analysis),\n",
        "            'sequence': self.analyze_sequential_dependencies(temp_data_for_math_analysis)\n",
        "        }\n",
        "\n",
        "        # --- Logic for 6-digit prediction ---\n",
        "        prediction_six_digit = ''\n",
        "        last_number_digits = [int(d) for d in self.data[-1]['six_digit']] # Get digits of the very last draw (R1)\n",
        "\n",
        "        # Digital Root\n",
        "        digital_root_distribution = digital_root_analysis['distribution']\n",
        "        if digital_root_distribution:\n",
        "            most_frequent_root_entry = sorted(digital_root_distribution.items(), key=lambda item: item[1]['count'], reverse=True)[0]\n",
        "            most_frequent_root = int(most_frequent_root_entry[0])\n",
        "        else:\n",
        "            most_frequent_root = 0\n",
        "\n",
        "        # Modular Pattern (Mod 9)\n",
        "        mod9_pattern = modular_analysis.get(9, {})\n",
        "        if mod9_pattern:\n",
        "            most_frequent_mod9_entry = sorted(mod9_pattern.items(), key=lambda item: len(item[1]), reverse=True)[0]\n",
        "            most_frequent_mod9_remainder = int(most_frequent_mod9_entry[0])\n",
        "        else:\n",
        "            most_frequent_mod9_remainder = 0\n",
        "\n",
        "        # Combine rules for 6-digit prediction (similar to React logic)\n",
        "        for pos in range(6):\n",
        "            predicted_digit = 0 # Default\n",
        "            if pos % 2 == 0: # Even positions (0, 2, 4)\n",
        "                # Use Fibonacci-like or Digital Root logic\n",
        "                # Simple sum with digital root, then modulo 10\n",
        "                predicted_digit = (last_number_digits[pos] + most_frequent_root) % 10\n",
        "            else: # Odd positions (1, 3, 5)\n",
        "                # Use Modular or sequential logic\n",
        "                predicted_digit = (last_number_digits[pos] + most_frequent_mod9_remainder) % 10\n",
        "\n",
        "            prediction_six_digit += str(predicted_digit)\n",
        "\n",
        "        # --- Logic for 2-digit prediction ---\n",
        "        # Use sum pattern from last 10 draws for 2-digit prediction\n",
        "        recent_sums_data = sum_analysis['data'][-10:] # Get last 10 sum data entries\n",
        "        recent_sums = [item['sum'] for item in recent_sums_data if 'sum' in item]\n",
        "\n",
        "        if recent_sums:\n",
        "            avg_sum_of_recent_draws = sum(recent_sums) / len(recent_sums)\n",
        "            # Use digital root of average sum for last 2 digits, then modulo for final two digits\n",
        "            # Ensure the result is a 2-digit string\n",
        "            predicted_last2 = str(self._get_digital_root(int(avg_sum_of_recent_draws)) * 13 % 100).zfill(2) # Added multiplier for more variation\n",
        "        else:\n",
        "            predicted_last2 = '00' # Default if no sums available\n",
        "\n",
        "        self.math_prediction = {\n",
        "            'sixDigit': prediction_six_digit,\n",
        "            'twoDigit': predicted_last2,\n",
        "            'confidence': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥)',\n",
        "            'method': 'Digital Root + Modular + Positional Analysis',\n",
        "            'digitalRoot': str(most_frequent_root),\n",
        "            'modularBase': str(most_frequent_mod9_remainder)\n",
        "        }\n",
        "        return self.math_prediction\n",
        "\n",
        "    # --- NEW: Flow Pattern Analysis and Prediction ---\n",
        "    def analyze_flow_pattern(self, data_to_analyze):\n",
        "        \"\"\"\n",
        "        Analyzes the 'Flow Pattern' based on the definition:\n",
        "        For each 'target row' (the (window_size + 1)th row in a chronological window), it checks\n",
        "        how its digits are derived/formed from the preceding 'window_size' 'historical rows'.\n",
        "        It records the 'pattern of occurrence, flow, and arrangement' by detailing\n",
        "        where each digit of the target row originated from in the historical rows.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Loop through the data starting from self.window_size index\n",
        "        for i in range(self.window_size, len(data_to_analyze)):\n",
        "            current_target_row = data_to_analyze[i] # This is the \"target row\" (the outcome to be explained)\n",
        "            preceding_history_rows = data_to_analyze[i-self.window_size:i] # These are the historical rows\n",
        "\n",
        "            # Combine the 2-digit lower and 6-digit first prize numbers into a single 8-digit string\n",
        "            # Example: R1=\"522630\", L2=\"37\" -> combined \"37522630\"\n",
        "            # Digits are then processed left-to-right from this combined string for the target row.\n",
        "            target_digits_combined_lr = current_target_row['two_digit'] + current_target_row['six_digit']\n",
        "\n",
        "            # Store analysis for each digit of the target row\n",
        "            digit_flow_analysis = []\n",
        "\n",
        "            # Iterate through each digit of the current_target_row (8 digits, left to right)\n",
        "            for target_digit_idx, target_digit_value in enumerate(target_digits_combined_lr):\n",
        "                sources_in_past_rows = [] # To store details of where this target digit was found\n",
        "                found_count_for_target_digit = 0 # Count of how many times this specific target digit appeared\n",
        "\n",
        "                # Check this target_digit_value against all digits in each of the preceding history rows\n",
        "                for past_row_offset, past_row in enumerate(preceding_history_rows):\n",
        "                    # Combine digits of the historical row for searching\n",
        "                    past_row_combined_digits_lr = past_row['two_digit'] + past_row['six_digit']\n",
        "\n",
        "                    # Iterate through each digit in the current historical row to find matches\n",
        "                    for past_digit_idx, past_digit_value in enumerate(past_row_combined_digits_lr):\n",
        "                        if past_digit_value == target_digit_value:\n",
        "                            found_count_for_target_digit += 1\n",
        "                            sources_in_past_rows.append({\n",
        "                                # Modified: Use self.window_size for relative index\n",
        "                                'source_row_relative_index': self.window_size - past_row_offset,\n",
        "                                'source_digit_value': past_digit_value,\n",
        "                                'source_position_desc': self.combined_digit_positions.get(past_digit_idx, f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {past_digit_idx+1} (‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)'),\n",
        "                                'source_date': past_row['date'].strftime('%Y/%m/%d')\n",
        "                            })\n",
        "\n",
        "                digit_flow_analysis.append({\n",
        "                    'target_digit_value': target_digit_value,\n",
        "                    'target_position_desc': self.combined_digit_positions.get(target_digit_idx, f'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà {target_digit_idx+1} (‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)'),\n",
        "                    'total_found_in_past_rows': found_count_for_target_digit,\n",
        "                    'sources': sources_in_past_rows # Detailed list of all occurrences\n",
        "                })\n",
        "\n",
        "            # Calculate summary metrics for the current target row\n",
        "            all_target_digits_found_at_least_once = all(item['total_found_in_past_rows'] > 0 for item in digit_flow_analysis)\n",
        "            total_occurrences_of_target_digits_from_past = sum(item['total_found_in_past_rows'] for item in digit_flow_analysis)\n",
        "            match_percentage_coverage = (len([item\n",
        "                                               for item in digit_flow_analysis\n",
        "                                               if item['total_found_in_past_rows'] > 0]) / 8) * 100\n",
        "            match_percentage_total_occurrence = (total_occurrences_of_target_digits_from_past / 8) * 100\n",
        "\n",
        "            results.append({\n",
        "                'index': i, # Index of the current_target_row in the original (sorted) data\n",
        "                'date': current_target_row['date'].strftime('%Y/%m/%d'),\n",
        "                'six_digit': current_target_row['six_digit'],\n",
        "                'two_digit': current_target_row['two_digit'],\n",
        "                'preceding_history_rows': [{'date': r['date'].strftime('%Y/%m/%d'),\n",
        "                                            'six_digit': r['six_digit'],\n",
        "                                            'two_digit': r['two_digit']} for r in preceding_history_rows],\n",
        "                'digit_flow_analysis': digit_flow_analysis, # Detailed flow analysis\n",
        "                'all_target_digits_found_at_least_once': all_target_digits_found_at_least_once,\n",
        "                'total_occurrences_of_target_digits_from_past': total_occurrences_of_target_digits_from_past,\n",
        "                'match_percentage_coverage': match_percentage_coverage,\n",
        "                'match_percentage_total_occurrence': match_percentage_total_occurrence\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def display_flow_pattern_summary(self):\n",
        "        \"\"\"Displays the overall summary of the Flow Pattern test.\"\"\"\n",
        "        print(\"\\nüìä ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Flow Pattern\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if not self.flow_pattern_analysis_results:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á.\")\n",
        "            return\n",
        "\n",
        "        total_rows = len(self.flow_pattern_analysis_results)\n",
        "        if total_rows == 0:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern\")\n",
        "            return\n",
        "\n",
        "        perfect_matches_coverage = len([r for r in self.flow_pattern_analysis_results if r['all_target_digits_found_at_least_once']])\n",
        "        partial_matches_coverage = len([r for r in self.flow_pattern_analysis_results if not r['all_target_digits_found_at_least_once'] and r['match_percentage_coverage'] > 0])\n",
        "        no_matches_coverage = len([r for r in self.flow_pattern_analysis_results if r['match_percentage_coverage'] == 0])\n",
        "\n",
        "        percentages_coverage = [r['match_percentage_coverage'] for r in self.flow_pattern_analysis_results]\n",
        "        avg_percentage_coverage = sum(percentages_coverage) / total_rows\n",
        "\n",
        "        percentages_total_occurrence = [r['match_percentage_total_occurrence'] for r in self.flow_pattern_analysis_results]\n",
        "        avg_percentage_total_occurrence = sum(percentages_total_occurrence) / total_rows\n",
        "\n",
        "        summary = {\n",
        "            'total_rows': total_rows,\n",
        "            'perfect_matches_coverage': perfect_matches_coverage,\n",
        "            'partial_matches_coverage': partial_matches_coverage,\n",
        "            'no_matches_coverage': no_matches_coverage,\n",
        "            'perfect_match_rate_coverage': (perfect_matches_coverage / total_rows) * 100,\n",
        "            'avg_percentage_coverage': round(avg_percentage_coverage, 1),\n",
        "            'min_percentage_coverage': min(percentages_coverage),\n",
        "            'max_percentage_coverage': max(percentages_coverage),\n",
        "            'avg_percentage_total_occurrence': round(avg_percentage_total_occurrence, 1),\n",
        "            'min_percentage_total_occurrence': min(percentages_total_occurrence),\n",
        "            'max_percentage_total_occurrence': max(percentages_total_occurrence)\n",
        "        }\n",
        "\n",
        "        print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß (‡∏á‡∏ß‡∏î) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {summary['total_rows']} ‡πÅ‡∏ñ‡∏ß\")\n",
        "        print(f\"\\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å (Percentage of Unique Digits Covered) (‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï) ---\")\n",
        "        print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (8 ‡∏´‡∏•‡∏±‡∏Å) ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÉ‡∏ô {self.window_size} ‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {summary['perfect_matches_coverage']} ‡∏á‡∏ß‡∏î ({summary['perfect_match_rate_coverage']:.1f}%)\")\n",
        "        print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô: {summary['partial_matches_coverage']} ‡∏á‡∏ß‡∏î\")\n",
        "        print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÄ‡∏•‡∏¢: {summary['no_matches_coverage']} ‡∏á‡∏ß‡∏î\")\n",
        "        print(f\"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: {summary['avg_percentage_coverage']}%\")\n",
        "        print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {summary['min_percentage_coverage']}%\")\n",
        "        print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {summary['max_percentage_coverage']}%\")\n",
        "\n",
        "        print(\"\\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è (Total Occurrence Percentage) ---\")\n",
        "        print(\"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡∏¥‡∏ô 100% ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
        "        print(f\"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è: {summary['avg_percentage_total_occurrence']}%\")\n",
        "        print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {summary['min_percentage_total_occurrence']}%\")\n",
        "        print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {summary['max_percentage_total_occurrence']}%\")\n",
        "\n",
        "        print(\"\\nüéØ ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏≠‡∏á Pattern ‡∏ô‡∏µ‡πâ:\")\n",
        "        if summary['perfect_match_rate_coverage'] > 50 and summary['avg_percentage_total_occurrence'] > 100:\n",
        "            print(f\"‚úÖ Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å! ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏™‡∏π‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á\")\n",
        "        elif summary['perfect_match_rate_coverage'] > 20 or summary['avg_percentage_total_occurrence'] > 50:\n",
        "            print(f\"‚ö†Ô∏è Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å {self.window_size} ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
        "        elif summary['perfect_matches_coverage'] > 0 or summary['avg_percentage_total_occurrence'] > 0:\n",
        "            print(\"‚ùå Pattern ‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\")\n",
        "        else:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Pattern ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ\")\n",
        "\n",
        "    def _extract_flow_rules(self):\n",
        "        \"\"\"\n",
        "        Extracts the most frequent digit-to-digit, position-to-position flow rules\n",
        "        from the historical flow pattern analysis results.\n",
        "        Returns a dictionary of rules: {target_pos: {target_digit: {source_pos: {source_digit: count}}}}\n",
        "        \"\"\"\n",
        "        if not self.flow_pattern_analysis_results:\n",
        "            return {}\n",
        "\n",
        "        flow_rules = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
        "\n",
        "        for result in self.flow_pattern_analysis_results:\n",
        "            for digit_analysis in result['digit_flow_analysis']:\n",
        "                target_digit_val = int(digit_analysis['target_digit_value'])\n",
        "                target_pos_desc = digit_analysis['target_position_desc'] # e.g., '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)'\n",
        "\n",
        "                # Map position description back to index (0-7)\n",
        "                target_pos_idx = -1\n",
        "                for idx, desc in self.combined_digit_positions.items():\n",
        "                    if desc == target_pos_desc:\n",
        "                        target_pos_idx = idx\n",
        "                        break\n",
        "\n",
        "                if target_pos_idx == -1: # Skip if position not found (shouldn't happen)\n",
        "                    continue\n",
        "\n",
        "                for source_detail in digit_analysis['sources']:\n",
        "                    source_digit_val = int(source_detail['source_digit_value'])\n",
        "                    source_pos_desc = source_detail['source_position_desc']\n",
        "\n",
        "                    source_pos_idx = -1\n",
        "                    for idx, desc in self.combined_digit_positions.items():\n",
        "                        if desc == source_pos_desc:\n",
        "                            source_pos_idx = idx\n",
        "                            break\n",
        "\n",
        "                    if source_pos_idx == -1: # Skip if source position not found\n",
        "                        continue\n",
        "\n",
        "                    # Store count of (target_pos, target_digit) -> (source_pos, source_digit)\n",
        "                    flow_rules[target_pos_idx][target_digit_val][source_pos_idx][source_digit_val] += 1\n",
        "\n",
        "        # Simplify rules to store only the most frequent source for each (target_pos, target_digit)\n",
        "        simplified_rules = {}\n",
        "        for target_pos_idx, target_digit_data in flow_rules.items():\n",
        "            simplified_rules[target_pos_idx] = {}\n",
        "            for target_digit_val, source_data in target_digit_data.items():\n",
        "                best_source_pos = None\n",
        "                best_source_digit = None\n",
        "                max_count = -1\n",
        "\n",
        "                for source_pos_idx, source_digit_data in source_data.items():\n",
        "                    for source_digit_val, count in source_digit_data.items():\n",
        "                        if count > max_count:\n",
        "                            max_count = count\n",
        "                            best_source_pos = source_pos_idx\n",
        "                            best_source_digit = source_digit_val\n",
        "\n",
        "                if best_source_pos is not None:\n",
        "                    simplified_rules[target_pos_idx][target_digit_val] = {\n",
        "                        'source_pos': best_source_pos,\n",
        "                        'source_digit': best_source_digit,\n",
        "                        'count': max_count\n",
        "                    }\n",
        "        return simplified_rules\n",
        "\n",
        "    def predict_flow_pattern_next_draw(self):\n",
        "        \"\"\"\n",
        "        Generates a prediction for the next draw based on the most frequent flow patterns.\n",
        "        This is Expert Set 6.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- üåä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern (Expert Set 6) ---\")\n",
        "        if not self.flow_pattern_analysis_results:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern ‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern.\")\n",
        "            return \"N/A\"*6, \"N/A\"*2\n",
        "\n",
        "        # Extract flow rules from historical data\n",
        "        flow_rules = self._extract_flow_rules()\n",
        "\n",
        "        if not flow_rules:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern ‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏•‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô.\")\n",
        "            return \"N/A\"*6, \"N/A\"*2\n",
        "\n",
        "        # Get the very last historical draw to apply the rules\n",
        "        if len(self.data) < 1:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern.\")\n",
        "            return \"N/A\"*6, \"N/A\"*2\n",
        "\n",
        "        last_historical_row = self.data[-1]\n",
        "        last_combined_digits_str = last_historical_row['two_digit'] + last_historical_row['six_digit']\n",
        "        last_combined_digits_list = [int(d) for d in last_combined_digits_str]\n",
        "\n",
        "        predicted_digits = [\"?\"] * 8 # Initialize with unknown\n",
        "\n",
        "        # Apply flow rules to predict each digit of the next draw\n",
        "        for target_pos_idx in range(8):\n",
        "            # Find the most likely target digit for this position based on flow rules\n",
        "            # We need to iterate through all possible target digits (0-9)\n",
        "            # and see which one has the strongest flow from the *current* last historical draw.\n",
        "\n",
        "            best_predicted_digit = -1\n",
        "            max_flow_strength = -1\n",
        "\n",
        "            for current_target_digit_candidate in range(10): # Iterate through 0-9 as potential next digit\n",
        "                # Check if there's a rule for this (target_pos, target_digit_candidate)\n",
        "                if target_pos_idx in flow_rules and current_target_digit_candidate in flow_rules[target_pos_idx]:\n",
        "                    rule = flow_rules[target_pos_idx][current_target_digit_candidate]\n",
        "                    source_pos_from_rule = rule['source_pos']\n",
        "                    source_digit_from_rule = rule['source_digit']\n",
        "                    flow_count = rule['count']\n",
        "\n",
        "                    # Check if the source digit in the *last historical draw* matches the rule's source digit\n",
        "                    if source_pos_from_rule < len(last_combined_digits_list) and \\\n",
        "                       last_combined_digits_list[source_pos_from_rule] == source_digit_from_rule:\n",
        "\n",
        "                        # If it matches, this rule is active. The strength is its count.\n",
        "                        if flow_count > max_flow_strength:\n",
        "                            max_flow_strength = flow_count\n",
        "                            best_predicted_digit = current_target_digit_candidate\n",
        "\n",
        "            if best_predicted_digit != -1:\n",
        "                predicted_digits[target_pos_idx] = str(best_predicted_digit)\n",
        "            else:\n",
        "                # Fallback if no strong flow rule is found for this position\n",
        "                # Use the digit from the same position in the last historical draw, or a random digit.\n",
        "                if target_pos_idx < len(last_combined_digits_list):\n",
        "                    predicted_digits[target_pos_idx] = str(last_combined_digits_list[target_pos_idx])\n",
        "                else:\n",
        "                    predicted_digits[target_pos_idx] = str(np.random.randint(0, 10)) # Random digit if no history/rule\n",
        "\n",
        "        flow_pred_two_digit = \"\".join(predicted_digits[0:2])\n",
        "        flow_pred_six_digit = \"\".join(predicted_digits[2:8])\n",
        "\n",
        "        print(f\"  - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (Flow Pattern): {flow_pred_six_digit}\")\n",
        "        print(f\"  - ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (Flow Pattern): {flow_pred_two_digit}\")\n",
        "        print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\")\n",
        "\n",
        "        return flow_pred_six_digit, flow_pred_two_digit\n",
        "\n",
        "    def _predict_seasonal_pattern(self, target_date, seasonal_window_days=15):\n",
        "        \"\"\"\n",
        "        Predicts based on digits frequently appearing around the target date (e.g., July 1st)\n",
        "        across all historical years.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- üå∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Seasonal Pattern (Expert Set 7, Window +/- {seasonal_window_days} ‡∏ß‡∏±‡∏ô) ---\")\n",
        "        seasonal_preds = {'6d': [], '2d': []}\n",
        "\n",
        "        # Collect all historical draws within the seasonal window\n",
        "        seasonal_data_points = []\n",
        "        for row in self.data:\n",
        "            draw_date = row['date']\n",
        "\n",
        "            # Create dummy dates in a common year to compare only month and day\n",
        "            dummy_target_date = datetime(2000, target_date.month, target_date.day)\n",
        "            dummy_draw_date = datetime(2000, draw_date.month, draw_date.day)\n",
        "\n",
        "            # Calculate difference in days, considering year-end wrap-around\n",
        "            day_of_year_target = dummy_target_date.timetuple().tm_yday\n",
        "            day_of_year_draw = dummy_draw_date.timetuple().tm_yday\n",
        "\n",
        "            delta = abs(day_of_year_target - day_of_year_draw)\n",
        "            # Consider wrap-around for dates near year end/start\n",
        "            delta = min(delta, 365 - delta) # For non-leap years (approximation, fine for this purpose)\n",
        "\n",
        "            if delta <= seasonal_window_days:\n",
        "                seasonal_data_points.append(row)\n",
        "\n",
        "        if not seasonal_data_points:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î. Seasonal Prediction ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 'N/A'.\")\n",
        "            return \"N/A\"*6, \"N/A\"*2\n",
        "\n",
        "        # Calculate digit frequencies for each position within the seasonal data\n",
        "        for prize_type, num_digits, col_name in [('6d', 6, 'six_digit'), ('2d', 2, 'two_digit')]:\n",
        "            position_digit_counts = [Counter() for _ in range(num_digits)]\n",
        "\n",
        "            for row in seasonal_data_points:\n",
        "                digits_str = row[col_name]\n",
        "                if len(digits_str) == num_digits: # Ensure string length matches expected digits\n",
        "                    for pos_idx, digit_char in enumerate(digits_str):\n",
        "                        position_digit_counts[pos_idx][int(digit_char)] += 1\n",
        "\n",
        "            predicted_digits = []\n",
        "            for pos_idx in range(num_digits):\n",
        "                if position_digit_counts[pos_idx]:\n",
        "                    # Get the most common digit for this position\n",
        "                    most_common_digit = position_digit_counts[pos_idx].most_common(1)[0][0]\n",
        "                    predicted_digits.append(str(most_common_digit))\n",
        "                else:\n",
        "                    predicted_digits.append('?') # Fallback if no data for this position\n",
        "\n",
        "            seasonal_preds[prize_type] = \"\".join(predicted_digits)\n",
        "\n",
        "        print(f\"  - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (Seasonal Pattern): {seasonal_preds['6d']}\")\n",
        "        print(f\"  - ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (Seasonal Pattern): {seasonal_preds['2d']}\")\n",
        "        print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Seasonal Pattern ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\")\n",
        "        return seasonal_preds['6d'], seasonal_preds['2d']\n",
        "\n",
        "    def _predict_cyclical_pattern(self, cycle_length=10):\n",
        "        \"\"\"\n",
        "        Predicts based on a simple cyclical pattern:\n",
        "        Takes the digit from the same position `cycle_length` draws ago.\n",
        "        If not enough data, falls back to a random digit.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Cyclical Pattern (Expert Set 8, Cycle Length={cycle_length}) ---\")\n",
        "        cyclical_preds = {'6d': [], '2d': []}\n",
        "\n",
        "        if len(self.data) < cycle_length:\n",
        "            print(f\"‚ùó ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cyclical Pattern (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ {cycle_length} ‡∏á‡∏ß‡∏î). Cyclical Prediction ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 'N/A'.\")\n",
        "            return \"N/A\"*6, \"N/A\"*2\n",
        "\n",
        "        # Get the draw from `cycle_length` draws ago\n",
        "        source_draw = self.data[-cycle_length]\n",
        "\n",
        "        # Predict 6-digit number\n",
        "        source_6d_digits = [int(d) for d in source_draw['six_digit']]\n",
        "        for pos in range(6):\n",
        "            cyclical_preds['6d'].append(str(source_6d_digits[pos]))\n",
        "\n",
        "        # Predict 2-digit number\n",
        "        source_2d_digits = [int(d) for d in source_draw['two_digit']]\n",
        "        for pos in range(2):\n",
        "            cyclical_preds['2d'].append(str(source_2d_digits[pos]))\n",
        "\n",
        "        final_6d = \"\".join(cyclical_preds['6d'])\n",
        "        final_2d = \"\".join(cyclical_preds['2d'])\n",
        "\n",
        "        print(f\"  - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (Cyclical Pattern): {final_6d}\")\n",
        "        print(f\"  - ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (Cyclical Pattern): {final_2d}\")\n",
        "        print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Cyclical Pattern ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\")\n",
        "        return final_6d, final_2d\n",
        "\n",
        "\n",
        "    # --- NEW: Synergy Prediction Logic ---\n",
        "    def _get_combined_digit_prediction(self, all_predictions_for_position):\n",
        "        \"\"\"\n",
        "        Combines predictions for a single digit position using mode,\n",
        "        with tie-breaking by picking the higher digit.\n",
        "        Then applies a 'higher value' adjustment.\n",
        "        \"\"\"\n",
        "        if not all_predictions_for_position:\n",
        "            return '?' # No predictions for this position\n",
        "\n",
        "        # Filter out non-digit predictions (like 'E' for error or '?')\n",
        "        valid_digits = [int(d) for d in all_predictions_for_position if str(d).isdigit()]\n",
        "        if not valid_digits:\n",
        "            return '?' # No valid digits to combine\n",
        "\n",
        "        counts = Counter(valid_digits)\n",
        "        if not counts:\n",
        "            return '?'\n",
        "\n",
        "        max_count = 0\n",
        "        most_frequent_digits = []\n",
        "        for digit, count in counts.items():\n",
        "            if count > max_count:\n",
        "                max_count = count\n",
        "                most_frequent_digits = [digit]\n",
        "            elif count == max_count:\n",
        "                most_frequent_digits.append(digit)\n",
        "\n",
        "        # Tie-breaking: pick the highest digit among the most frequent ones\n",
        "        combined_digit = max(most_frequent_digits)\n",
        "\n",
        "        # Apply \"adjust values higher\" logic: (digit + 1) % 10\n",
        "        # This shifts the digit up by 1, wrapping 9 to 0.\n",
        "        adjusted_digit = (combined_digit + 1) % 10\n",
        "\n",
        "        return str(adjusted_digit)\n",
        "\n",
        "    def generate_synergy_prediction(self, ml_predictions_dict):\n",
        "        \"\"\"\n",
        "        Generates a final 'synergy' prediction by combining ML and Mathematical predictions.\n",
        "        Applies a 'higher value' adjustment.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- ü§ù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á 'Synergy Prediction' (ML + ‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå) ---\")\n",
        "\n",
        "        synergy_six_digit = []\n",
        "        synergy_two_digit = []\n",
        "\n",
        "        # Combine predictions for 6-digit number\n",
        "        for pos in range(6):\n",
        "            all_preds_at_pos = []\n",
        "            # Add ML predictions (set1 to set8)\n",
        "            for i in range(1, 9): # Changed range to include set7 and set8\n",
        "                pred_str = ml_predictions_dict.get(f'set{i}_6d', '')\n",
        "                if len(pred_str) > pos and pred_str[pos].isdigit():\n",
        "                    all_preds_at_pos.append(pred_str[pos])\n",
        "\n",
        "            # Add Mathematical prediction from self.math_prediction\n",
        "            if self.math_prediction and self.math_prediction['sixDigit'] and \\\n",
        "               len(self.math_prediction['sixDigit']) > pos and \\\n",
        "               self.math_prediction['sixDigit'][pos].isdigit():\n",
        "                all_preds_at_pos.append(self.math_prediction['sixDigit'][pos])\n",
        "\n",
        "            synergy_six_digit.append(self._get_combined_digit_prediction(all_preds_at_pos))\n",
        "\n",
        "        # Combine predictions for 2-digit number\n",
        "        for pos in range(2):\n",
        "            all_preds_at_pos = []\n",
        "            # Add ML predictions (set1 to set8)\n",
        "            for i in range(1, 9): # Changed range to include set7 and set8\n",
        "                pred_str = ml_predictions_dict.get(f'set{i}_2d', '')\n",
        "                if len(pred_str) > pos and pred_str[pos].isdigit():\n",
        "                    all_preds_at_pos.append(pred_str[pos])\n",
        "\n",
        "            # Add Mathematical prediction from self.math_prediction\n",
        "            if self.math_prediction and self.math_prediction['twoDigit'] and \\\n",
        "               len(self.math_prediction['twoDigit']) > pos and \\\n",
        "               self.math_prediction['twoDigit'][pos].isdigit():\n",
        "                all_preds_at_pos.append(self.math_prediction['twoDigit'][pos])\n",
        "\n",
        "            synergy_two_digit.append(self._get_combined_digit_prediction(all_preds_at_pos))\n",
        "\n",
        "        final_six = \"\".join(synergy_six_digit)\n",
        "        final_two = \"\".join(synergy_two_digit)\n",
        "\n",
        "        self.synergy_prediction_results = {'6d': final_six, '2d': final_two}\n",
        "\n",
        "        print(f\"  - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (Synergy): {final_six}\")\n",
        "        print(f\"  - ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (Synergy): {final_two}\")\n",
        "        print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Synergy Prediction ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.\")\n",
        "\n",
        "        return final_six, final_two\n",
        "\n",
        "    # --- ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏à‡∏≤‡∏Å React UI Logic) ---\n",
        "    def display_all_results(self, ml_predictions_dict): # Changed parameter to dict\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üåü ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Ultimate Synergy Engine üåü\")\n",
        "        print(\"================================================================================\\n\")\n",
        "\n",
        "        print(\"--- üîÆ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ML (The Ultimate Synergy Engine) ---\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 1 (‡∏Ñ‡∏≥‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set1_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set1_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 2 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Visionary - CNN): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set2_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set2_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 3 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Historian - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set3_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set3_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 4 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Hybrid Master): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set4_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set4_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 5 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏≤‡∏Å The Forest Scout): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set5_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set5_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 6 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Flow Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set6_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set6_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 7 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Seasonal Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set7_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set7_2d', 'N/A')}\")\n",
        "        print(f\"  - ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 8 (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Cyclical Pattern): ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {ml_predictions_dict.get('set8_6d', 'N/A')} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {ml_predictions_dict.get('set8_2d', 'N/A')}\")\n",
        "        print(\"\\nüí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏ç‡∏≤‡∏ì\")\n",
        "\n",
        "        # Display Synergy Prediction\n",
        "        print(\"\\n--- ‚ú® ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö Synergy (‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• + ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô) ---\")\n",
        "        print(f\"  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {self.synergy_prediction_results['6d']}\")\n",
        "        print(f\"  ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {self.synergy_prediction_results['2d']}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- üßÆ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ---\")\n",
        "        if not self.math_analysis_results:\n",
        "            print(\"  ‚ùó ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å predict_with_math() ‡∏Å‡πà‡∏≠‡∏ô.\")\n",
        "        else:\n",
        "            # 1. Fibonacci Analysis\n",
        "            fib_res = self.math_analysis_results['fibonacci']\n",
        "            print(\"\\n  üåü Fibonacci Sequence Analysis:\")\n",
        "            print(f\"    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç Fibonacci ‡πÄ‡∏î‡πà‡∏ô‡∏ä‡∏±‡∏î (>=3 ‡∏ï‡∏±‡∏ß): {fib_res['total_matches']}\")\n",
        "            print(f\"    - ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç Fibonacci ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏á‡∏ß‡∏î: {fib_res['average_fib_percentage']:.1f}%\")\n",
        "            if fib_res['matches']:\n",
        "                print(\"    - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö:\")\n",
        "                for match in fib_res['matches'][:3]: # Show top 3 examples\n",
        "                    print(f\"      - {match['date']}: {match['number']} ({match['fib_count']}/6 digits)\")\n",
        "\n",
        "            # 2. Digital Root Distribution\n",
        "            dr_res = self.math_analysis_results['digitalRoot']\n",
        "            print(\"\\n  üî¢ Digital Root Distribution:\")\n",
        "            if dr_res['chart_data']:\n",
        "                # Find most frequent root by count, not by root value\n",
        "                most_frequent_root_entry = sorted(dr_res['chart_data'], key=lambda x: x['count'], reverse=True)[0]\n",
        "                print(f\"    - Digital Root ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {most_frequent_root_entry['root']} ({most_frequent_root_entry['count']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)\")\n",
        "                print(\"    - ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á Digital Root:\")\n",
        "                for item in dr_res['chart_data']:\n",
        "                    print(f\"      Root {item['root']}: {'‚ñà' * int(item['percentage'] / 5)} {item['count']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({item['percentage']:.1f}%)\") # Simple ASCII bar chart\n",
        "\n",
        "            # 3. Mathematical Progressions\n",
        "            prog_res = self.math_analysis_results['progression']\n",
        "            print(\"\\n  üìà Mathematical Progressions:\")\n",
        "            if prog_res:\n",
        "                print(f\"    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏Å‡∏£‡∏°: {len(prog_res)}\")\n",
        "                print(\"    - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö:\")\n",
        "                for prog in prog_res[:3]: # Show top 3 examples\n",
        "                    print(f\"      - {prog['date']} ({prog['type']}): {prog['pattern']}\")\n",
        "            else:\n",
        "                print(\"    - ‡πÑ‡∏°‡πà‡∏û‡∏ö Arithmetic ‡∏´‡∏£‡∏∑‡∏≠ Geometric Progression ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\")\n",
        "\n",
        "            # 4. Prime Number Relationships\n",
        "            prime_res = self.math_analysis_results['prime']\n",
        "            print(\"\\n  ‚≠ê Prime Number Relationships:\")\n",
        "            if prime_res:\n",
        "                print(f\"    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏î‡πà‡∏ô‡∏ä‡∏±‡∏î (>=2 ‡∏ï‡∏±‡∏ß) ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏•‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {len(prime_res)}\")\n",
        "                print(\"    - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö:\")\n",
        "                for item in prime_res[:3]:\n",
        "                    print(f\"      - {item['date']}: {item['number']} (‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞ {item['prime_digits']} ‡∏ï‡∏±‡∏ß, ‡∏ú‡∏•‡∏£‡∏ß‡∏° {item['sum']} ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {item['is_prime_sum']})\")\n",
        "            else:\n",
        "                print(\"    - ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏î‡πà‡∏ô‡∏ä‡∏±‡∏î\")\n",
        "\n",
        "            # 5. Modular Arithmetic\n",
        "            mod_res = self.math_analysis_results['modular']\n",
        "            print(\"\\n  ‚ûó Modular Arithmetic Patterns (Top 2 remainders per modulo):\")\n",
        "            for mod, remainders in mod_res.items():\n",
        "                print(f\"    - Modulo {mod}:\")\n",
        "                sorted_remainders = sorted(remainders.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "                for rem, numbers in sorted_remainders[:2]: # Show top 2 remainders\n",
        "                    print(f\"      Remainder {rem}: {len(numbers)} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {', '.join(numbers[:3])})\")\n",
        "\n",
        "            # 6. Sequential Dependencies\n",
        "            seq_res = self.math_analysis_results['sequence']\n",
        "            print(\"\\n  ‚õìÔ∏è Sequential Dependencies (Top 3 correlations):\")\n",
        "            if seq_res:\n",
        "                print(f\"    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (>={len(seq_res)}): {len(seq_res)}\")\n",
        "                for dep in seq_res[:3]:\n",
        "                    print(f\"      - {dep['from_number']} ({dep['from']}) -> {dep['to_number']} ({dep['to']}) | Correlation Score: {dep['correlation']}\")\n",
        "            else:\n",
        "                print(\"    - ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏î‡πà‡∏ô‡∏ä‡∏±‡∏î\")\n",
        "\n",
        "            # 7. Sum Patterns\n",
        "            sum_res = self.math_analysis_results['sum']\n",
        "            print(\"\\n  ‚ûï Sum Pattern Analysis:\")\n",
        "            if sum_res['chart_data']:\n",
        "                most_freq_sum_range = sorted(sum_res['chart_data'], key=lambda x: x['count'], reverse=True)[0]\n",
        "                print(f\"    - ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á: {most_freq_sum_range['range']} (‡∏û‡∏ö {most_freq_sum_range['count']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)\")\n",
        "                print(\"    - ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏£‡∏ß‡∏°:\")\n",
        "                for item in sum_res['chart_data']:\n",
        "                    print(f\"      ‡∏ä‡πà‡∏ß‡∏á {item['range']}: {'‚ñà' * int(item['count'] / (max(c['count'] for c in sum_res['chart_data']) / 20 + 1))} {item['count']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\") # Scaled ASCII bar chart\n",
        "\n",
        "\n",
        "        print(\"\\n--- üîÆ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (Mathematical Analyzer) ---\")\n",
        "        if self.math_prediction:\n",
        "            print(f\"  ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å): {self.math_prediction['sixDigit']}\")\n",
        "            print(f\"  ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {self.math_prediction['twoDigit']}\")\n",
        "            print(f\"  ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£: {self.math_prediction['method']}\")\n",
        "            print(f\"  Digital Root Base: {self.math_prediction['digitalRoot']}\")\n",
        "            print(f\"  Modular Base: {self.math_prediction['modularBase']}\")\n",
        "            print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô: {self.math_prediction['confidence']}\")\n",
        "        else:\n",
        "            print(\"  ‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÑ‡∏î‡πâ\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏ç‡∏≤‡∏ì\")\n",
        "        print(\"üòä ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    engine = UltimateSynergyEngine(window_size=6)\n",
        "    if not engine.load_data_from_file(): return\n",
        "\n",
        "    # Run Flow Pattern analysis first\n",
        "    print(\"\\n\" + \"=\"*80); print(\"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1.5: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern\"); print(\"=\"*80)\n",
        "    engine.flow_pattern_analysis_results = engine.analyze_flow_pattern(engine.data)\n",
        "    engine.display_flow_pattern_summary()\n",
        "\n",
        "    # Run ML model training and get all its predictions\n",
        "    engine.build_and_train_engine() # This trains the models and populates self.expert_models, self.meta_models\n",
        "    ml_all_predictions = engine.predict_next_draw() # Now returns the full dict\n",
        "\n",
        "    # Run mathematical analysis and prediction\n",
        "    engine.predict_with_math() # This will populate self.math_analysis_results and self.math_prediction\n",
        "\n",
        "    # Generate and store Synergy Prediction\n",
        "    engine.generate_synergy_prediction(ml_all_predictions)\n",
        "\n",
        "    # Display all results, including the new Synergy Prediction\n",
        "    engine.display_all_results(ml_all_predictions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920,
          "referenced_widgets": [
            "6a48423f8e2e4b728844b91461598f52",
            "e289ec2f481a4ac1b0408b3933f86461",
            "2da732da55fd4b1cab631d6c30c74b23",
            "7ad9c7a46a384c239bb63a2d2e39651b",
            "34b2390db6b84b47807a75f4cb523a83",
            "4da717f7527f450ab8d669cb46c3adfd",
            "751858a32600460ab6e9f294622b5190",
            "02c32d2c0b1f4f29b2cde7e6881e5bf2",
            "bbdb11e358624b7c9dbfc9c68a105170",
            "c83f55963efe42279304f45bcacbe176",
            "dc4ba4e705ad4f6fb870be0ea8afce70"
          ]
        },
        "id": "tsnsLICz4Q7A",
        "outputId": "2d0526d8-2447-40dd-b8c5-112c00433459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Ultimate Synergy Engine ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Window Size = 6)\n",
            "   - ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
            "\n",
            "================================================================================\n",
            "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
            "================================================================================\n",
            "üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90a038e4-40bb-403b-ad9b-f318f9652c0d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90a038e4-40bb-403b-ad9b-f318f9652c0d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv to ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.csv\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 560 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "================================================================================\n",
            "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1.5: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Flow Pattern\n",
            "================================================================================\n",
            "\n",
            "üìä ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Flow Pattern\n",
            "============================================================\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß (‡∏á‡∏ß‡∏î) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö: 554 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å (Percentage of Unique Digits Covered) (‡∏à‡∏≤‡∏Å 6 ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï) ---\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (8 ‡∏´‡∏•‡∏±‡∏Å) ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÉ‡∏ô 6 ‡∏á‡∏ß‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: 534 ‡∏á‡∏ß‡∏î (96.4%)\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô: 20 ‡∏á‡∏ß‡∏î\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏ö‡πÄ‡∏•‡∏¢: 0 ‡∏á‡∏ß‡∏î\n",
            "‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: 99.4%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: 75.0%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: 100.0%\n",
            "\n",
            "--- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è (Total Occurrence Percentage) ---\n",
            "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡∏¥‡∏ô 100% ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
            "‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è: 478.0%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: 275.0%\n",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: 762.5%\n",
            "\n",
            "üéØ ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏≠‡∏á Pattern ‡∏ô‡∏µ‡πâ:\n",
            "‚úÖ Pattern ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å! ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å 6 ‡∏á‡∏ß‡∏î‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ '‡πÑ‡∏´‡∏•' ‡πÑ‡∏õ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏™‡∏π‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á\n",
            "\n",
            "================================================================================\n",
            "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å 'Ultimate Synergy Engine'\n",
            "================================================================================\n",
            "\n",
            "--- üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1' ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing 6d:   0%|          | 0/554 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a48423f8e2e4b728844b91461598f52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å CNN Feature Extractor...\n",
            "\n",
            "--- üë• ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1' ---\n",
            "  - ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1:\n",
            "    - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏°' (Meta-Model)...\n",
            "      ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏°': 0.586\n",
            "  - ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 2:\n"
          ]
        }
      ]
    }
  ]
}