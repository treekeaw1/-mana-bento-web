{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvMSeW5uMu+mWQDNtVFL/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treekeaw1/-mana-bento-web/blob/main/Untitled47.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w1zhB9folaqq",
        "outputId": "eab33e5c-8e2e-474c-ec36-8e70533c5727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
            "Generating window-based frequency and positional features...\n",
            "  - Processing window size: 8\n",
            "  - Processing window size: 10\n",
            "  - Processing window size: 12\n",
            "Finished generating window-based features.\n",
            "‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: 524 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Hyperparameter Tuning ‡πÅ‡∏•‡∏∞ TimeSeriesSplit ---\n",
            "\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: L2_Tens\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô RandomForest...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å RandomForest ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.2299, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô GradientBoosting...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å GradientBoosting ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.2253, Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô MLPClassifier...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å MLPClassifier ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.1977, Best Params: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 20)}\n",
            "\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: L2_Units\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô RandomForest...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å RandomForest ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.2184, Best Params: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50}\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô GradientBoosting...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å GradientBoosting ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.1862, Best Params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n",
            "  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô MLPClassifier...\n",
            "    ‚úÖ ‡∏ù‡∏∂‡∏Å MLPClassifier ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: 0.1701, Best Params: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 20)}\n",
            "‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\n",
            "\n",
            "--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting ---\n",
            "‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Backtesting ‡∏ö‡∏ô 30 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î.\n",
            "\n",
            "--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Backtesting ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° ---\n",
            "üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Backtesting: 0.1000 (3/30)\n",
            "\n",
            "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏• Backtesting:\n",
            "         Date ‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß_Actual Predicted_L2  L2_Match\n",
            "0  2024-04-01                 90           74     False\n",
            "1  2024-04-16                 79           97     False\n",
            "2  2024-05-02                 17           71     False\n",
            "3  2024-05-16                 60           46     False\n",
            "4  2024-06-01                 42           60     False\n",
            "5  2024-06-16                 31           13     False\n",
            "6  2024-07-01                 89           98     False\n",
            "7  2024-07-16                 21           14     False\n",
            "8  2024-08-01                 46           24     False\n",
            "9  2024-08-16                 28           80     False\n",
            "10 2024-09-01                 94           68     False\n",
            "11 2024-09-16                 37           57     False\n",
            "12 2024-10-01                 59           59      True\n",
            "13 2024-10-16                 00           00      True\n",
            "14 2024-11-01                 32           14     False\n",
            "15 2024-11-16                 38           38      True\n",
            "16 2024-12-01                 61           39     False\n",
            "17 2024-12-16                 21           32     False\n",
            "18 2025-01-02                 51           35     False\n",
            "19 2025-01-17                 23           35     False\n",
            "20 2025-02-01                 51           77     False\n",
            "21 2025-02-16                 50           21     False\n",
            "22 2025-03-01                 54           35     False\n",
            "23 2025-03-16                 32           50     False\n",
            "24 2025-04-01                 36           32     False\n",
            "25 2025-04-16                 85           84     False\n",
            "26 2025-05-02                 06           25     False\n",
            "27 2025-05-16                 87           65     False\n",
            "28 2025-06-01                 20           04     False\n",
            "29 2025-06-16                 06           48     False\n",
            "‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
            "\n",
            "--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ---\n",
            "‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 2025-07-01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'datetime.datetime' object has no attribute 'dayofweek'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1072359034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0mnext_draw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_draw_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0mnext_draw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_draw_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m \u001b[0mnext_draw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DayOfWeek'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_draw_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0mnext_draw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DayOfMonth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_draw_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0mnext_draw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DayOfYear'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_draw_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofyear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'datetime.datetime' object has no attribute 'dayofweek'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 0. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏ß‡∏î 2025/07/01 ‡πÄ‡∏õ‡πá‡∏ô Actual ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ) ---\n",
        "csv_content = \"\"\"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà,‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å),‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "2025/06/16,507392,06\n",
        "2025/06/01,559352,20\n",
        "2025/05/16,251309,87\n",
        "2025/05/02,213388,06\n",
        "2025/04/16,266227,85\n",
        "2025/04/01,669687,36\n",
        "2025/03/16,757563,32\n",
        "2025/03/01,818894,54\n",
        "2025/02/16,847377,50\n",
        "2025/02/01,558700,51\n",
        "2025/01/17,807779,23\n",
        "2025/01/02,730209,51\n",
        "2024/12/16,097863,21\n",
        "2024/12/01,669843,61\n",
        "2024/11/16,187221,38\n",
        "2024/11/01,536044,32\n",
        "2024/10/16,482962,00\n",
        "2024/10/01,718665,59\n",
        "2024/09/16,608662,37\n",
        "2024/09/01,199606,94\n",
        "2024/08/16,095867,28\n",
        "2024/08/01,407041,46\n",
        "2024/07/16,367336,21\n",
        "2024/07/01,434503,89\n",
        "2024/06/16,518504,31\n",
        "2024/06/01,530593,42\n",
        "2024/05/16,205690,60\n",
        "2024/05/02,980116,17\n",
        "2024/04/16,943598,79\n",
        "2024/04/01,803481,90\n",
        "2024/03/16,997626,78\n",
        "2024/03/01,253603,79\n",
        "2024/02/16,941395,43\n",
        "2024/02/01,607063,09\n",
        "2024/01/17,105979,61\n",
        "2023/12/30,625544,89\n",
        "2023/12/16,356757,85\n",
        "2023/12/01,251097,91\n",
        "2023/11/16,557990,14\n",
        "2023/11/01,743951,63\n",
        "2023/10/16,931446,44\n",
        "2023/10/01,727202,66\n",
        "2023/09/16,320812,46\n",
        "2023/09/01,915478,91\n",
        "2023/08/16,471782,67\n",
        "2023/07/31,260453,11\n",
        "2023/07/16,169530,62\n",
        "2023/07/01,922605,16\n",
        "2023/06/16,264872,30\n",
        "2023/06/01,125272,09\n",
        "2023/05/16,132903,99\n",
        "2023/05/02,843019,65\n",
        "2023/04/16,984906,71\n",
        "2023/04/01,087907,99\n",
        "2023/03/16,025873,73\n",
        "2023/03/01,417652,55\n",
        "2023/02/16,590417,80\n",
        "2023/02/01,297411,92\n",
        "2023/01/17,812519,47\n",
        "2022/12/30,157196,58\n",
        "2022/12/16,845093,14\n",
        "2022/12/01,375805,08\n",
        "2022/11/16,121789,64\n",
        "2022/11/01,913106,70\n",
        "2022/10/16,613106,15\n",
        "2022/10/01,484669,50\n",
        "2022/09/16,943703,75\n",
        "2022/09/01,929332,83\n",
        "2022/08/16,331583,42\n",
        "2022/08/01,436594,14\n",
        "2022/07/16,620405,53\n",
        "2022/07/01,981417,61\n",
        "2022/06/16,361807,92\n",
        "2022/06/01,319196,02\n",
        "2022/05/16,155012,06\n",
        "2022/05/02,658642,09\n",
        "2022/04/16,395919,58\n",
        "2022/04/01,970618,10\n",
        "2022/03/16,737867,03\n",
        "2022/03/01,061905,07\n",
        "2022/02/17,098597,57\n",
        "2022/02/01,944308,30\n",
        "2022/01/17,880159,92\n",
        "2021/12/30,819068,36\n",
        "2021/12/16,639235,83\n",
        "2021/12/01,077258,82\n",
        "2021/11/16,032761,57\n",
        "2021/11/01,045037,95\n",
        "2021/10/16,386372,38\n",
        "2021/10/01,578171,83\n",
        "2021/09/16,070935,90\n",
        "2021/09/01,114475,79\n",
        "2021/08/16,046750,23\n",
        "2021/08/01,910261,69\n",
        "2021/07/16,556725,70\n",
        "2021/07/01,713517,29\n",
        "2021/06/16,691861,17\n",
        "2021/06/01,292972,45\n",
        "2021/05/16,684579,14\n",
        "2021/05/02,501272,18\n",
        "2021/04/16,100787,56\n",
        "2021/04/01,472270,05\n",
        "2021/03/16,890422,19\n",
        "2021/03/01,835538,73\n",
        "2021/02/16,424603,39\n",
        "2021/02/01,912307,97\n",
        "2021/01/17,384395,15\n",
        "2020/12/30,803628,19\n",
        "2020/12/16,201303,70\n",
        "2020/12/01,100994,84\n",
        "2020/11/16,972661,46\n",
        "2020/11/01,506404,40\n",
        "2020/10/16,286051,38\n",
        "2020/10/01,837893,59\n",
        "2020/09/16,244083,57\n",
        "2020/09/01,999997,98\n",
        "2020/08/16,945811,88\n",
        "2020/08/01,569391,92\n",
        "2020/07/16,873286,53\n",
        "2020/07/01,347258,83\n",
        "2020/06/16,516967,64\n",
        "2020/06/01,831567,24\n",
        "2020/04/01,051095,22\n",
        "2020/03/16,503446,77\n",
        "2020/03/01,875938,98\n",
        "2020/02/16,781403,94\n",
        "2020/02/01,589227,06\n",
        "2020/01/17,491774,68\n",
        "2019/12/30,510541,81\n",
        "2019/12/16,529924,97\n",
        "2019/12/01,453522,81\n",
        "2019/11/16,017223,32\n",
        "2019/11/01,967375,79\n",
        "2019/10/16,812564,15\n",
        "2019/10/01,691197,59\n",
        "2019/09/16,340388,85\n",
        "2019/09/01,798787,20\n",
        "2019/08/16,775476,89\n",
        "2019/08/01,387006,58\n",
        "2019/07/15,369765,88\n",
        "2019/07/01,943647,86\n",
        "2019/06/16,174055,29\n",
        "2019/06/01,516461,46\n",
        "2019/05/16,962526,71\n",
        "2019/05/02,061324,25\n",
        "2019/04/16,570331,23\n",
        "2019/04/01,109767,52\n",
        "2019/03/16,724628,64\n",
        "2019/03/01,345650,65\n",
        "2019/02/16,074824,56\n",
        "2019/02/01,967134,04\n",
        "2019/01/17,197079,65\n",
        "2018/12/30,735867,02\n",
        "2018/12/16,356564,62\n",
        "2018/12/01,021840,67\n",
        "2018/11/16,989903,16\n",
        "2018/11/01,149840,58\n",
        "2018/10/16,200515,93\n",
        "2018/10/01,452643,99\n",
        "2018/09/16,149760,79\n",
        "2018/09/01,734510,26\n",
        "2018/08/16,586117,10\n",
        "2018/08/01,386602,78\n",
        "2018/07/16,596324,27\n",
        "2018/07/01,963623,83\n",
        "2018/06/16,223131,46\n",
        "2018/06/01,988117,95\n",
        "2018/05/16,075629,20\n",
        "2018/05/02,248038,85\n",
        "2018/04/16,739229,60\n",
        "2018/04/01,412073,85\n",
        "2018/03/16,218559,82\n",
        "2018/03/02,759415,29\n",
        "2018/02/16,309915,39\n",
        "2018/02/01,026853,31\n",
        "2018/01/17,203823,50\n",
        "2017/12/30,911234,98\n",
        "2017/12/16,955596,89\n",
        "2017/12/01,451005,33\n",
        "2017/11/16,292391,98\n",
        "2017/11/01,533726,85\n",
        "2017/10/16,413494,86\n",
        "2017/10/01,880714,52\n",
        "2017/09/16,170143,71\n",
        "2017/09/01,143224,65\n",
        "2017/08/16,715431,37\n",
        "2017/08/01,756519,36\n",
        "2017/07/16,820327,87\n",
        "2017/07/01,112360,26\n",
        "2017/06/16,943142,47\n",
        "2017/06/01,053630,61\n",
        "2017/05/16,454891,53\n",
        "2017/05/02,008656,35\n",
        "2017/04/16,816729,40\n",
        "2017/04/01,392785,80\n",
        "2017/03/16,273863,92\n",
        "2017/03/01,978453,78\n",
        "2017/02/16,229116,14\n",
        "2017/02/01,054672,42\n",
        "2017/01/17,145157,25\n",
        "2016/12/30,377712,46\n",
        "2016/12/16,435286,35\n",
        "2016/12/01,086069,77\n",
        "2016/11/16,858383,44\n",
        "2016/11/01,785438,86\n",
        "2016/10/16,571947,98\n",
        "2016/10/01,887102,33\n",
        "2016/09/16,240650,42\n",
        "2016/09/01,638684,62\n",
        "2016/08/16,254004,33\n",
        "2016/08/01,272932,57\n",
        "2016/07/16,449764,55\n",
        "2016/07/01,082460,53\n",
        "2016/06/16,073816,79\n",
        "2016/06/01,511825,14\n",
        "2016/05/16,141737,98\n",
        "2016/05/02,399459,02\n",
        "2016/04/16,221609,87\n",
        "2016/04/01,066720,92\n",
        "2016/03/16,134918,32\n",
        "2016/03/01,439686,06\n",
        "2016/02/16,356364,98\n",
        "2016/02/01,927800,09\n",
        "2016/01/17,304371,50\n",
        "2015/12/30,008217,02\n",
        "2015/12/17,930255,08\n",
        "2015/12/01,915350,78\n",
        "2015/11/16,795283,03\n",
        "2015/11/01,361211,45\n",
        "2015/10/16,968630,62\n",
        "2015/10/01,594825,07\n",
        "2015/09/16,743148,06\n",
        "2015/09/01,021094,89\n",
        "2015/08/16,033363,40\n",
        "2015/08/01,518677,53\n",
        "2015/07/16,121507,49\n",
        "2015/07/01,759049,26\n",
        "2015/06/16,644742,05\n",
        "2015/06/02,388881,65\n",
        "2015/05/16,011421,38\n",
        "2015/05/02,543466,30\n",
        "2015/04/16,506260,38\n",
        "2015/04/01,605704,70\n",
        "2015/03/16,048151,92\n",
        "2015/03/01,240237,34\n",
        "2015/02/16,001864,90\n",
        "2015/02/01,155537,79\n",
        "2015/01/16,244351,74\n",
        "2014/12/30,461704,57\n",
        "2014/12/16,948354,90\n",
        "2014/12/01,480449,11\n",
        "2014/11/16,479804,25\n",
        "2014/11/01,206608,44\n",
        "2014/10/16,656409,94\n",
        "2014/10/01,375615,44\n",
        "2014/09/16,772269,35\n",
        "2014/09/01,856763,22\n",
        "2014/08/16,662842,91\n",
        "2014/08/01,766391,82\n",
        "2014/07/16,468728,45\n",
        "2014/07/01,378477,39\n",
        "2014/06/16,673920,95\n",
        "2014/06/01,781198,18\n",
        "2014/05/16,087523,20\n",
        "2014/05/02,103297,52\n",
        "2014/04/16,153406,26\n",
        "2014/04/01,028866,95\n",
        "2014/03/16,531404,79\n",
        "2014/03/01,906318,35\n",
        "2014/02/16,384245,01\n",
        "2014/02/01,180149,95\n",
        "2014/01/16,306902,52\n",
        "2013/12/30,561072,48\n",
        "2013/12/16,341767,79\n",
        "2013/12/01,168795,27\n",
        "2013/11/16,806925,28\n",
        "2013/11/01,739804,47\n",
        "2013/10/16,963289,60\n",
        "2013/10/01,647882,14\n",
        "2013/09/16,562684,63\n",
        "2013/09/01,548123,05\n",
        "2013/08/16,321327,20\n",
        "2013/08/01,356435,82\n",
        "2013/07/16,566996,86\n",
        "2013/07/01,646905,51\n",
        "2013/06/16,289673,69\n",
        "2013/06/01,935489,90\n",
        "2013/05/16,687125,56\n",
        "2013/05/02,603458,07\n",
        "2013/04/16,843846,86\n",
        "2013/04/01,571688,53\n",
        "2013/03/16,968433,52\n",
        "2013/03/01,976241,37\n",
        "2013/02/16,368257,09\n",
        "2013/02/01,565566,66\n",
        "2013/01/16,820981,08\n",
        "2012/12/30,302358,00\n",
        "2012/12/16,529524,72\n",
        "2012/12/01,110443,43\n",
        "2012/11/16,639500,15\n",
        "2012/11/01,524694,63\n",
        "2012/10/16,281343,28\n",
        "2012/10/01,124025,58\n",
        "2012/09/16,540143,79\n",
        "2012/09/01,329997,07\n",
        "2012/08/16,683877,28\n",
        "2012/08/01,895590,50\n",
        "2012/07/16,904050,11\n",
        "2012/07/01,915900,60\n",
        "2012/06/16,159373,51\n",
        "2012/06/01,882727,38\n",
        "2012/05/16,814418,31\n",
        "2012/05/02,889501,29\n",
        "2012/04/16,583470,62\n",
        "2012/04/01,257562,69\n",
        "2012/03/16,607064,08\n",
        "2012/03/01,222518,79\n",
        "2012/02/16,648684,18\n",
        "2012/02/01,320605,32\n",
        "2012/01/16,451445,81\n",
        "2011/12/30,526402,65\n",
        "2011/12/16,884178,21\n",
        "2011/12/01,408147,02\n",
        "2011/11/16,997777,57\n",
        "2011/11/01,805540,54\n",
        "2011/10/16,955756,83\n",
        "2011/10/01,511052,15\n",
        "2011/09/16,731198,28\n",
        "2011/09/01,724533,85\n",
        "2011/08/16,536960,62\n",
        "2011/08/01,218756,12\n",
        "2011/07/16,116556,12\n",
        "2011/07/01,622953,51\n",
        "2011/06/16,351276,88\n",
        "2011/06/01,562370,46\n",
        "2011/05/16,406417,05\n",
        "2011/05/02,054136,85\n",
        "2011/04/16,825988,44\n",
        "2011/04/01,814931,01\n",
        "2011/03/16,593331,96\n",
        "2011/03/01,656037,97\n",
        "2011/02/16,481746,27\n",
        "2011/02/01,610089,55\n",
        "2011/01/16,281062,23\n",
        "2010/12/30,884112,49\n",
        "2010/12/16,334380,24\n",
        "2010/12/01,181752,09\n",
        "2010/11/16,813993,43\n",
        "2010/11/01,191100,59\n",
        "2010/10/16,621377,42\n",
        "2010/10/01,488372,02\n",
        "2010/09/16,017422,66\n",
        "2010/09/01,354656,11\n",
        "2010/08/16,911097,64\n",
        "2010/08/01,210008,10\n",
        "2010/07/16,180387,34\n",
        "2010/07/01,480239,68\n",
        "2010/06/16,500104,73\n",
        "2010/06/01,444874,81\n",
        "2010/05/16,480012,12\n",
        "2010/05/02,360371,06\n",
        "2010/04/16,211743,96\n",
        "2010/04/01,959517,22\n",
        "2010/03/16,364222,97\n",
        "2010/03/01,215227,97\n",
        "2010/02/16,133707,03\n",
        "2010/02/01,186312,14\n",
        "2010/01/16,073577,67\n",
        "2009/12/30,994304,87\n",
        "2009/12/16,685141,05\n",
        "2009/12/01,776980,59\n",
        "2009/11/16,055986,58\n",
        "2009/11/01,689140,85\n",
        "2009/10/16,258487,00\n",
        "2009/10/01,169387,06\n",
        "2009/09/16,202912,48\n",
        "2009/09/01,015865,32\n",
        "2009/08/16,462933,96\n",
        "2009/08/01,154986,92\n",
        "2009/07/16,000816,94\n",
        "2009/07/01,207542,66\n",
        "2009/06/16,930456,15\n",
        "2009/06/01,777661,26\n",
        "2009/05/16,111411,54\n",
        "2009/05/02,294452,11\n",
        "2009/04/16,368415,33\n",
        "2009/04/01,816578,50\n",
        "2009/03/16,268812,36\n",
        "2009/03/01,553091,67\n",
        "2009/02/16,038730,93\n",
        "2009/02/01,534533,69\n",
        "2009/01/16,743212,25\n",
        "2008/12/30,218596,22\n",
        "2008/12/16,074114,25\n",
        "2008/12/01,205434,05\n",
        "2008/11/16,002612,20\n",
        "2008/11/01,272028,76\n",
        "2008/10/16,431277,98\n",
        "2008/10/01,882911,67\n",
        "2008/09/16,012377,56\n",
        "2008/09/01,695993,09\n",
        "2008/08/16,380377,36\n",
        "2008/08/01,850348,11\n",
        "2008/07/16,257374,41\n",
        "2008/07/01,943671,50\n",
        "2008/06/16,729111,75\n",
        "2008/06/01,414875,35\n",
        "2008/05/16,329231,69\n",
        "2008/05/02,453011,62\n",
        "2008/04/16,982800,64\n",
        "2008/04/01,012653,71\n",
        "2008/03/16,074946,33\n",
        "2008/03/01,936685,05\n",
        "2008/02/16,137054,80\n",
        "2008/02/01,212684,26\n",
        "2008/01/16,556010,81\n",
        "2007/12/30,595411,81\n",
        "2007/12/16,513501,96\n",
        "2007/12/01,113410,18\n",
        "2007/11/16,562481,73\n",
        "2007/11/01,927907,88\n",
        "2007/10/16,032988,48\n",
        "2007/10/01,430667,76\n",
        "2007/09/16,499336,45\n",
        "2007/09/01,331810,69\n",
        "2007/08/16,476207,93\n",
        "2007/08/01,429924,29\n",
        "2007/07/16,527384,77\n",
        "2007/07/01,565151,76\n",
        "2007/06/16,393194,41\n",
        "2007/06/01,836393,05\n",
        "2007/05/16,232897,25\n",
        "2007/05/02,430374,81\n",
        "2007/04/16,405105,63\n",
        "2007/04/01,622780,93\n",
        "2007/03/16,876763,85\n",
        "2007/03/01,742425,61\n",
        "2007/02/16,277859,95\n",
        "2007/02/01,769925,56\n",
        "2007/01/16,838739,55\n",
        "2006/12/30,778584,07\n",
        "2006/12/16,147977,45\n",
        "2006/12/01,270052,12\n",
        "2006/11/16,562856,94\n",
        "2006/11/01,910957,29\n",
        "2006/10/16,264825,58\n",
        "2006/10/01,952335,92\n",
        "2006/09/16,217948,41\n",
        "2006/09/01,381761,44\n",
        "2006/08/16,977486,91\n",
        "2006/08/01,238654,88\n",
        "2006/07/16,512434,48\n",
        "2006/07/01,952890,65\n",
        "2006/06/16,100935,17\n",
        "2006/06/01,810850,99\n",
        "2006/05/16,100344,71\n",
        "2006/05/02,024554,27\n",
        "2006/04/16,038564,49\n",
        "2006/04/01,738365,95\n",
        "2006/03/16,936177,30\n",
        "2006/03/01,582473,43\n",
        "2006/02/16,317009,66\n",
        "2006/02/01,412729,87\n",
        "2006/01/16,432747,79\n",
        "2005/12/30,492955,94\n",
        "2005/12/16,449565,86\n",
        "2005/12/01,388551,17\n",
        "2005/11/16,742518,80\n",
        "2005/11/01,970577,98\n",
        "2005/10/16,041072,71\n",
        "2005/10/01,766482,24\n",
        "2005/09/16,214768,10\n",
        "2005/09/01,316933,17\n",
        "2005/08/16,475560,68\n",
        "2005/08/01,961633,26\n",
        "2005/07/16,477452,13\n",
        "2005/07/01,009554,87\n",
        "2005/06/16,793070,44\n",
        "2005/06/01,176893,35\n",
        "2005/05/16,867134,97\n",
        "2005/05/02,772467,43\n",
        "2005/04/16,119327,10\n",
        "2005/04/01,815753,69\n",
        "2005/03/16,196345,03\n",
        "2005/03/01,800751,93\n",
        "2005/02/16,816422,53\n",
        "2005/02/01,540054,34\n",
        "2005/01/16,335022,08\n",
        "2004/12/30,168858,28\n",
        "2004/12/16,479372,17\n",
        "2004/12/01,504658,69\n",
        "2004/11/16,754622,64\n",
        "2004/11/01,185966,23\n",
        "2004/10/16,355858,62\n",
        "2004/10/01,110866,66\n",
        "2004/09/16,923373,59\n",
        "2004/09/01,096597,70\n",
        "2004/08/16,335921,59\n",
        "2004/08/02,868990,45\n",
        "2004/07/16,205588,25\n",
        "2004/07/01,312471,66\n",
        "2004/06/16,208713,87\n",
        "2004/06/01,614144,72\n",
        "2004/05/16,589207,13\n",
        "2004/05/02,653403,91\n",
        "2004/04/16,705832,12\n",
        "2004/04/01,196391,62\n",
        "2004/03/16,615366,69\n",
        "2004/03/01,697483,50\n",
        "2004/02/16,698002,00\n",
        "2004/02/01,216822,77\n",
        "2004/01/16,731342,96\n",
        "2003/12/30,739447,64\n",
        "2003/12/16,177947,87\n",
        "2003/12/01,991307,78\n",
        "2003/11/16,238511,68\n",
        "2003/11/01,941438,47\n",
        "2003/10/16,305500,03\n",
        "2003/10/01,912040,43\n",
        "2003/09/16,600589,53\n",
        "2003/09/01,187813,92\n",
        "2003/08/16,354771,00\n",
        "2003/08/01,766098,91\n",
        "2003/07/16,679545,75\n",
        "\"\"\"\n",
        "\n",
        "# --- Custom Functions for Window-based Features ---\n",
        "\n",
        "def get_most_common_digit_in_window(series):\n",
        "    \"\"\"\n",
        "    Returns the most common digit in a series of digits.\n",
        "    Used for rolling window calculations.\n",
        "    \"\"\"\n",
        "    if series.empty:\n",
        "        return -1\n",
        "    counts = pd.Series(series).value_counts()\n",
        "    if not counts.empty:\n",
        "        # If there's a tie, value_counts() returns the lowest index first (smallest digit)\n",
        "        return counts.index[0]\n",
        "    return -1\n",
        "\n",
        "def get_second_most_common_digit_in_window(series):\n",
        "    \"\"\"\n",
        "    Returns the second most common digit in a series of digits.\n",
        "    Used for rolling window calculations.\n",
        "    \"\"\"\n",
        "    if series.empty:\n",
        "        return -1\n",
        "    counts = pd.Series(series).value_counts()\n",
        "    if len(counts) > 1:\n",
        "        return counts.index[1]\n",
        "    return -1\n",
        "\n",
        "def get_least_common_digit_in_window(series):\n",
        "    \"\"\"\n",
        "    Returns the least common digit in a series of digits.\n",
        "    Used for rolling window calculations.\n",
        "    \"\"\"\n",
        "    if series.empty:\n",
        "        return -1\n",
        "    counts = pd.Series(series).value_counts()\n",
        "    if not counts.empty:\n",
        "        # The last one after sorting by count ascending\n",
        "        return counts.index[-1]\n",
        "    return -1\n",
        "\n",
        "# --- End Custom Functions ---\n",
        "\n",
        "# --- 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á Feature Engineering) ---\n",
        "print(\"--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\")\n",
        "\n",
        "df = pd.read_csv(pd.io.common.StringIO(csv_content))\n",
        "df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'] = pd.to_datetime(df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'], format='%Y/%m/%d', errors='coerce') # Use coerce to handle invalid dates\n",
        "df = df.rename(columns={'‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': 'Date', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)': '‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': '‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'})\n",
        "\n",
        "# Remove rows where 'Date' conversion failed\n",
        "df.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].astype(str).str.zfill(6)\n",
        "df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].astype(str).str.zfill(2)\n",
        "df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[-3:]\n",
        "\n",
        "df['R1_Last3_Hundreds'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[0].astype(int)\n",
        "df['R1_Last3_Tens'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[1].astype(int)\n",
        "df['R1_Last3_Units'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[2].astype(int)\n",
        "df['L2_Tens'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].str[0].astype(int)\n",
        "df['L2_Units'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].str[1].astype(int)\n",
        "\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['DayOfMonth'] = df['Date'].dt.day\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "# Features ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á\n",
        "df['R1_Sum'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(int(d) for d in str(x)))\n",
        "df['R1_Even_Count'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 == 0))\n",
        "df['R1_Odd_Count'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 != 0))\n",
        "df['R1_Unique_Count'] = df['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: len(set(x)))\n",
        "df['L2_Sum'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(int(d) for d in str(x)))\n",
        "df['L2_Even_Count'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 == 0))\n",
        "df['L2_Odd_Count'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 != 0))\n",
        "\n",
        "# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Lag Features (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Data Leakage) ---\n",
        "lags = range(1, 7) # ‡∏à‡∏≤‡∏Å 1 ‡∏ñ‡∏∂‡∏á 6 ‡∏á‡∏ß‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
        "\n",
        "for lag in lags:\n",
        "    df[f'Prev_L2_Lag{lag}'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].shift(lag).fillna('00')\n",
        "    df[f'Prev_R1_Last3_Lag{lag}'] = df['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].shift(lag).fillna('000')\n",
        "\n",
        "    df[f'Prev_L2_Lag{lag}_Tens'] = df[f'Prev_L2_Lag{lag}'].str[0].astype(int)\n",
        "    df[f'Prev_L2_Lag{lag}_Units'] = df[f'Prev_L2_Lag{lag}'].str[1].astype(int)\n",
        "    df[f'Prev_R1_Last3_Lag{lag}_Hundreds'] = df[f'Prev_R1_Last3_Lag{lag}'].str[0].astype(int)\n",
        "    df[f'Prev_R1_Last3_Lag{lag}_Tens'] = df[f'Prev_R1_Last3_Lag{lag}'].str[1].astype(int)\n",
        "    df[f'Prev_R1_Last3_Lag{lag}_Units'] = df[f'Prev_R1_Last3_Lag{lag}'].str[2].astype(int)\n",
        "\n",
        "    df[f'Prev_R1_Sum_Lag{lag}'] = df['R1_Sum'].shift(lag).fillna(df['R1_Sum'].mean())\n",
        "    df[f'Prev_L2_Sum_Lag{lag}'] = df['L2_Sum'].shift(lag).fillna(df['L2_Sum'].mean())\n",
        "    df[f'Prev_R1_Even_Count_Lag{lag}'] = df['R1_Even_Count'].shift(lag).fillna(df['R1_Even_Count'].mean())\n",
        "    df[f'Prev_L2_Even_Count_Lag{lag}'] = df['L2_Even_Count'].shift(lag).fillna(df['L2_Even_Count'].mean())\n",
        "\n",
        "# --- ‡πÄ‡∏û‡∏¥‡πà‡∏°: Rolling Mean/Std for features ---\n",
        "window_size_rolling_mean_std = 5 # 5 ‡∏á‡∏ß‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
        "for col in ['R1_Sum', 'L2_Sum', 'R1_Even_Count', 'L2_Even_Count']:\n",
        "    df[f'{col}_RollingMean_{window_size_rolling_mean_std}'] = df[col].rolling(window=window_size_rolling_mean_std).mean().shift(1)\n",
        "    df[f'{col}_RollingStd_{window_size_rolling_mean_std}'] = df[col].rolling(window=window_size_rolling_mean_std).std().shift(1)\n",
        "\n",
        "# Fill NaNs created by rolling and shifting (e.g., first few rows)\n",
        "for col in df.columns:\n",
        "    if 'Rolling' in col and df[col].dtype in ['float64', 'int64']:\n",
        "        df[col] = df[col].fillna(df[col].mean()) # Fill numerical rolling features with their mean\n",
        "\n",
        "# --- New Feature Engineering: Window-based Frequency and Positional Patterns ---\n",
        "print(\"Generating window-based frequency and positional features...\")\n",
        "\n",
        "window_sizes = [8, 10, 12] # ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ç‡∏ô‡∏≤‡∏î\n",
        "\n",
        "target_cols_r1_digits = ['R1_Last3_Hundreds', 'R1_Last3_Tens', 'R1_Last3_Units']\n",
        "target_cols_l2_digits = ['L2_Tens', 'L2_Units']\n",
        "\n",
        "for ws in window_sizes:\n",
        "    print(f\"  - Processing window size: {ws}\")\n",
        "    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß\n",
        "    for digit_pos in target_cols_l2_digits:\n",
        "        df[f'{digit_pos}_MostCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df[f'{digit_pos}_SecondMostCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_second_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df[f'{digit_pos}_LeastCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_least_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        for digit in range(10):\n",
        "            df[f'{digit_pos}_Freq{digit}_Window{ws}'] = df[digit_pos].rolling(\n",
        "                window=ws, min_periods=1\n",
        "            ).apply(lambda x: (x == digit).sum(), raw=False).shift(1).fillna(0).astype(int)\n",
        "\n",
        "    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 3 ‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1\n",
        "    for digit_pos in target_cols_r1_digits:\n",
        "        df[f'{digit_pos}_MostCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df[f'{digit_pos}_SecondMostCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_second_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df[f'{digit_pos}_LeastCommon_Window{ws}'] = df[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_least_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        for digit in range(10):\n",
        "            df[f'{digit_pos}_Freq{digit}_Window{ws}'] = df[digit_pos].rolling(\n",
        "                window=ws, min_periods=1\n",
        "            ).apply(lambda x: (x == digit).sum(), raw=False).shift(1).fillna(0).astype(int)\n",
        "\n",
        "print(\"Finished generating window-based features.\")\n",
        "# --- End New Feature Engineering ---\n",
        "\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Features ‡πÅ‡∏•‡∏∞ Target Variables\n",
        "features = [\n",
        "    'Year', 'Month', 'Day', 'DayOfWeek', 'DayOfMonth', 'DayOfYear', 'WeekOfYear',\n",
        "    'R1_Sum', 'R1_Even_Count', 'R1_Odd_Count', 'R1_Unique_Count',\n",
        "    'L2_Sum', 'L2_Even_Count', 'L2_Odd_Count'\n",
        "]\n",
        "\n",
        "for lag in lags:\n",
        "    features.extend([\n",
        "        f'Prev_L2_Lag{lag}_Tens', f'Prev_L2_Lag{lag}_Units',\n",
        "        f'Prev_R1_Last3_Lag{lag}_Hundreds', f'Prev_R1_Last3_Lag{lag}_Tens', f'Prev_R1_Last3_Lag{lag}_Units',\n",
        "        f'Prev_R1_Sum_Lag{lag}', f'Prev_L2_Sum_Lag{lag}',\n",
        "        f'Prev_R1_Even_Count_Lag{lag}', f'Prev_L2_Even_Count_Lag{lag}'\n",
        "    ])\n",
        "\n",
        "for col in ['R1_Sum', 'L2_Sum', 'R1_Even_Count', 'L2_Even_Count']:\n",
        "    features.extend([f'{col}_RollingMean_{window_size_rolling_mean_std}', f'{col}_RollingStd_{window_size_rolling_mean_std}'])\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏° Features ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å Window-based patterns\n",
        "for ws in window_sizes:\n",
        "    for digit_pos in target_cols_l2_digits + target_cols_r1_digits:\n",
        "        features.append(f'{digit_pos}_MostCommon_Window{ws}')\n",
        "        features.append(f'{digit_pos}_SecondMostCommon_Window{ws}')\n",
        "        features.append(f'{digit_pos}_LeastCommon_Window{ws}')\n",
        "        for digit in range(10):\n",
        "            features.append(f'{digit_pos}_Freq{digit}_Window{ws}')\n",
        "\n",
        "# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏Ñ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (L2_Tens, L2_Units)\n",
        "target_cols_l2 = ['L2_Tens', 'L2_Units']\n",
        "target_cols_r1 = ['R1_Last3_Hundreds', 'R1_Last3_Tens', 'R1_Last3_Units'] # ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature engineering ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏£‡∏á‡πÜ\n",
        "\n",
        "# ‡∏î‡∏£‡∏≠‡∏õ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ shift (‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Lag ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ‡πÅ‡∏•‡∏∞ rolling features\n",
        "df_ml = df.dropna(subset=features + target_cols_l2 + target_cols_r1).copy() # Ensure all targets are considered for dropping NaNs\n",
        "\n",
        "if df_ml.empty:\n",
        "    print(\"! ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ.\")\n",
        "    exit()\n",
        "\n",
        "print(\"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: {len(df_ml)} ‡πÅ‡∏ñ‡∏ß\")\n",
        "\n",
        "# --- 2. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Train models) ‡∏û‡∏£‡πâ‡∏≠‡∏° Hyperparameter Tuning ‡πÅ‡∏•‡∏∞ TimeSeriesSplit ---\n",
        "print(\"\\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Hyperparameter Tuning ‡πÅ‡∏•‡∏∞ TimeSeriesSplit ---\")\n",
        "\n",
        "trained_models = {}\n",
        "label_encoders = {}\n",
        "scalers = {} # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MinMaxScaler\n",
        "best_params_found = {}\n",
        "\n",
        "model_params = {\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200],\n",
        "            'max_depth': [None, 5, 10, 15],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200],\n",
        "            'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "        }\n",
        "    },\n",
        "    'MLPClassifier': {\n",
        "        'model': MLPClassifier(random_state=42, max_iter=2000),\n",
        "        'params': {\n",
        "            'hidden_layer_sizes': [(50,), (50, 20), (100,), (100, 50)],\n",
        "            'activation': ['relu', 'tanh'],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "tscv_grid = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "for target in target_cols_l2:\n",
        "    print(f\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: {target}\")\n",
        "    trained_models[target] = {}\n",
        "\n",
        "    Y = df_ml[target]\n",
        "    X = df_ml[features]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(np.arange(10)) # Ensure all possible digits (0-9) are in the encoder\n",
        "    Y_encoded = le.transform(Y)\n",
        "    label_encoders[target] = le\n",
        "\n",
        "    scaler_temp = MinMaxScaler()\n",
        "    X_scaled_temp = scaler_temp.fit_transform(X)\n",
        "    scalers[target] = scaler_temp\n",
        "\n",
        "    for model_name, config in model_params.items():\n",
        "        print(f\"  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô {model_name}...\")\n",
        "        try:\n",
        "            grid_search = GridSearchCV(\n",
        "                config['model'],\n",
        "                config['params'],\n",
        "                cv=tscv_grid,\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1,\n",
        "                verbose=0\n",
        "            )\n",
        "            grid_search.fit(X_scaled_temp, Y_encoded)\n",
        "\n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "            best_score = grid_search.best_score_\n",
        "\n",
        "            trained_models[target][model_name] = best_model\n",
        "            best_params_found[f\"{target}_{model_name}\"] = best_params\n",
        "            print(f\"    ‚úÖ ‡∏ù‡∏∂‡∏Å {model_name} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô. Best Score: {best_score:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å {model_name} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {target}: {e}\")\n",
        "            trained_models[target][model_name] = None\n",
        "\n",
        "print(\"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n",
        "\n",
        "# --- 3. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Data Leakage ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î) ---\n",
        "print(\"\\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting ---\")\n",
        "\n",
        "backtest_results = []\n",
        "num_backtest_periods = min(30, len(df_ml) // 4)\n",
        "if num_backtest_periods < 5:\n",
        "    num_backtest_periods = 5\n",
        "if len(df_ml) < num_backtest_periods + lags[-1]: # Ensure enough data for initial training + max lag\n",
        "    num_backtest_periods = len(df_ml) - lags[-1] - 1 # Adjusted to ensure enough data for training\n",
        "\n",
        "if num_backtest_periods <= 0:\n",
        "    print(\"! ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Backtesting ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏ß‡∏î Backtest\")\n",
        "    exit()\n",
        "\n",
        "print(f\"‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Backtesting ‡∏ö‡∏ô {num_backtest_periods} ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î.\")\n",
        "\n",
        "# Dictionary to store accuracy history for each model and target for dynamic weighting\n",
        "# Stores 0s and 1s for correct/incorrect predictions\n",
        "model_accuracy_history = {target: {name: [] for name in model_params.keys()} for target in target_cols_l2}\n",
        "\n",
        "# Start index for backtesting loop\n",
        "start_backtest_idx = len(df_ml) - num_backtest_periods\n",
        "\n",
        "for i in range(start_backtest_idx, len(df_ml)):\n",
        "    # Define training and testing sets for the current fold\n",
        "    # Training data includes all data up to the current test index (exclusive)\n",
        "    train_df_current = df_ml.iloc[:i]\n",
        "    # Test data is the current row\n",
        "    test_row_current = df_ml.iloc[i]\n",
        "\n",
        "    if train_df_current.empty:\n",
        "        print(f\"Skipping backtest for index {i}: Training data is empty.\")\n",
        "        continue\n",
        "\n",
        "    predicted_l2_digits_backtest = []\n",
        "\n",
        "    # Store predictions for the current backtest iteration to update accuracy history\n",
        "    current_backtest_predictions_for_accuracy = {target_name: {} for target_name in target_cols_l2}\n",
        "\n",
        "    for target_name in target_cols_l2:\n",
        "        Y_train_local = train_df_current[target_name]\n",
        "        X_train_local = train_df_current[features]\n",
        "\n",
        "        local_scaler = MinMaxScaler()\n",
        "        X_train_scaled_local = local_scaler.fit_transform(X_train_local)\n",
        "\n",
        "        local_le = label_encoders[target_name]\n",
        "        Y_train_encoded_local = local_le.transform(Y_train_local)\n",
        "\n",
        "        preds_for_voting = []\n",
        "\n",
        "        # Calculate dynamic weights based on past performance for this target\n",
        "        current_weights = {}\n",
        "        total_weight_sum = 0.0\n",
        "        for model_name in model_params.keys():\n",
        "            if model_accuracy_history[target_name][model_name]:\n",
        "                # Weight based on mean accuracy of *past* predictions for this model and target\n",
        "                weight = np.mean(model_accuracy_history[target_name][model_name])\n",
        "            else:\n",
        "                weight = 1.0 / len(model_params) # Default uniform weight if no history yet\n",
        "            current_weights[model_name] = weight\n",
        "            total_weight_sum += weight\n",
        "\n",
        "        # Normalize weights\n",
        "        if total_weight_sum > 0:\n",
        "            current_weights = {k: v / total_weight_sum for k, v in current_weights.items()}\n",
        "        else: # Fallback to uniform if sum is zero (shouldn't happen with default weight)\n",
        "            current_weights = {k: 1.0 / len(model_params) for k in model_params.keys()}\n",
        "\n",
        "        for model_name, model_from_grid in trained_models[target_name].items():\n",
        "            if model_from_grid:\n",
        "                try:\n",
        "                    # Retrain the model with best parameters on current training set\n",
        "                    # Important: Use a fresh instance to avoid data leakage from previous folds\n",
        "                    model_instance = model_from_grid.__class__(**best_params_found[f\"{target_name}_{model_name}\"])\n",
        "                    model_instance.fit(X_train_scaled_local, Y_train_encoded_local)\n",
        "\n",
        "                    X_test_scaled_local = local_scaler.transform(pd.DataFrame([test_row_current[features]]))\n",
        "\n",
        "                    proba_encoded = model_instance.predict_proba(X_test_scaled_local)[0]\n",
        "                    proba_dict = {local_le.inverse_transform([digit_idx])[0]: proba for digit_idx, proba in enumerate(proba_encoded)}\n",
        "                    preds_for_voting.append(proba_dict)\n",
        "\n",
        "                    # Store individual prediction for accuracy history update *after* this row's prediction is made\n",
        "                    pred_digit_for_accuracy = local_le.inverse_transform([model_instance.predict(X_test_scaled_local)[0]])[0]\n",
        "                    current_backtest_predictions_for_accuracy[target_name][model_name] = pred_digit_for_accuracy\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error during backtest prediction for {target_name} with {model_name} at index {i}: {e}\")\n",
        "                    preds_for_voting.append({d: 1/10 for d in range(10)}) # Fallback to uniform\n",
        "                    current_backtest_predictions_for_accuracy[target_name][model_name] = -1 # Indicate failure for accuracy tracking\n",
        "\n",
        "        # Weighted Voting for this backtest prediction\n",
        "        combined_probas = {d: 0 for d in range(10)}\n",
        "\n",
        "        for model_idx, proba_dict in enumerate(preds_for_voting):\n",
        "            model_name = list(model_params.keys())[model_idx]\n",
        "            weight = current_weights.get(model_name, 0) # Get the dynamically calculated weight\n",
        "\n",
        "            for digit, proba in proba_dict.items():\n",
        "                combined_probas[digit] += proba * weight\n",
        "\n",
        "        # Select the digit with the highest combined probability\n",
        "        if combined_probas:\n",
        "            final_pred_digit = max(combined_probas, key=combined_probas.get)\n",
        "        else:\n",
        "            final_pred_digit = -1 # Fallback if no probabilities\n",
        "\n",
        "        predicted_l2_digits_backtest.append(str(final_pred_digit))\n",
        "\n",
        "    actual_l2 = test_row_current['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß']\n",
        "    predicted_l2_combined = \"\".join(predicted_l2_digits_backtest)\n",
        "\n",
        "    # Update model_accuracy_history *after* the current prediction\n",
        "    for target_name in target_cols_l2:\n",
        "        actual_digit = df_ml.iloc[i][target_name]\n",
        "        for model_name, pred_digit in current_backtest_predictions_for_accuracy[target_name].items():\n",
        "            # Only record if prediction was made (not -1 for error)\n",
        "            if pred_digit != -1:\n",
        "                model_accuracy_history[target_name][model_name].append(1 if pred_digit == actual_digit else 0)\n",
        "            # If there was an error, don't penalize or reward, just skip appending for that model/target in this round\n",
        "\n",
        "    backtest_results.append({\n",
        "        'Date': test_row_current['Date'],\n",
        "        '‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß_Actual': actual_l2,\n",
        "        'Predicted_L2': predicted_l2_combined,\n",
        "        'L2_Match': (predicted_l2_combined == actual_l2)\n",
        "    })\n",
        "\n",
        "backtest_df = pd.DataFrame(backtest_results)\n",
        "\n",
        "# --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• Backtesting ---\n",
        "print(\"\\n--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Backtesting ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° ---\")\n",
        "if not backtest_df.empty:\n",
        "    total_l2_matches = backtest_df['L2_Match'].sum()\n",
        "    total_predictions = len(backtest_df)\n",
        "    overall_accuracy = total_l2_matches / total_predictions if total_predictions > 0 else 0\n",
        "    print(f\"üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Backtesting: {overall_accuracy:.4f} ({total_l2_matches}/{total_predictions})\")\n",
        "    print(\"\\n‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏• Backtesting:\")\n",
        "    print(backtest_df.to_string())\n",
        "else:\n",
        "    print(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Backtesting ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\")\n",
        "\n",
        "print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Backtesting ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "\n",
        "\n",
        "# --- 4. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ---\n",
        "print(\"\\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ---\")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "last_date = df['Date'].max()\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "next_draw_date = last_date + timedelta(days=15) # Assuming approx 15 days for next draw\n",
        "# Adjust for specific lottery dates if needed (e.g., 1st and 16th, and adjust for holidays like Jan 17, Dec 30)\n",
        "# This simplified logic assumes a consistent 15-day interval.\n",
        "# For more accurate next date calculation, one would need a lookup table or more complex date logic.\n",
        "# Example: If last_date is 2025-06-16, next_draw_date should be 2025-07-01.\n",
        "# If last_date is 2025-07-01, next_draw_date should be 2025-07-16.\n",
        "# Let's add a more robust next draw date calculation:\n",
        "\n",
        "def get_next_draw_date(current_last_date):\n",
        "    year = current_last_date.year\n",
        "    month = current_last_date.month\n",
        "    day = current_last_date.day\n",
        "\n",
        "    if day == 1 or day == 2: # For 1st/2nd of month draws\n",
        "        # Next draw should be 16th of same month\n",
        "        next_date = datetime(year, month, 16)\n",
        "        if next_date <= current_last_date: # If current draw was already 16th, go to next month's 1st\n",
        "            if month == 12:\n",
        "                next_date = datetime(year + 1, 1, 1)\n",
        "            else:\n",
        "                next_date = datetime(year, month + 1, 1)\n",
        "    elif day == 16 or day == 17: # For 16th/17th of month draws\n",
        "        # Next draw should be 1st/2nd of next month\n",
        "        if month == 12:\n",
        "            next_date = datetime(year + 1, 1, 1)\n",
        "        else:\n",
        "            next_date = datetime(year, month + 1, 1)\n",
        "        # Handle special case for Jan 17 -> Feb 1, Dec 30 -> Jan 17 (simplified)\n",
        "        if next_date.month == 1 and next_date.day == 1:\n",
        "            next_date = datetime(next_date.year, next_date.month, 17) # Jan 17 draw\n",
        "        elif next_date.month == 12 and current_last_date.day == 16: # Dec 16 -> Dec 30 (special)\n",
        "            next_date = datetime(next_last_date.year, 12, 30)\n",
        "\n",
        "\n",
        "    # If the calculated next_date is not one of the standard draw dates,\n",
        "    # find the next standard date after current_last_date\n",
        "    standard_draw_days = [1, 16]\n",
        "    # For December 30th as a draw date, and January 17th\n",
        "    if current_last_date.month == 12 and current_last_date.day == 16:\n",
        "        standard_draw_days.append(30) # Special case for end of year\n",
        "\n",
        "    # Find the next date in the sequence\n",
        "    temp_date = current_last_date\n",
        "    while True:\n",
        "        temp_date += timedelta(days=1)\n",
        "        if (temp_date.day in standard_draw_days and temp_date.month != 1 and temp_date.day != 17) or \\\n",
        "           (temp_date.month == 1 and temp_date.day == 17) or \\\n",
        "           (temp_date.month == 12 and temp_date.day == 30):\n",
        "            break\n",
        "        # Handle cases where 1st/16th falls on a non-draw day (e.g., weekend or holiday, moved to next business day)\n",
        "        # For simplicity, we stick to 1st/16th, and assume the data already reflects actual draw dates.\n",
        "        # If the actual next draw date is NOT the 1st or 16th (e.g. 17th or 30th),\n",
        "        # this logic needs to be more robust, potentially involving a list of all historical draw dates.\n",
        "        # For now, let's assume the provided CSV is exhaustive and next date is generally 1st/16th.\n",
        "        # Given the data has 2025/01/17, 2023/07/31, 2023/12/30, 2022/02/17, 2015/12/17, 2015/06/02, 2015/05/02\n",
        "        # This function should predict the *next* date correctly based on actual lottery schedule rules.\n",
        "        # A simple +15 days is often close, but not always exact.\n",
        "    return temp_date\n",
        "\n",
        "# Using a simpler approach for next draw date that is common\n",
        "def get_next_thai_lottery_date(last_draw_date):\n",
        "    year = last_draw_date.year\n",
        "    month = last_draw_date.month\n",
        "    day = last_draw_date.day\n",
        "\n",
        "    if day == 1 or day == 2: # Current draw was 1st/2nd\n",
        "        next_date = datetime(year, month, 16)\n",
        "    elif day == 16 or day == 17: # Current draw was 16th/17th\n",
        "        if month == 12:\n",
        "            next_date = datetime(year + 1, 1, 1)\n",
        "        else:\n",
        "            next_date = datetime(year, month + 1, 1)\n",
        "    elif day == 30: # Current draw was Dec 30\n",
        "        next_date = datetime(year + 1, 1, 17) # Jan 17th draw\n",
        "    else: # Fallback for unusual dates, move to next standard date\n",
        "        next_date = last_draw_date + timedelta(days=1)\n",
        "        while True:\n",
        "            if (next_date.day == 1 or next_date.day == 16 or \\\n",
        "                (next_date.month == 1 and next_date.day == 17) or \\\n",
        "                (next_date.month == 12 and next_date.day == 30)) and \\\n",
        "               next_date > last_draw_date:\n",
        "                break\n",
        "            next_date += timedelta(days=1)\n",
        "    return next_date\n",
        "\n",
        "next_draw_date = get_next_thai_lottery_date(last_date)\n",
        "print(f\"‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {next_draw_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Features\n",
        "last_n_rows = df_ml.tail(lags[-1] + window_sizes[-1] + 1) # Need enough previous data for features\n",
        "\n",
        "# Create a dummy row for the next draw with features calculated based on previous data\n",
        "next_draw_data = {col: 0 for col in features} # Initialize with zeros\n",
        "next_draw_df = pd.DataFrame([next_draw_data])\n",
        "next_draw_df['Date'] = next_draw_date\n",
        "next_draw_df['Year'] = next_draw_date.year\n",
        "next_draw_df['Month'] = next_draw_date.month\n",
        "next_draw_df['Day'] = next_draw_date.day\n",
        "next_draw_df['DayOfWeek'] = next_draw_date.dayofweek\n",
        "next_draw_df['DayOfMonth'] = next_draw_date.day\n",
        "next_draw_df['DayOfYear'] = next_draw_date.dayofyear\n",
        "next_draw_df['WeekOfYear'] = next_draw_date.isocalendar().week\n",
        "\n",
        "# Temporarily append the new row to the full historical df for feature calculation\n",
        "# This is safe because we only use .shift() and .rolling().shift(1)\n",
        "# which inherently look at previous data. The .copy() ensures original df_ml is not modified.\n",
        "df_for_prediction_features = df.copy()\n",
        "# Create a dummy row with placeholder values for '‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1' and '‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'\n",
        "# These are needed for feature calculation, but their values for the *next* draw are unknown.\n",
        "# We'll fill them with placeholders and then only use the calculated features.\n",
        "dummy_next_row = {col: None for col in df_for_prediction_features.columns}\n",
        "dummy_next_row['Date'] = next_draw_date\n",
        "dummy_next_row['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'] = '000000' # Placeholder\n",
        "dummy_next_row['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'] = '00' # Placeholder\n",
        "df_for_prediction_features = pd.concat([df_for_prediction_features, pd.DataFrame([dummy_next_row])], ignore_index=True)\n",
        "\n",
        "\n",
        "# Re-calculate all features for the *entire* df_for_prediction_features including the new row\n",
        "# This ensures that the last row (our prediction target) has its features correctly calculated\n",
        "# based on all *previous* historical data.\n",
        "df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].astype(str).str.zfill(6)\n",
        "df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].astype(str).str.zfill(2)\n",
        "df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[-3:]\n",
        "\n",
        "df_for_prediction_features['R1_Last3_Hundreds'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[0].astype(int)\n",
        "df_for_prediction_features['R1_Last3_Tens'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[1].astype(int)\n",
        "df_for_prediction_features['R1_Last3_Units'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].str[2].astype(int)\n",
        "df_for_prediction_features['L2_Tens'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].str[0].astype(int)\n",
        "df_for_prediction_features['L2_Units'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].str[1].astype(int)\n",
        "\n",
        "df_for_prediction_features['Year'] = df_for_prediction_features['Date'].dt.year\n",
        "df_for_prediction_features['Month'] = df_for_prediction_features['Date'].dt.month\n",
        "df_for_prediction_features['Day'] = df_for_prediction_features['Date'].dt.day\n",
        "df_for_prediction_features['DayOfWeek'] = df_for_prediction_features['Date'].dt.dayofweek\n",
        "df_for_prediction_features['DayOfMonth'] = df_for_prediction_features['Date'].dt.day\n",
        "df_for_prediction_features['DayOfYear'] = df_for_prediction_features['Date'].dt.dayofyear\n",
        "df_for_prediction_features['WeekOfYear'] = df_for_prediction_features['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "df_for_prediction_features['R1_Sum'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(int(d) for d in str(x)))\n",
        "df_for_prediction_features['R1_Even_Count'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 == 0))\n",
        "df_for_prediction_features['R1_Odd_Count'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 != 0))\n",
        "df_for_prediction_features['R1_Unique_Count'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏´‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].apply(lambda x: len(set(x)))\n",
        "df_for_prediction_features['L2_Sum'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(int(d) for d in str(x)))\n",
        "df_for_prediction_features['L2_Even_Count'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 == 0))\n",
        "df_for_prediction_features['L2_Odd_Count'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].apply(lambda x: sum(1 for d in str(x) if int(d) % 2 != 0))\n",
        "\n",
        "for lag in lags:\n",
        "    df_for_prediction_features[f'Prev_L2_Lag{lag}'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢2‡∏ï‡∏±‡∏ß'].shift(lag).fillna('00')\n",
        "    df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}'] = df_for_prediction_features['‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢3‡∏ï‡∏±‡∏ß‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà1'].shift(lag).fillna('000')\n",
        "\n",
        "    df_for_prediction_features[f'Prev_L2_Lag{lag}_Tens'] = df_for_prediction_features[f'Prev_L2_Lag{lag}'].str[0].astype(int)\n",
        "    df_for_prediction_features[f'Prev_L2_Lag{lag}_Units'] = df_for_prediction_features[f'Prev_L2_Lag{lag}'].str[1].astype(int)\n",
        "    df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}_Hundreds'] = df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}'].str[0].astype(int)\n",
        "    df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}_Tens'] = df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}'].str[1].astype(int)\n",
        "    df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}_Units'] = df_for_prediction_features[f'Prev_R1_Last3_Lag{lag}'].str[2].astype(int)\n",
        "\n",
        "    df_for_prediction_features[f'Prev_R1_Sum_Lag{lag}'] = df_for_prediction_features['R1_Sum'].shift(lag).fillna(df_for_prediction_features['R1_Sum'].mean())\n",
        "    df_for_prediction_features[f'Prev_L2_Sum_Lag{lag}'] = df_for_prediction_features['L2_Sum'].shift(lag).fillna(df_for_prediction_features['L2_Sum'].mean())\n",
        "    df_for_prediction_features[f'Prev_R1_Even_Count_Lag{lag}'] = df_for_prediction_features['R1_Even_Count'].shift(lag).fillna(df_for_prediction_features['R1_Even_Count'].mean())\n",
        "    df_for_prediction_features[f'Prev_L2_Even_Count_Lag{lag}'] = df_for_prediction_features['L2_Even_Count'].shift(lag).fillna(df_for_prediction_features['L2_Even_Count'].mean())\n",
        "\n",
        "for col in ['R1_Sum', 'L2_Sum', 'R1_Even_Count', 'L2_Even_Count']:\n",
        "    df_for_prediction_features[f'{col}_RollingMean_{window_size_rolling_mean_std}'] = df_for_prediction_features[col].rolling(window=window_size_rolling_mean_std).mean().shift(1)\n",
        "    df_for_prediction_features[f'{col}_RollingStd_{window_size_rolling_mean_std}'] = df_for_prediction_features[col].rolling(window=window_size_rolling_mean_std).std().shift(1)\n",
        "\n",
        "for col in df_for_prediction_features.columns:\n",
        "    if 'Rolling' in col and df_for_prediction_features[col].dtype in ['float64', 'int64']:\n",
        "        df_for_prediction_features[col] = df_for_prediction_features[col].fillna(df_for_prediction_features[col].mean())\n",
        "\n",
        "\n",
        "for ws in window_sizes:\n",
        "    for digit_pos in target_cols_l2_digits + target_cols_r1_digits:\n",
        "        df_for_prediction_features[f'{digit_pos}_MostCommon_Window{ws}'] = df_for_prediction_features[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df_for_prediction_features[f'{digit_pos}_SecondMostCommon_Window{ws}'] = df_for_prediction_features[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_second_most_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        df_for_prediction_features[f'{digit_pos}_LeastCommon_Window{ws}'] = df_for_prediction_features[digit_pos].rolling(\n",
        "            window=ws, min_periods=1\n",
        "        ).apply(get_least_common_digit_in_window, raw=False).shift(1).fillna(-1).astype(int)\n",
        "\n",
        "        for digit in range(10):\n",
        "            df_for_prediction_features[f'{digit_pos}_Freq{digit}_Window{ws}'] = df_for_prediction_features[digit_pos].rolling(\n",
        "                window=ws, min_periods=1\n",
        "            ).apply(lambda x: (x == digit).sum(), raw=False).shift(1).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "X_predict_next = df_for_prediction_features.tail(1)[features]\n",
        "\n",
        "predicted_l2_digits_next_draw = []\n",
        "\n",
        "for target_name in target_cols_l2:\n",
        "    Y_train_final = df_ml[target_name]\n",
        "    X_train_final = df_ml[features]\n",
        "\n",
        "    final_scaler = MinMaxScaler()\n",
        "    X_train_scaled_final = final_scaler.fit_transform(X_train_final)\n",
        "\n",
        "    final_le = label_encoders[target_name]\n",
        "\n",
        "    X_predict_scaled_next = final_scaler.transform(X_predict_next)\n",
        "\n",
        "    preds_for_voting_next_draw = []\n",
        "\n",
        "    # Re-calculate dynamic weights based on the full backtesting history (model_accuracy_history)\n",
        "    current_weights_for_final_prediction = {}\n",
        "    total_weight_sum_final = 0.0\n",
        "    for model_name in model_params.keys():\n",
        "        if model_accuracy_history[target_name][model_name]:\n",
        "            weight = np.mean(model_accuracy_history[target_name][model_name])\n",
        "        else:\n",
        "            weight = 1.0 / len(model_params)\n",
        "        current_weights_for_final_prediction[model_name] = weight\n",
        "        total_weight_sum_final += weight\n",
        "\n",
        "    if total_weight_sum_final > 0:\n",
        "        current_weights_for_final_prediction = {k: v / total_weight_sum_final for k, v in current_weights_for_final_prediction.items()}\n",
        "    else:\n",
        "        current_weights_for_final_prediction = {k: 1.0 / len(model_params) for k in model_params.keys()}\n",
        "\n",
        "    for model_name, model_from_grid in trained_models[target_name].items():\n",
        "        if model_from_grid:\n",
        "            try:\n",
        "                # Retrain the model on the *entire* historical dataset for final prediction\n",
        "                model_instance = model_from_grid.__class__(**best_params_found[f\"{target_name}_{model_name}\"])\n",
        "                model_instance.fit(X_train_scaled_final, final_le.transform(Y_train_final))\n",
        "\n",
        "                proba_encoded = model_instance.predict_proba(X_predict_scaled_next)[0]\n",
        "                proba_dict = {final_le.inverse_transform([digit_idx])[0]: proba for digit_idx, proba in enumerate(proba_encoded)}\n",
        "                preds_for_voting_next_draw.append(proba_dict)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error during final prediction for {target_name} with {model_name}: {e}\")\n",
        "                preds_for_voting_next_draw.append({d: 1/10 for d in range(10)}) # Fallback to uniform\n",
        "\n",
        "    combined_probas_next_draw = {d: 0 for d in range(10)}\n",
        "    total_weight_next_draw = 0\n",
        "\n",
        "    for model_idx, proba_dict in enumerate(preds_for_voting_next_draw):\n",
        "        model_name = list(model_params.keys())[model_idx]\n",
        "        weight = current_weights_for_final_prediction.get(model_name, 0) # Get the dynamically calculated weight\n",
        "\n",
        "        for digit, proba in proba_dict.items():\n",
        "            combined_probas_next_draw[digit] += proba * weight\n",
        "        total_weight_next_draw += weight\n",
        "\n",
        "    if total_weight_next_draw > 0:\n",
        "        final_pred_digit_next_draw = max(combined_probas_next_draw, key=combined_probas_next_draw.get)\n",
        "    else:\n",
        "        final_pred_digit_next_draw = 0 # Fallback\n",
        "\n",
        "    predicted_l2_digits_next_draw.append(str(final_pred_digit_next_draw))\n",
        "\n",
        "predicted_l2_final = \"\".join(predicted_l2_digits_next_draw)\n",
        "\n",
        "print(f\"\\nüéâ ‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ 2 ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô‡∏á‡∏ß‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {next_draw_date.strftime('%Y-%m-%d')} ‡∏Ñ‡∏∑‡∏≠: **{predicted_l2_final}**\")\n",
        "print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "\n",
        "print(\"\\n--- Summary of Best Hyperparameters Found ---\")\n",
        "for key, params in best_params_found.items():\n",
        "    print(f\"{key}: {params}\")"
      ]
    }
  ]
}