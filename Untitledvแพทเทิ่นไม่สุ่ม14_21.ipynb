{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treekeaw1/-mana-bento-web/blob/main/Untitledv%E0%B9%81%E0%B8%9E%E0%B8%97%E0%B9%80%E0%B8%97%E0%B8%B4%E0%B9%88%E0%B8%99%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%AA%E0%B8%B8%E0%B9%88%E0%B8%A114_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import product\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class LottoPatternEngineV6:\n",
        "    def __init__(self, window_size=7): # Default window size to 7 as per latest analysis\n",
        "        print(\"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Lotto Pattern Engine v6.4 (Enhanced)...\")\n",
        "        self.models_6d = {}\n",
        "        self.scaler_6d = StandardScaler()\n",
        "        self.models_2d = {}\n",
        "        self.scaler_2d = StandardScaler()\n",
        "        self.df = pd.DataFrame()\n",
        "        self.window_size = window_size # Define window_size for feature extraction\n",
        "\n",
        "        self.CANDIDATE_POOL_SIZE_6D = 50000 # Increased pool size for more candidates\n",
        "        self.TOP_N_PREDICTIONS = 5 # How many top predictions to show\n",
        "\n",
        "        # These will be dynamically updated\n",
        "        self.banned_sequences_6d = set() # Sequences to avoid in 6-digit numbers\n",
        "        self.excluded_2d_numbers = set() # 2-digit numbers to exclude\n",
        "        self.hot_2d_numbers = set() # Hot 2-digit numbers\n",
        "\n",
        "        # Updated weights to incorporate new features\n",
        "        # Note: 'pattern_match' and 'dynamic' weights are broadened to include new arrangement/flow features\n",
        "        self.WEIGHTS_6D = {'ml_prob': 0.25, 'pattern_match': 0.40, 'dynamic': 0.25, 'uniqueness': 0.10}\n",
        "        self.WEIGHTS_2D = {'ml_prob': 0.40, 'hot_cold': 0.60} # Hot/cold is now dynamic\n",
        "\n",
        "        self.is_ready = False # Becomes True after successful data loading\n",
        "\n",
        "        # Mapping for detailed position descriptions (0-7 for combined 2-digit + 6-digit)\n",
        "        self.combined_digit_positions = {\n",
        "            0: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            1: '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)',\n",
        "            2: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏™‡∏ô)',\n",
        "            3: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô)',\n",
        "            4: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô)',\n",
        "            5: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢)',\n",
        "            6: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏¥‡∏ö)',\n",
        "            7: '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢)'\n",
        "        }\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"\n",
        "        Allows the user to select a CSV file, loads the data, and performs cleaning.\n",
        "        - Displays a button for file selection.\n",
        "        - Skips the first row (if it's empty or not actual CSV data).\n",
        "        - Validates and pads numbers with leading zeros.\n",
        "        - Converts dates to datetime objects.\n",
        "        - Sorts data from oldest to newest for chronological analysis.\n",
        "        \"\"\"\n",
        "        print(\"üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\")\n",
        "\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                print(\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå\")\n",
        "                return False\n",
        "\n",
        "            filepath = list(uploaded.keys())[0]\n",
        "            file_content = uploaded[filepath]\n",
        "\n",
        "            print(f\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {filepath}\")\n",
        "            df = pd.read_csv(BytesIO(file_content), encoding='utf-8-sig', skiprows=[0], header=0)\n",
        "\n",
        "            thai_col_mapping = {'‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': 'date', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1 (6 ‡∏´‡∏•‡∏±‡∏Å)': 'six_digit', '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': 'two_digit'}\n",
        "            normalized_columns_map = {col.strip().lower(): col for col in df.columns}\n",
        "            rename_dict = {}\n",
        "            for thai_name, eng_name in thai_col_mapping.items():\n",
        "                if thai_name.lower() in normalized_columns_map:\n",
        "                    rename_dict[normalized_columns_map[thai_name.lower()]] = eng_name\n",
        "            df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "            required_cols = ['date', 'six_digit', 'two_digit']\n",
        "            if not all(col in df.columns for col in required_cols):\n",
        "                missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "                raise ValueError(f\"‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join([c for c in required_cols if c not in df.columns])}\")\n",
        "\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "            df.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "            df['six_digit'] = pd.to_numeric(df['six_digit'], errors='coerce')\n",
        "            df['two_digit'] = pd.to_numeric(df['two_digit'], errors='coerce')\n",
        "            df.dropna(subset=['six_digit', 'two_digit'], inplace=True)\n",
        "\n",
        "            df['six_digit'] = df['six_digit'].astype(int).astype(str).str.zfill(6)\n",
        "            df['two_digit'] = df['two_digit'].astype(int).astype(str).str.zfill(2)\n",
        "\n",
        "            # --- UPDATED: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô DataFrame (‡∏á‡∏ß‡∏î 16/06/2568) ---\n",
        "            # This is hardcoded for specific update, in a real system this would be fetched from API\n",
        "            new_data = {'date': [pd.to_datetime('2025-06-16')], 'six_digit': ['507392'], 'two_digit': ['06']}\n",
        "            new_df = pd.DataFrame(new_data)\n",
        "            df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "            self.df = df.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
        "            print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(self.df)} ‡πÅ‡∏ñ‡∏ß\")\n",
        "            self.is_ready = True\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}\")\n",
        "            self.is_ready = False\n",
        "            return False\n",
        "\n",
        "    def _calculate_arrangement_patterns(self, number_str_6d):\n",
        "        \"\"\"Calculates arrangement patterns for a 6-digit number.\"\"\"\n",
        "        counts = Counter(number_str_6d)\n",
        "        num_unique = len(counts)\n",
        "        features = {}\n",
        "        # one-repeat: one digit repeats twice, others unique (e.g., 123445)\n",
        "        features['one_repeat'] = 1 if (num_unique == 5 and any(c == 2 for c in counts.values())) else 0\n",
        "        # high-repeat: two digits repeat twice (e.g., 112234) or one digit repeats 3+ times (e.g., 111234)\n",
        "        features['high_repeat'] = 1 if (any(c >= 3 for c in counts.values()) or \\\n",
        "                                        (num_unique == 4 and sum(1 for c in counts.values() if c == 2) == 2)) else 0\n",
        "        # balanced-parity: equal number of even/odd digits (3 even, 3 odd)\n",
        "        even_count = sum(1 for d in number_str_6d if int(d) % 2 == 0)\n",
        "        features['balanced_parity'] = 1 if even_count == 3 else 0\n",
        "        return features\n",
        "\n",
        "    def _calculate_retention_patterns(self, current_combined_str, previous_combined_str):\n",
        "        \"\"\"Calculates retention (common digits) between two 8-digit combined strings.\"\"\"\n",
        "        common_digits_count = len(set(current_combined_str) & set(previous_combined_str))\n",
        "        # This can be used as a numerical feature directly.\n",
        "        return common_digits_count\n",
        "\n",
        "    def _create_features_for_prediction(self, historical_data_for_features_df):\n",
        "        \"\"\"\n",
        "        Extracts a rich set of features from the historical data for prediction.\n",
        "        This DataFrame represents the 'window_size' preceding rows.\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "        if historical_data_for_features_df.empty:\n",
        "            return {}\n",
        "\n",
        "        last_date = historical_data_for_features_df['date'].iloc[-1]\n",
        "\n",
        "        # Original features\n",
        "        features['sin_month'] = np.sin(2 * np.pi * last_date.month / 12)\n",
        "        features['cos_month'] = np.cos(2 * np.pi * last_date.month / 12)\n",
        "\n",
        "        last_two_digit_num = int(historical_data_for_features_df['two_digit'].iloc[-1])\n",
        "        features['last_2d_num'] = last_two_digit_num\n",
        "        features['last_2d_sum'] = sum(int(d) for d in historical_data_for_features_df['two_digit'].iloc[-1])\n",
        "        features['last_2d_is_hot'] = 1 if last_two_digit_num in self.hot_2d_numbers else 0 # Dynamic hot check\n",
        "\n",
        "        # --- New Features from Data Analysis ---\n",
        "\n",
        "        # 1. Overall Digit Frequencies (0-9) in Preceding `window_size` rows (8 digits * window_size total)\n",
        "        all_preceding_digits_combined = \"\".join(\n",
        "            row['two_digit'] + row['six_digit']\n",
        "            for _, row in historical_data_for_features_df.iterrows()\n",
        "        )\n",
        "        for digit in range(10):\n",
        "            features[f'overall_digit_freq_{digit}'] = all_preceding_digits_combined.count(str(digit))\n",
        "\n",
        "        # 2. Positional Digit Frequencies (0-9) in Preceding `window_size` rows (for each of 8 positions)\n",
        "        for pos_idx in range(8):\n",
        "            digits_at_this_pos = \"\".join(\n",
        "                (row['two_digit'] + row['six_digit'])[pos_idx]\n",
        "                for _, row in historical_data_for_features_df.iterrows()\n",
        "                if pos_idx < len(row['two_digit'] + row['six_digit'])\n",
        "            )\n",
        "            for digit in range(10):\n",
        "                features[f'pos{pos_idx}_digit_freq_{digit}'] = digits_at_this_pos.count(str(digit))\n",
        "\n",
        "        # 3. Arrangement Patterns from the LAST 6-digit draw\n",
        "        last_six_digit_str = historical_data_for_features_df['six_digit'].iloc[-1]\n",
        "        arrangement_features = self._calculate_arrangement_patterns(last_six_digit_str)\n",
        "        features.update({f'arrangement_{k}': v for k, v in arrangement_features.items()})\n",
        "\n",
        "        # 4. Retention Pattern from the LAST draw to the SECOND LAST draw\n",
        "        if len(historical_data_for_features_df) >= 2:\n",
        "            last_draw_combined = historical_data_for_features_df['two_digit'].iloc[-1] + historical_data_for_features_df['six_digit'].iloc[-1]\n",
        "            second_last_draw_combined = historical_data_for_features_df['two_digit'].iloc[-2] + historical_data_for_features_df['six_digit'].iloc[-2]\n",
        "            features['retention_count_last_2_draws'] = self._calculate_retention_patterns(last_draw_combined, second_last_draw_combined)\n",
        "        else:\n",
        "            features['retention_count_last_2_draws'] = 0 # No sufficient history\n",
        "\n",
        "        # 5. Cyclical/Yearly Anomalies (e.g., deviation from 24 draws/year)\n",
        "        # Calculate actual draws in the last 12 months (or last full year in data)\n",
        "        # This requires more context than just `historical_data_for_features_df` if we want yearly counts.\n",
        "        # For simplicity for now, let's just use the year of the last draw for cyclical features if needed.\n",
        "        # If the full self.df is available, we can count for whole years.\n",
        "\n",
        "        # To make yearly anomaly calculation, we need access to the full df, or pass yearly counts in context\n",
        "        # For now, let's skip complex yearly anomaly in features, stick to simpler monthly cycles.\n",
        "\n",
        "        return list(features.values())\n",
        "\n",
        "    def _update_dynamic_number_lists(self, historical_df):\n",
        "        \"\"\"\n",
        "        Dynamically updates hot and excluded 2-digit number lists based on recent historical data.\n",
        "        \"\"\"\n",
        "        # Define the window for calculating hot/excluded numbers (e.g., last 100 draws)\n",
        "        recent_2d_window_size = min(150, len(historical_df))\n",
        "        recent_2d_data = historical_df['two_digit'].tail(recent_2d_window_size)\n",
        "\n",
        "        all_2d_numbers = [f\"{i:02d}\" for i in range(100)] # All possible 00-99\n",
        "\n",
        "        # Calculate frequency of 2-digit numbers\n",
        "        two_digit_counts = Counter(recent_2d_data)\n",
        "\n",
        "        # Hot Numbers: Top N most frequent in the recent window\n",
        "        # Consider any number that appears more than X times or is in the top Y%\n",
        "        hot_threshold = 2 # Appear at least 2 times in recent window\n",
        "        self.hot_2d_numbers = {int(num) for num, count in two_digit_counts.items() if count >= hot_threshold}\n",
        "\n",
        "        # Excluded Numbers: Numbers that haven't appeared in a long time (e.g., last 50 draws)\n",
        "        # Or, very rare historically.\n",
        "        self.excluded_2d_numbers = set()\n",
        "        long_term_window_size = min(200, len(historical_df))\n",
        "        long_term_2d_data = historical_df['two_digit'].tail(long_term_window_size)\n",
        "        long_term_2d_counts = Counter(long_term_2d_data)\n",
        "\n",
        "        # Exclude numbers that haven't appeared in the 'long_term_window'\n",
        "        for num_str in all_2d_numbers:\n",
        "            if int(num_str) not in self.hot_2d_numbers and long_term_2d_counts[num_str] == 0:\n",
        "                self.excluded_2d_numbers.add(int(num_str))\n",
        "\n",
        "        # Update banned 6-digit sequences (simple example: sequences from last N draws)\n",
        "        # This is a very basic heuristic; complex sequence banning requires more thought.\n",
        "        self.banned_sequences_6d = set()\n",
        "        last_6d_window_size = min(10, len(historical_df)) # Look at last 10 6-digit draws\n",
        "        for _, row in historical_df.tail(last_6d_window_size).iterrows():\n",
        "            six_digit_str = row['six_digit']\n",
        "            # Simple banned: repeat digits in sequence\n",
        "            if len(set(six_digit_str)) < len(six_digit_str): # If there are any repeated digits\n",
        "                 # Add some simple sequences like '11', '22' if they appear\n",
        "                 for i in range(len(six_digit_str) - 1):\n",
        "                     if six_digit_str[i] == six_digit_str[i+1]:\n",
        "                         self.banned_sequences_6d.add(six_digit_str[i:i+2])\n",
        "            # Adding some general highly undesirable sequences\n",
        "            self.banned_sequences_6d.update({\"123\", \"987\", \"789\", \"012\", \"111\", \"000\"})\n",
        "\n",
        "\n",
        "    def train(self, historical_df):\n",
        "        \"\"\"\n",
        "        Trains GradientBoostingClassifier models for 6-digit and 2-digit numbers.\n",
        "        Also updates dynamic number lists.\n",
        "        \"\"\"\n",
        "        # Update dynamic lists BEFORE training, as features depend on them\n",
        "        self._update_dynamic_number_lists(historical_df)\n",
        "\n",
        "        # Train 6D models\n",
        "        X_6d, Y_6d = [], [[] for _ in range(6)]\n",
        "        # Ensure enough data for feature extraction (window_size + minimum for first feature)\n",
        "        for i in range(self.window_size, len(historical_df)):\n",
        "            past_df_for_features = historical_df.iloc[i - self.window_size : i] # Get window_size rows before target\n",
        "\n",
        "            features = self._create_features_for_prediction(past_df_for_features)\n",
        "            # Ensure features are not empty (e.g., if historical_df was too small for window_size)\n",
        "            if not features:\n",
        "                continue\n",
        "\n",
        "            X_6d.append(features)\n",
        "            target_6d = historical_df['six_digit'].iloc[i]\n",
        "            for pos in range(6):\n",
        "                Y_6d[pos].append(int(target_6d[pos]))\n",
        "\n",
        "        if not X_6d:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• 6 ‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞\")\n",
        "            return\n",
        "\n",
        "        # Scale features\n",
        "        X_6d_scaled = self.scaler_6d.fit_transform(X_6d)\n",
        "\n",
        "        for pos in range(6):\n",
        "            model = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=42)\n",
        "            model.fit(X_6d_scaled, Y_6d[pos])\n",
        "            self.models_6d[pos] = model\n",
        "\n",
        "        # Train 2D models\n",
        "        X_2d, Y_2d = [], [[] for _ in range(2)]\n",
        "        for i in range(self.window_size, len(historical_df)):\n",
        "            past_df_for_features = historical_df.iloc[i - self.window_size : i]\n",
        "            features = self._create_features_for_prediction(past_df_for_features)\n",
        "            if not features:\n",
        "                continue\n",
        "\n",
        "            X_2d.append(features)\n",
        "            target_2d = historical_df['two_digit'].iloc[i]\n",
        "            for pos in range(2):\n",
        "                Y_2d[pos].append(int(target_2d[pos]))\n",
        "\n",
        "        if not X_2d:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• 2 ‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞\")\n",
        "            return\n",
        "\n",
        "        X_2d_scaled = self.scaler_2d.fit_transform(X_2d)\n",
        "        for pos in range(2):\n",
        "            model = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=42)\n",
        "            model.fit(X_2d_scaled, Y_2d[pos])\n",
        "            self.models_2d[pos] = model\n",
        "\n",
        "    def _score_and_rank_6d(self, features_scaled, context):\n",
        "        \"\"\"\n",
        "        Scores and ranks 6-digit lottery number candidates based on ML probabilities and pattern metrics.\n",
        "        New features are implicitly used through `features_scaled`.\n",
        "        \"\"\"\n",
        "        # Get top probable digits for each position (e.g., top 4)\n",
        "        top_cands_per_pos = [model.classes_[np.argsort(model.predict_proba(features_scaled)[0])[::-1][:4]] for model in self.models_6d.values()]\n",
        "\n",
        "        # Generate candidates from the product of top digits\n",
        "        candidates_raw = [\"\".join(map(str, c)) for c in product(*top_cands_per_pos)]\n",
        "\n",
        "        # If candidate pool is too small, expand by including more digits\n",
        "        if len(candidates_raw) < self.CANDIDATE_POOL_SIZE_6D:\n",
        "            top_cands_per_pos = [model.classes_[np.argsort(model.predict_proba(features_scaled)[0])[::-1][:5]] for model in self.models_6d.values()] # Top 5\n",
        "            candidates_raw = [\"\".join(map(str, c)) for c in product(*top_cands_per_pos)]\n",
        "\n",
        "        # Sample a large candidate pool if raw candidates are too many\n",
        "        candidates_to_score = candidates_raw\n",
        "        if len(candidates_raw) > self.CANDIDATE_POOL_SIZE_6D:\n",
        "            candidates_to_score = np.random.choice(candidates_raw, self.CANDIDATE_POOL_SIZE_6D, replace=False)\n",
        "\n",
        "        scored_candidates = []\n",
        "        recent_digit_pool = set(digit for s in context['recent_draws_6d'] for digit in s) # Digits from last 5 6D draws\n",
        "        last_2_draws_pool = set(digit for s in context['last_2_draws_6d'] for digit in s) # Digits from last 2 6D draws\n",
        "\n",
        "        for cand_str in candidates_to_score:\n",
        "            # Exclude if starts with '0' (for 6-digit R1 usually) or contains banned sequences\n",
        "            if cand_str.startswith('0') or any(seq in cand_str for seq in self.banned_sequences_6d):\n",
        "                continue\n",
        "\n",
        "            # ML Probability Score (log probability for stability)\n",
        "            log_prob_sum = 0\n",
        "            for pos, digit in enumerate(cand_str):\n",
        "                try:\n",
        "                    # Get probability for the specific digit in the candidate\n",
        "                    digit_prob = self.models_6d[pos].predict_proba(features_scaled)[0][np.where(self.models_6d[pos].classes_ == int(digit))[0][0]]\n",
        "                    log_prob_sum += np.log(digit_prob + 1e-9) # Add small epsilon to avoid log(0)\n",
        "                except IndexError: # Digit not in model's known classes for this position (unlikely)\n",
        "                    log_prob_sum += np.log(1e-9) # Penalize heavily\n",
        "\n",
        "            # Pattern Match Score (based on digits in recent history)\n",
        "            # This incorporates the spirit of 'Flow Pattern'\n",
        "            matches_recent_pool = sum(1 for d in set(cand_str) if d in recent_digit_pool)\n",
        "            pattern_score = 50 * (matches_recent_pool / 6) if matches_recent_pool >= 3 else 0 # Adjust threshold\n",
        "\n",
        "            # Dynamic Score (based on very recent draws - hotness)\n",
        "            dynamic_score = sum(15 for d in set(cand_str) if d in last_2_draws_pool)\n",
        "\n",
        "            # Uniqueness Score (for diversity within the number itself)\n",
        "            unique_score = len(set(cand_str)) * 10 # More unique digits, higher score\n",
        "\n",
        "            # Arrangement Pattern Score (new)\n",
        "            arr_patterns = self._calculate_arrangement_patterns(cand_str)\n",
        "            arrangement_score = 0\n",
        "            if arr_patterns['one_repeat']: arrangement_score += 15 # Reward one-repeat as it's very common\n",
        "            if arr_patterns['high_repeat']: arrangement_score += 20 # Reward high-repeat as it's common\n",
        "            if arr_patterns['balanced_parity']: arrangement_score += 10 # Reward balanced parity\n",
        "\n",
        "            # Retention Score (new - how many digits match the last draw's digits)\n",
        "            current_combined_cand = \"\".join(cand_str) # 6D\n",
        "            last_historical_draw_combined = context['last_historical_combined_draw'] # This should be 8D\n",
        "\n",
        "            # Take only the 6-digit part from the last historical draw for comparison\n",
        "            last_historical_6d_str = last_historical_draw_combined[2:] # Assuming last_historical_combined_draw is '2d6d'\n",
        "\n",
        "            retention_count = self._calculate_retention_patterns(current_combined_cand, last_historical_6d_str) # Compare 6D to 6D\n",
        "            retention_score = retention_count * 10 # 10 points per retained digit (heuristic)\n",
        "\n",
        "\n",
        "            # Final Score Calculation - Adjusted weights and new features\n",
        "            final_score = (log_prob_sum * self.WEIGHTS_6D['ml_prob'] +\n",
        "                           pattern_score * self.WEIGHTS_6D['pattern_match'] +\n",
        "                           dynamic_score * self.WEIGHTS_6D['dynamic'] +\n",
        "                           unique_score * self.WEIGHTS_6D['uniqueness'] +\n",
        "                           arrangement_score + # Added arrangement score\n",
        "                           retention_score) # Added retention score\n",
        "\n",
        "            # Adjust score so it's positive and scaled (arbitrary offset)\n",
        "            final_score = max(0, final_score + 100) # Ensure score is non-negative for display\n",
        "\n",
        "            scored_candidates.append({'number': cand_str, 'score': final_score})\n",
        "\n",
        "        # Ensure we return TOP_N_PREDICTIONS even if the list is smaller\n",
        "        return sorted(scored_candidates, key=lambda x: x['score'], reverse=True)[:self.TOP_N_PREDICTIONS]\n",
        "\n",
        "    def _score_and_rank_2d(self, features_scaled):\n",
        "        \"\"\"\n",
        "        Scores and ranks 2-digit lottery number candidates based on ML probabilities and hot/cold status.\n",
        "        New features are implicitly used through `features_scaled`.\n",
        "        \"\"\"\n",
        "        scored_candidates = []\n",
        "        for i in range(100):\n",
        "            cand_str = f\"{i:02d}\"\n",
        "            cand_num = int(cand_str)\n",
        "\n",
        "            if cand_num in self.excluded_2d_numbers: # Use dynamically excluded list\n",
        "                continue\n",
        "\n",
        "            # ML Probability Score\n",
        "            try:\n",
        "                prob_d1 = self.models_2d[0].predict_proba(features_scaled)[0][np.where(self.models_2d[0].classes_ == int(cand_str[0]))[0][0]]\n",
        "                prob_d2 = self.models_2d[1].predict_proba(features_scaled)[0][np.where(self.models_2d[1].classes_ == int(cand_str[1]))[0][0]]\n",
        "                ml_score = prob_d1 * prob_d2\n",
        "            except IndexError: # Should not happen if models are well trained on 0-9\n",
        "                ml_score = 1e-9 # Very low score\n",
        "\n",
        "            # Hot/Cold Score (uses dynamic hot list)\n",
        "            hot_cold_score = 1.0 if cand_num in self.hot_2d_numbers else 0.5 # Higher for hot numbers\n",
        "\n",
        "            final_score = (ml_score * self.WEIGHTS_2D['ml_prob']) + (hot_cold_score * self.WEIGHTS_2D['hot_cold'])\n",
        "            scored_candidates.append({'number': cand_str, 'score': final_score})\n",
        "\n",
        "        return sorted(scored_candidates, key=lambda x: x['score'], reverse=True)[:self.TOP_N_PREDICTIONS]\n",
        "\n",
        "    def run_backtest(self, num_backtest_draws=50):\n",
        "        \"\"\"\n",
        "        Runs a backtest to evaluate model performance over a specified number of past draws.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80); print(f\"V6.4 Backtest: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á {num_backtest_draws} ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...\"); print(\"=\"*80)\n",
        "        results = []\n",
        "        # Start training from a point that allows enough history for feature extraction and initial training\n",
        "        start_index = len(self.df) - num_backtest_draws\n",
        "        if start_index < self.window_size * 2: # Ensure enough data for initial training and then for features\n",
        "            print(f\"‚ùó ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backtest (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ {self.window_size * 2} ‡∏á‡∏ß‡∏î)\")\n",
        "            return\n",
        "\n",
        "        # Iterate from the calculated start index up to (but not including) the last row\n",
        "        # The last row of self.df is the *actual* latest draw, which we want to predict FOR\n",
        "        # So we backtest up to the second to last row, and the final run predicts the very last.\n",
        "        for i in tqdm(range(start_index, len(self.df)), desc=\"Backtesting\"):\n",
        "            # Ensure `train_df` has at least `self.window_size` rows for feature extraction\n",
        "            # and enough rows for initial model training (e.g., first 10-20 draws).\n",
        "            # The `train` method itself will check `len(historical_df)`.\n",
        "            train_df = self.df.iloc[:i]\n",
        "\n",
        "            # Re-train models with data available *up to this point* in the backtest\n",
        "            if len(train_df) < self.window_size + 10: # Ensure sufficient data to train (arbitrary 10 for initial runs)\n",
        "                continue # Skip if not enough historical data for robust training\n",
        "\n",
        "            self.train(train_df) # Train models and update dynamic lists\n",
        "\n",
        "            # Get the actual outcome for this iteration's target draw\n",
        "            actual_row = self.df.iloc[i]\n",
        "\n",
        "            # Features for prediction of `actual_row` would be based on data *before* `actual_row`\n",
        "            features_for_pred_df = self.df.iloc[i - self.window_size : i]\n",
        "\n",
        "            if features_for_pred_df.empty: # Should not happen if start_index is calculated correctly\n",
        "                continue\n",
        "\n",
        "            features = self._create_features_for_prediction(features_for_pred_df)\n",
        "            if not features: # Ensure feature creation was successful\n",
        "                continue\n",
        "\n",
        "            features_scaled_6d = self.scaler_6d.transform([features])\n",
        "            features_scaled_2d = self.scaler_2d.transform([features])\n",
        "\n",
        "            # Context for scoring - based on `features_for_pred_df` (the history)\n",
        "            context = {\n",
        "                'recent_draws_6d': [r['six_digit'] for r in features_for_pred_df.tail(5).to_dict('records')],\n",
        "                'last_2_draws_6d': [r['six_digit'] for r in features_for_pred_df.tail(2).to_dict('records')],\n",
        "                'last_historical_combined_draw': features_for_pred_df['two_digit'].iloc[-1] + features_for_pred_df['six_digit'].iloc[-1]\n",
        "            }\n",
        "\n",
        "            top5_6d = [p['number'] for p in self._score_and_rank_6d(features_scaled_6d, context)]\n",
        "            top5_2d = [p['number'] for p in self._score_and_rank_2d(features_scaled_2d)]\n",
        "\n",
        "            results.append({\n",
        "                'actual_6d': actual_row['six_digit'],\n",
        "                'pred_6d': top5_6d,\n",
        "                'actual_2d': actual_row['two_digit'],\n",
        "                'pred_2d': top5_2d\n",
        "            })\n",
        "\n",
        "        # Calculate overall accuracy for the backtested draws\n",
        "        acc_6d_top5 = np.mean([1 if r['actual_6d'] in r['pred_6d'] else 0 for r in results])\n",
        "        acc_2d_top5 = np.mean([1 if r['actual_2d'] in r['pred_2d'] else 0 for r in results])\n",
        "\n",
        "        print(\"\\n--- üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (Backtest V6.4) ---\")\n",
        "        print(f\"‚Ä¢ ‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Top 5): {acc_6d_top5:.1%}\")\n",
        "        print(f\"‚Ä¢ ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Top 5): {acc_2d_top5:.1%}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    def run_final_prediction(self):\n",
        "        \"\"\"\n",
        "        Runs the final prediction for the very next lottery draw using all available data.\n",
        "        Generates 4 diverse prediction sets and applies post-processing to avoid exact repeats\n",
        "        of the last historical row in the first prediction set.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80); print(\"V6.4 Final Prediction: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\"); print(\"=\"*80)\n",
        "        print(\"üí™ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ...\")\n",
        "        self.train(self.df) # Train models with the full historical dataset\n",
        "\n",
        "        # Features for the next prediction (based on the very last `window_size` rows of self.df)\n",
        "        features_for_next_pred_df = self.df.tail(self.window_size)\n",
        "\n",
        "        if features_for_next_pred_df.empty or len(features_for_next_pred_df) < self.window_size:\n",
        "            print(f\"‚ùó ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ {self.window_size} ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\")\n",
        "            return [] # Return empty list\n",
        "\n",
        "        features = self._create_features_for_prediction(features_for_next_pred_df)\n",
        "        if not features:\n",
        "            print(\"‚ùó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ\")\n",
        "            return []\n",
        "\n",
        "        features_scaled_6d = self.scaler_6d.transform([features])\n",
        "        features_scaled_2d = self.scaler_2d.transform([features])\n",
        "\n",
        "        # Context for scoring the final prediction\n",
        "        context_for_pred = {\n",
        "            'recent_draws_6d': [r['six_digit'] for r in features_for_next_pred_df.tail(5).to_dict('records')],\n",
        "            'last_2_draws_6d': [r['six_digit'] for r in features_for_next_pred_df.tail(2).to_dict('records')],\n",
        "            'last_historical_combined_draw': features_for_next_pred_df['two_digit'].iloc[-1] + features_for_next_pred_df['six_digit'].iloc[-1]\n",
        "        }\n",
        "\n",
        "        # --- Generate Prediction Sets (4 sets as requested) ---\n",
        "        all_predicted_sets = []\n",
        "        generated_sets_strings = set() # To ensure uniqueness among generated sets\n",
        "\n",
        "        # Set 1: Most Probable (with non-recurrence post-processing)\n",
        "        top5_6d_raw = self._score_and_rank_6d(features_scaled_6d, context_for_pred)\n",
        "        top5_2d_raw = self._score_and_rank_2d(features_scaled_2d)\n",
        "\n",
        "        predicted_6d_set1 = top5_6d_raw[0]['number'] if top5_6d_raw else '??????'\n",
        "        predicted_2d_set1 = top5_2d_raw[0]['number'] if top5_2d_raw else '??'\n",
        "\n",
        "        combined_pred_set1_str = predicted_2d_set1 + predicted_6d_set1\n",
        "        last_historical_combined_str = context_for_pred['last_historical_combined_draw']\n",
        "\n",
        "        # Post-processing for Set 1: Avoid exact repeat of the last historical draw\n",
        "        if combined_pred_set1_str == last_historical_combined_str:\n",
        "            print(\"\\nüö® ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 1 ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥...\")\n",
        "            modified_digits = list(combined_pred_set1_str)\n",
        "\n",
        "            # Simple strategy: Increment 2-3 specific digits to ensure difference\n",
        "            # Choose digits from less predictable positions (e.g., those with lower individual accuracy during backtest)\n",
        "            # For now, let's pick fixed positions for simplicity and guarantee difference\n",
        "            positions_to_modify = [0, 2, 5] # Example: L2-tens, R1-hundred-thousands, R1-hundreds\n",
        "\n",
        "            for pos in positions_to_modify:\n",
        "                if pos < len(modified_digits):\n",
        "                    current_val = int(modified_digits[pos])\n",
        "                    modified_digits[pos] = str((current_val + 1) % 10) # Increment by 1\n",
        "\n",
        "            combined_pred_set1_str = \"\".join(modified_digits)\n",
        "            predicted_2d_set1 = combined_pred_set1_str[0:2]\n",
        "            predicted_6d_set1 = combined_pred_set1_str[2:8]\n",
        "\n",
        "        all_predicted_sets.append({\n",
        "            '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1': predicted_6d_set1,\n",
        "            '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': predicted_2d_set1\n",
        "        })\n",
        "        generated_sets_strings.add(predicted_2d_set1 + predicted_6d_set1)\n",
        "\n",
        "        # Generate additional diverse prediction sets\n",
        "        # Strategy: Mix top predictions from ML and introduce slight variations\n",
        "\n",
        "        # Set 2: Combine top 6D, and 2nd 2D (or vice versa)\n",
        "        if len(top5_6d_raw) > 0 and len(top5_2d_raw) > 1:\n",
        "            set2_6d = top5_6d_raw[0]['number']\n",
        "            set2_2d = top5_2d_raw[1]['number']\n",
        "            new_combined_str = set2_2d + set2_6d\n",
        "            if new_combined_str not in generated_sets_strings:\n",
        "                all_predicted_sets.append({'‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1': set2_6d, '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': set2_2d})\n",
        "                generated_sets_strings.add(new_combined_str)\n",
        "\n",
        "        # Set 3: Use 2nd top 6D, and top 2D (or introduce a small offset)\n",
        "        if len(top5_6d_raw) > 1 and len(top5_2d_raw) > 0:\n",
        "            set3_6d = top5_6d_raw[1]['number']\n",
        "            set3_2d = top5_2d_raw[0]['number']\n",
        "            new_combined_str = set3_2d + set3_6d\n",
        "            if new_combined_str not in generated_sets_strings:\n",
        "                all_predicted_sets.append({'‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1': set3_6d, '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': set3_2d})\n",
        "                generated_sets_strings.add(new_combined_str)\n",
        "\n",
        "        # Set 4: Combine 2nd top 6D and 2nd top 2D, or introduce random changes\n",
        "        if len(top5_6d_raw) > 1 and len(top5_2d_raw) > 1:\n",
        "            set4_6d = top5_6d_raw[1]['number']\n",
        "            set4_2d = top5_2d_raw[1]['number']\n",
        "            new_combined_str = set4_2d + set4_6d\n",
        "            if new_combined_str not in generated_sets_strings:\n",
        "                all_predicted_sets.append({'‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1': set4_6d, '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': set4_2d})\n",
        "                generated_sets_strings.add(new_combined_str)\n",
        "\n",
        "        # Ensure we always have 4 sets, even if the above strategies don't yield 4 unique ones\n",
        "        while len(all_predicted_sets) < 4:\n",
        "            # Take the last generated set, and randomly change a few digits\n",
        "            last_generated_6d = all_predicted_sets[-1]['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1']\n",
        "            last_generated_2d = all_predicted_sets[-1]['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á']\n",
        "            modified_digits = list(last_generated_2d + last_generated_6d)\n",
        "\n",
        "            # Change 2-3 random positions\n",
        "            num_changes = np.random.randint(2, 4)\n",
        "            positions_to_change = np.random.choice(8, num_changes, replace=False)\n",
        "\n",
        "            for pos in positions_to_change:\n",
        "                current_val = int(modified_digits[pos])\n",
        "                modified_digits[pos] = str((current_val + np.random.randint(1, 5)) % 10) # Random increment 1-4\n",
        "\n",
        "            new_combined_str = \"\".join(modified_digits)\n",
        "            if new_combined_str not in generated_sets_strings:\n",
        "                all_predicted_sets.append({\n",
        "                    '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1': new_combined_str[2:],\n",
        "                    '‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á': new_combined_str[0:2]\n",
        "                })\n",
        "                generated_sets_strings.add(new_combined_str)\n",
        "\n",
        "            # Safety break to prevent infinite loop if somehow always generates non-unique despite changes\n",
        "            if len(generated_sets_strings) >= self.CANDIDATE_POOL_SIZE_6D: # Fallback to prevent infinite loop\n",
        "                break\n",
        "\n",
        "\n",
        "        print(\"\\n--- üèÜ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 4 ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÄ‡∏•‡∏Ç 6 ‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏•‡∏∞ ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á) ---\")\n",
        "        for i, prediction_set in enumerate(all_predicted_sets):\n",
        "            print(f\"  ‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà {i+1}: ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1: {prediction_set['‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà 1']} | ‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á: {prediction_set['‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏á']}\")\n",
        "\n",
        "        print(\"\\nüí° ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° ML Models):\")\n",
        "        print(\"  - ‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning (GradientBoostingClassifier) ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô\")\n",
        "        print(\"    ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á, ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\")\n",
        "        print(\"  - ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç '‡∏£‡πâ‡∏≠‡∏ô' ‡πÅ‡∏•‡∏∞ '‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏≠‡∏Å' ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥\")\n",
        "        print(\"  - ‡∏ä‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\")\n",
        "        print(\"    **‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏≤‡∏Å‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏Ç‡∏≠‡∏á‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∏‡∏î)**\")\n",
        "        print(\"  - ‡∏ä‡∏∏‡∏î‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà 2, 3, 4) ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô\")\n",
        "        print(\"    ‡πÇ‡∏î‡∏¢‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\")\n",
        "        print(\"\\n‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ‡∏ú‡∏• 100%\")\n",
        "\n",
        "        return all_predicted_sets\n",
        "\n",
        "\n",
        "# Main Execution Block\n",
        "def main():\n",
        "    print(\"=\"*80); print(\"‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà The Lotto Pattern Engine v6.4\"); print(\"=\"*80)\n",
        "    engine = LottoPatternEngineV6(window_size=7) # Instantiate with window_size=7 as per plan\n",
        "\n",
        "    if engine.load_and_prepare_data():\n",
        "        engine.run_backtest(num_backtest_draws=50) # Run backtest\n",
        "        engine.run_final_prediction() # Run final prediction\n",
        "\n",
        "    print(\"\\nüòä ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏£‡∏±‡∏ö!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "0410b451f2534d35840726d63ce973bc",
            "e25e9bd07e1e49ffbaa280402c8b71d1",
            "94f171943d7548aa903279e510580cbc",
            "0b757d0974304c148c231912e7bcfaf5",
            "9f3df2a471ce4a41a64644da30af98f5",
            "4b0582072ab847c98d0c7bc2a41bb419",
            "00fb356f748a43e08482d5c0628f82de",
            "5b30b6eb2f3348299c97c7a550bbf65a",
            "d2abec5769494a92820e8260304c238d",
            "10fa3ed3f40540b9a05d5d03664b40a4",
            "34516a19ea4340d58763644528de078f"
          ]
        },
        "id": "xFR-hHg22RX-",
        "outputId": "6b6721d2-e66c-4521-e5a1-a9d0fe0bad31"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà The Lotto Pattern Engine v6.4\n",
            "================================================================================\n",
            "üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Lotto Pattern Engine v6.4 (Enhanced)...\n",
            "üìä ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0b2ce13-7617-4b6e-8e4d-9bf0d57e3ee5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b0b2ce13-7617-4b6e-8e4d-9bf0d57e3ee5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv to ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 21.csv\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 560 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "================================================================================\n",
            "V6.4 Backtest: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 50 ‡∏á‡∏ß‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0410b451f2534d35840726d63ce973bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Backtesting:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-eB3IcRbN1FTrZesoRssYUMkx2nTDalo",
      "authorship_tag": "ABX9TyOgUSNPFKLcGYOdhr8xHXki",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0410b451f2534d35840726d63ce973bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e25e9bd07e1e49ffbaa280402c8b71d1",
              "IPY_MODEL_94f171943d7548aa903279e510580cbc",
              "IPY_MODEL_0b757d0974304c148c231912e7bcfaf5"
            ],
            "layout": "IPY_MODEL_9f3df2a471ce4a41a64644da30af98f5"
          }
        },
        "e25e9bd07e1e49ffbaa280402c8b71d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b0582072ab847c98d0c7bc2a41bb419",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00fb356f748a43e08482d5c0628f82de",
            "value": "Backtesting:‚Äá‚Äá32%"
          }
        },
        "94f171943d7548aa903279e510580cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b30b6eb2f3348299c97c7a550bbf65a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2abec5769494a92820e8260304c238d",
            "value": 16
          }
        },
        "0b757d0974304c148c231912e7bcfaf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fa3ed3f40540b9a05d5d03664b40a4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34516a19ea4340d58763644528de078f",
            "value": "‚Äá16/50‚Äá[1:23:07&lt;2:49:56,‚Äá299.89s/it]"
          }
        },
        "9f3df2a471ce4a41a64644da30af98f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0582072ab847c98d0c7bc2a41bb419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00fb356f748a43e08482d5c0628f82de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b30b6eb2f3348299c97c7a550bbf65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2abec5769494a92820e8260304c238d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10fa3ed3f40540b9a05d5d03664b40a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34516a19ea4340d58763644528de078f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}